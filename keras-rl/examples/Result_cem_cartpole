$ python cem_cartpole.py 
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: 
I tensorflow/stream_executor/cuda/cuda_dnn.cc:3448] Unable to load cuDNN DSO
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
[2016-10-08 15:55:00,078] Making new env: CartPole-v0
actions,dim: 2 4
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 660 Ti
major: 3 minor: 0 memoryClockRate (GHz) 1.0195
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.62GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660 Ti, pci bus id: 0000:01:00.0)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_1 (Dense)                  (None, 2)             10          dense_input_1[0][0]              
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 2)             0           dense_1[0][0]                    
====================================================================================================
Total params: 10
____________________________________________________________________________________________________
None
Training for 100000 steps ...
    11/100000: episode: 1, duration: 0.193s, episode steps: 11, steps per second: 57, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.337, 1.362], mean_best_reward: --
    22/100000: episode: 2, duration: 0.021s, episode steps: 11, steps per second: 521, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.111 [-3.316, 2.196], mean_best_reward: --
    39/100000: episode: 3, duration: 0.026s, episode steps: 17, steps per second: 660, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.096 [-0.926, 1.498], mean_best_reward: --
    52/100000: episode: 4, duration: 0.020s, episode steps: 13, steps per second: 644, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.098 [-1.784, 2.811], mean_best_reward: --
    66/100000: episode: 5, duration: 0.027s, episode steps: 14, steps per second: 511, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.100 [-2.677, 1.609], mean_best_reward: --
    75/100000: episode: 6, duration: 0.017s, episode steps: 9, steps per second: 521, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.762, 1.768], mean_best_reward: --
    85/100000: episode: 7, duration: 0.019s, episode steps: 10, steps per second: 522, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.134 [-1.524, 2.510], mean_best_reward: --
    96/100000: episode: 8, duration: 0.018s, episode steps: 11, steps per second: 603, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.120 [-1.376, 2.223], mean_best_reward: --
   105/100000: episode: 9, duration: 0.016s, episode steps: 9, steps per second: 556, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.741, 1.805], mean_best_reward: --
   133/100000: episode: 10, duration: 0.050s, episode steps: 28, steps per second: 565, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.321 [0.000, 1.000], mean observation: 0.026 [-1.963, 2.926], mean_best_reward: --
   142/100000: episode: 11, duration: 0.018s, episode steps: 9, steps per second: 492, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.802, 1.734], mean_best_reward: --
   152/100000: episode: 12, duration: 0.018s, episode steps: 10, steps per second: 559, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.915, 3.108], mean_best_reward: --
   164/100000: episode: 13, duration: 0.021s, episode steps: 12, steps per second: 572, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.094 [-1.954, 1.219], mean_best_reward: --
   208/100000: episode: 14, duration: 0.071s, episode steps: 44, steps per second: 616, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.379, 0.999], mean_best_reward: --
   217/100000: episode: 15, duration: 0.017s, episode steps: 9, steps per second: 537, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.768, 2.831], mean_best_reward: --
   245/100000: episode: 16, duration: 0.047s, episode steps: 28, steps per second: 601, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.387, 1.018], mean_best_reward: --
   278/100000: episode: 17, duration: 0.059s, episode steps: 33, steps per second: 560, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.086 [-1.595, 0.632], mean_best_reward: --
   293/100000: episode: 18, duration: 0.022s, episode steps: 15, steps per second: 670, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.087 [-2.823, 1.751], mean_best_reward: --
   303/100000: episode: 19, duration: 0.016s, episode steps: 10, steps per second: 637, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.932, 3.121], mean_best_reward: --
   312/100000: episode: 20, duration: 0.015s, episode steps: 9, steps per second: 608, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.756, 2.807], mean_best_reward: --
   320/100000: episode: 21, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.546, 2.557], mean_best_reward: --
   329/100000: episode: 22, duration: 0.015s, episode steps: 9, steps per second: 601, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.743, 1.748], mean_best_reward: --
   343/100000: episode: 23, duration: 0.022s, episode steps: 14, steps per second: 622, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.076 [-1.922, 1.214], mean_best_reward: --
   352/100000: episode: 24, duration: 0.014s, episode steps: 9, steps per second: 638, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.357, 2.215], mean_best_reward: --
   371/100000: episode: 25, duration: 0.031s, episode steps: 19, steps per second: 619, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.057 [-1.004, 1.783], mean_best_reward: --
   384/100000: episode: 26, duration: 0.023s, episode steps: 13, steps per second: 557, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.112 [-2.771, 1.777], mean_best_reward: --
   393/100000: episode: 27, duration: 0.016s, episode steps: 9, steps per second: 563, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.159 [-2.307, 1.363], mean_best_reward: --
   411/100000: episode: 28, duration: 0.028s, episode steps: 18, steps per second: 647, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.068 [-1.035, 1.789], mean_best_reward: --
   424/100000: episode: 29, duration: 0.021s, episode steps: 13, steps per second: 606, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.083 [-1.965, 1.225], mean_best_reward: --
   433/100000: episode: 30, duration: 0.015s, episode steps: 9, steps per second: 592, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.760, 1.804], mean_best_reward: --
   442/100000: episode: 31, duration: 0.014s, episode steps: 9, steps per second: 621, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.267, 1.342], mean_best_reward: --
   473/100000: episode: 32, duration: 0.048s, episode steps: 31, steps per second: 639, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.069 [-0.436, 0.993], mean_best_reward: --
   484/100000: episode: 33, duration: 0.018s, episode steps: 11, steps per second: 623, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.133 [-2.876, 1.786], mean_best_reward: --
   495/100000: episode: 34, duration: 0.018s, episode steps: 11, steps per second: 605, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.782, 1.811], mean_best_reward: --
   527/100000: episode: 35, duration: 0.051s, episode steps: 32, steps per second: 623, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.092 [-0.879, 0.562], mean_best_reward: --
   546/100000: episode: 36, duration: 0.036s, episode steps: 19, steps per second: 533, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.062 [-0.808, 1.297], mean_best_reward: --
   626/100000: episode: 37, duration: 0.143s, episode steps: 80, steps per second: 560, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.128 [-0.992, 0.857], mean_best_reward: --
   652/100000: episode: 38, duration: 0.046s, episode steps: 26, steps per second: 567, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-1.089, 0.798], mean_best_reward: --
   663/100000: episode: 39, duration: 0.022s, episode steps: 11, steps per second: 496, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.129 [-2.322, 1.372], mean_best_reward: --
   673/100000: episode: 40, duration: 0.019s, episode steps: 10, steps per second: 521, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.074, 1.943], mean_best_reward: --
   684/100000: episode: 41, duration: 0.020s, episode steps: 11, steps per second: 560, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.302, 2.151], mean_best_reward: --
   695/100000: episode: 42, duration: 0.022s, episode steps: 11, steps per second: 507, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.423, 2.302], mean_best_reward: --
   708/100000: episode: 43, duration: 0.026s, episode steps: 13, steps per second: 503, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.073 [-2.683, 1.796], mean_best_reward: --
   719/100000: episode: 44, duration: 0.023s, episode steps: 11, steps per second: 477, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-2.199, 1.411], mean_best_reward: --
   729/100000: episode: 45, duration: 0.020s, episode steps: 10, steps per second: 498, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.154 [-1.528, 2.588], mean_best_reward: --
   740/100000: episode: 46, duration: 0.022s, episode steps: 11, steps per second: 509, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.161 [-1.728, 2.911], mean_best_reward: --
   750/100000: episode: 47, duration: 0.018s, episode steps: 10, steps per second: 571, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.163 [-1.528, 2.555], mean_best_reward: --
   762/100000: episode: 48, duration: 0.023s, episode steps: 12, steps per second: 526, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.117 [-2.148, 1.336], mean_best_reward: --
   773/100000: episode: 49, duration: 0.022s, episode steps: 11, steps per second: 511, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.452, 1.558], mean_best_reward: --
   783/100000: episode: 50, duration: 0.017s, episode steps: 10, steps per second: 573, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.566, 1.547], mean_best_reward: --
   794/100000: episode: 51, duration: 0.019s, episode steps: 11, steps per second: 566, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.784, 1.795], mean_best_reward: --
   819/100000: episode: 52, duration: 0.042s, episode steps: 25, steps per second: 592, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.105 [-1.292, 0.752], mean_best_reward: --
   834/100000: episode: 53, duration: 0.026s, episode steps: 15, steps per second: 572, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.101 [-2.360, 1.336], mean_best_reward: --
   845/100000: episode: 54, duration: 0.020s, episode steps: 11, steps per second: 552, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.120 [-2.270, 1.369], mean_best_reward: --
   857/100000: episode: 55, duration: 0.022s, episode steps: 12, steps per second: 555, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-0.963, 1.635], mean_best_reward: --
   871/100000: episode: 56, duration: 0.024s, episode steps: 14, steps per second: 584, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.087 [-1.597, 2.543], mean_best_reward: --
   880/100000: episode: 57, duration: 0.016s, episode steps: 9, steps per second: 564, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.385, 2.231], mean_best_reward: --
   905/100000: episode: 58, duration: 0.042s, episode steps: 25, steps per second: 601, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.082 [-1.472, 0.640], mean_best_reward: --
   914/100000: episode: 59, duration: 0.016s, episode steps: 9, steps per second: 553, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.131 [-1.213, 1.899], mean_best_reward: --
   927/100000: episode: 60, duration: 0.026s, episode steps: 13, steps per second: 504, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.106 [-2.822, 1.720], mean_best_reward: --
   940/100000: episode: 61, duration: 0.022s, episode steps: 13, steps per second: 590, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.096 [-1.000, 1.762], mean_best_reward: --
   965/100000: episode: 62, duration: 0.039s, episode steps: 25, steps per second: 636, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.070 [-2.854, 1.768], mean_best_reward: --
   985/100000: episode: 63, duration: 0.036s, episode steps: 20, steps per second: 552, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-0.942, 0.437], mean_best_reward: --
  1035/100000: episode: 64, duration: 0.087s, episode steps: 50, steps per second: 573, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.197 [-1.064, 0.629], mean_best_reward: --
  1045/100000: episode: 65, duration: 0.021s, episode steps: 10, steps per second: 474, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.968, 3.016], mean_best_reward: --
  1055/100000: episode: 66, duration: 0.018s, episode steps: 10, steps per second: 571, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.548, 2.475], mean_best_reward: --
  1073/100000: episode: 67, duration: 0.032s, episode steps: 18, steps per second: 562, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.106 [-2.730, 1.571], mean_best_reward: --
  1098/100000: episode: 68, duration: 0.039s, episode steps: 25, steps per second: 643, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.101 [-0.374, 0.837], mean_best_reward: --
  1121/100000: episode: 69, duration: 0.036s, episode steps: 23, steps per second: 636, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.067 [-0.766, 1.137], mean_best_reward: --
  1159/100000: episode: 70, duration: 0.056s, episode steps: 38, steps per second: 677, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.762, 1.092], mean_best_reward: --
  1177/100000: episode: 71, duration: 0.028s, episode steps: 18, steps per second: 652, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.056 [-1.842, 1.129], mean_best_reward: --
  1189/100000: episode: 72, duration: 0.019s, episode steps: 12, steps per second: 641, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.133 [-1.541, 2.668], mean_best_reward: --
  1208/100000: episode: 73, duration: 0.031s, episode steps: 19, steps per second: 614, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.037 [-1.019, 1.449], mean_best_reward: --
  1257/100000: episode: 74, duration: 0.077s, episode steps: 49, steps per second: 639, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.002 [-1.088, 0.615], mean_best_reward: --
  1276/100000: episode: 75, duration: 0.032s, episode steps: 19, steps per second: 595, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.099 [-0.589, 1.435], mean_best_reward: --
  1285/100000: episode: 76, duration: 0.017s, episode steps: 9, steps per second: 522, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.718, 2.807], mean_best_reward: --
  1298/100000: episode: 77, duration: 0.020s, episode steps: 13, steps per second: 634, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.093 [-1.148, 1.757], mean_best_reward: --
  1308/100000: episode: 78, duration: 0.018s, episode steps: 10, steps per second: 558, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.138 [-1.138, 2.011], mean_best_reward: --
  1317/100000: episode: 79, duration: 0.015s, episode steps: 9, steps per second: 611, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.763, 1.795], mean_best_reward: --
  1344/100000: episode: 80, duration: 0.043s, episode steps: 27, steps per second: 630, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.044 [-0.929, 1.366], mean_best_reward: --
  1365/100000: episode: 81, duration: 0.033s, episode steps: 21, steps per second: 631, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.103 [-0.594, 1.383], mean_best_reward: --
  1376/100000: episode: 82, duration: 0.018s, episode steps: 11, steps per second: 606, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.116 [-1.329, 2.202], mean_best_reward: --
  1386/100000: episode: 83, duration: 0.018s, episode steps: 10, steps per second: 545, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.978, 3.100], mean_best_reward: --
  1398/100000: episode: 84, duration: 0.023s, episode steps: 12, steps per second: 530, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.504, 1.021], mean_best_reward: --
  1414/100000: episode: 85, duration: 0.030s, episode steps: 16, steps per second: 539, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.105 [-1.197, 2.151], mean_best_reward: --
  1423/100000: episode: 86, duration: 0.018s, episode steps: 9, steps per second: 500, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.750, 2.767], mean_best_reward: --
  1448/100000: episode: 87, duration: 0.048s, episode steps: 25, steps per second: 519, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.076 [-0.749, 1.521], mean_best_reward: --
  1463/100000: episode: 88, duration: 0.027s, episode steps: 15, steps per second: 557, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.082 [-1.406, 2.228], mean_best_reward: --
  1477/100000: episode: 89, duration: 0.022s, episode steps: 14, steps per second: 629, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.106 [-1.979, 3.087], mean_best_reward: --
  1489/100000: episode: 90, duration: 0.018s, episode steps: 12, steps per second: 675, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.121 [-1.914, 3.053], mean_best_reward: --
  1499/100000: episode: 91, duration: 0.017s, episode steps: 10, steps per second: 584, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-2.095, 1.231], mean_best_reward: --
  1509/100000: episode: 92, duration: 0.018s, episode steps: 10, steps per second: 570, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.585, 2.376], mean_best_reward: --
  1527/100000: episode: 93, duration: 0.030s, episode steps: 18, steps per second: 591, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.053 [-1.565, 2.412], mean_best_reward: --
  1543/100000: episode: 94, duration: 0.027s, episode steps: 16, steps per second: 600, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.086 [-2.542, 1.546], mean_best_reward: --
  1553/100000: episode: 95, duration: 0.017s, episode steps: 10, steps per second: 591, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.145 [-0.757, 1.552], mean_best_reward: --
  1566/100000: episode: 96, duration: 0.024s, episode steps: 13, steps per second: 546, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.126 [-1.717, 2.847], mean_best_reward: --
  1578/100000: episode: 97, duration: 0.024s, episode steps: 12, steps per second: 511, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.111 [-1.180, 0.584], mean_best_reward: --
  1586/100000: episode: 98, duration: 0.013s, episode steps: 8, steps per second: 634, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.139 [-1.150, 1.990], mean_best_reward: --
  1599/100000: episode: 99, duration: 0.022s, episode steps: 13, steps per second: 603, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.108 [-0.999, 1.819], mean_best_reward: --
  1653/100000: episode: 100, duration: 0.084s, episode steps: 54, steps per second: 642, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.154 [-1.032, 0.650], mean_best_reward: --
  1666/100000: episode: 101, duration: 0.021s, episode steps: 13, steps per second: 624, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.112 [-1.675, 0.991], mean_best_reward: --
  1676/100000: episode: 102, duration: 0.017s, episode steps: 10, steps per second: 591, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.963, 2.976], mean_best_reward: --
  1701/100000: episode: 103, duration: 0.041s, episode steps: 25, steps per second: 615, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.053 [-1.366, 2.258], mean_best_reward: --
  1713/100000: episode: 104, duration: 0.023s, episode steps: 12, steps per second: 526, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.182, 2.108], mean_best_reward: --
  1724/100000: episode: 105, duration: 0.021s, episode steps: 11, steps per second: 535, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.336, 1.359], mean_best_reward: --
  1734/100000: episode: 106, duration: 0.017s, episode steps: 10, steps per second: 587, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.961, 3.042], mean_best_reward: --
  1743/100000: episode: 107, duration: 0.016s, episode steps: 9, steps per second: 550, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.121 [-2.224, 1.387], mean_best_reward: --
  1775/100000: episode: 108, duration: 0.055s, episode steps: 32, steps per second: 583, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.065 [-1.297, 0.647], mean_best_reward: --
  1794/100000: episode: 109, duration: 0.032s, episode steps: 19, steps per second: 603, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.116 [-0.381, 1.201], mean_best_reward: --
  1804/100000: episode: 110, duration: 0.016s, episode steps: 10, steps per second: 613, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.148 [-2.531, 1.569], mean_best_reward: --
  1825/100000: episode: 111, duration: 0.035s, episode steps: 21, steps per second: 606, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.092 [-1.920, 0.983], mean_best_reward: --
  1845/100000: episode: 112, duration: 0.033s, episode steps: 20, steps per second: 609, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.080 [-1.138, 0.615], mean_best_reward: --
  1857/100000: episode: 113, duration: 0.020s, episode steps: 12, steps per second: 593, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.107 [-3.100, 1.981], mean_best_reward: --
  1875/100000: episode: 114, duration: 0.029s, episode steps: 18, steps per second: 613, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.060 [-3.333, 2.297], mean_best_reward: --
  1884/100000: episode: 115, duration: 0.015s, episode steps: 9, steps per second: 583, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.798, 2.845], mean_best_reward: --
  1894/100000: episode: 116, duration: 0.017s, episode steps: 10, steps per second: 595, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.007, 1.925], mean_best_reward: --
  1907/100000: episode: 117, duration: 0.020s, episode steps: 13, steps per second: 636, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.109 [-1.845, 0.998], mean_best_reward: --
  1919/100000: episode: 118, duration: 0.019s, episode steps: 12, steps per second: 624, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.104 [-1.588, 2.383], mean_best_reward: --
  1941/100000: episode: 119, duration: 0.036s, episode steps: 22, steps per second: 618, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.097 [-1.172, 0.603], mean_best_reward: --
  1971/100000: episode: 120, duration: 0.048s, episode steps: 30, steps per second: 620, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.051 [-2.261, 2.691], mean_best_reward: --
  1980/100000: episode: 121, duration: 0.015s, episode steps: 9, steps per second: 599, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.848, 1.726], mean_best_reward: --
  1995/100000: episode: 122, duration: 0.023s, episode steps: 15, steps per second: 643, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.119 [-0.795, 1.213], mean_best_reward: --
  2005/100000: episode: 123, duration: 0.016s, episode steps: 10, steps per second: 611, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.111 [-1.991, 3.010], mean_best_reward: --
  2013/100000: episode: 124, duration: 0.014s, episode steps: 8, steps per second: 561, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.567, 2.574], mean_best_reward: --
  2034/100000: episode: 125, duration: 0.036s, episode steps: 21, steps per second: 577, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.079 [-0.605, 1.227], mean_best_reward: --
  2042/100000: episode: 126, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.589, 1.596], mean_best_reward: --
  2059/100000: episode: 127, duration: 0.028s, episode steps: 17, steps per second: 616, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.080 [-1.724, 2.722], mean_best_reward: --
  2071/100000: episode: 128, duration: 0.020s, episode steps: 12, steps per second: 605, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.116 [-0.933, 1.563], mean_best_reward: --
  2081/100000: episode: 129, duration: 0.015s, episode steps: 10, steps per second: 650, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.008, 1.935], mean_best_reward: --
  2105/100000: episode: 130, duration: 0.037s, episode steps: 24, steps per second: 645, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.027 [-2.383, 1.540], mean_best_reward: --
  2115/100000: episode: 131, duration: 0.019s, episode steps: 10, steps per second: 539, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.109 [-1.196, 1.944], mean_best_reward: --
  2125/100000: episode: 132, duration: 0.020s, episode steps: 10, steps per second: 505, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.554, 2.562], mean_best_reward: --
  2136/100000: episode: 133, duration: 0.020s, episode steps: 11, steps per second: 563, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.137 [-1.345, 2.314], mean_best_reward: --
  2155/100000: episode: 134, duration: 0.031s, episode steps: 19, steps per second: 611, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.069 [-0.958, 1.581], mean_best_reward: --
  2164/100000: episode: 135, duration: 0.018s, episode steps: 9, steps per second: 510, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.138 [-2.310, 1.359], mean_best_reward: --
  2174/100000: episode: 136, duration: 0.017s, episode steps: 10, steps per second: 606, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.070, 1.913], mean_best_reward: --
  2189/100000: episode: 137, duration: 0.024s, episode steps: 15, steps per second: 612, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.082 [-0.780, 1.221], mean_best_reward: --
  2198/100000: episode: 138, duration: 0.014s, episode steps: 9, steps per second: 636, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.799, 1.783], mean_best_reward: --
  2211/100000: episode: 139, duration: 0.020s, episode steps: 13, steps per second: 640, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.102 [-0.837, 1.466], mean_best_reward: --
  2230/100000: episode: 140, duration: 0.032s, episode steps: 19, steps per second: 587, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.081 [-2.832, 1.766], mean_best_reward: --
  2244/100000: episode: 141, duration: 0.024s, episode steps: 14, steps per second: 575, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.104 [-1.486, 0.956], mean_best_reward: --
  2254/100000: episode: 142, duration: 0.017s, episode steps: 10, steps per second: 598, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-2.102, 1.170], mean_best_reward: --
  2278/100000: episode: 143, duration: 0.038s, episode steps: 24, steps per second: 639, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.107 [-0.740, 1.312], mean_best_reward: --
  2291/100000: episode: 144, duration: 0.021s, episode steps: 13, steps per second: 619, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.113 [-1.399, 2.374], mean_best_reward: --
  2341/100000: episode: 145, duration: 0.090s, episode steps: 50, steps per second: 553, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.176 [-0.782, 0.925], mean_best_reward: --
  2349/100000: episode: 146, duration: 0.012s, episode steps: 8, steps per second: 657, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-2.586, 1.548], mean_best_reward: --
  2358/100000: episode: 147, duration: 0.014s, episode steps: 9, steps per second: 625, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.825, 1.727], mean_best_reward: --
  2367/100000: episode: 148, duration: 0.013s, episode steps: 9, steps per second: 692, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.760, 2.839], mean_best_reward: --
  2393/100000: episode: 149, duration: 0.039s, episode steps: 26, steps per second: 663, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.066 [-1.230, 0.604], mean_best_reward: --
  2439/100000: episode: 150, duration: 0.072s, episode steps: 46, steps per second: 642, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.059 [-1.395, 0.840], mean_best_reward: 27.500000
  2462/100000: episode: 151, duration: 0.035s, episode steps: 23, steps per second: 650, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.077 [-0.593, 1.089], mean_best_reward: --
  2478/100000: episode: 152, duration: 0.026s, episode steps: 16, steps per second: 606, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.639, 1.049], mean_best_reward: --
  2501/100000: episode: 153, duration: 0.035s, episode steps: 23, steps per second: 657, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.101 [-0.583, 0.991], mean_best_reward: --
  2542/100000: episode: 154, duration: 0.062s, episode steps: 41, steps per second: 662, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.075 [-0.420, 0.990], mean_best_reward: --
  2556/100000: episode: 155, duration: 0.023s, episode steps: 14, steps per second: 616, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.113 [-0.784, 1.532], mean_best_reward: --
  2583/100000: episode: 156, duration: 0.047s, episode steps: 27, steps per second: 580, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.114 [-1.362, 0.405], mean_best_reward: --
  2595/100000: episode: 157, duration: 0.021s, episode steps: 12, steps per second: 571, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.618, 1.169], mean_best_reward: --
  2607/100000: episode: 158, duration: 0.021s, episode steps: 12, steps per second: 560, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.173, 2.014], mean_best_reward: --
  2619/100000: episode: 159, duration: 0.021s, episode steps: 12, steps per second: 575, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.755, 1.285], mean_best_reward: --
  2654/100000: episode: 160, duration: 0.063s, episode steps: 35, steps per second: 552, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.058 [-1.176, 0.502], mean_best_reward: --
  2666/100000: episode: 161, duration: 0.021s, episode steps: 12, steps per second: 566, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.123 [-0.609, 1.248], mean_best_reward: --
  2674/100000: episode: 162, duration: 0.016s, episode steps: 8, steps per second: 489, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.164 [-1.351, 2.229], mean_best_reward: --
  2692/100000: episode: 163, duration: 0.031s, episode steps: 18, steps per second: 584, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.823, 1.254], mean_best_reward: --
  2707/100000: episode: 164, duration: 0.021s, episode steps: 15, steps per second: 700, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.118 [-0.580, 1.319], mean_best_reward: --
  2745/100000: episode: 165, duration: 0.057s, episode steps: 38, steps per second: 661, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.074 [-0.481, 0.876], mean_best_reward: --
  2773/100000: episode: 166, duration: 0.041s, episode steps: 28, steps per second: 678, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.449, 1.126], mean_best_reward: --
  2810/100000: episode: 167, duration: 0.055s, episode steps: 37, steps per second: 667, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.085 [-0.378, 1.044], mean_best_reward: --
  2838/100000: episode: 168, duration: 0.045s, episode steps: 28, steps per second: 622, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.363, 0.901], mean_best_reward: --
  2906/100000: episode: 169, duration: 0.108s, episode steps: 68, steps per second: 630, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.156 [-1.049, 0.582], mean_best_reward: --
  2981/100000: episode: 170, duration: 0.114s, episode steps: 75, steps per second: 658, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.105 [-0.754, 0.942], mean_best_reward: --
  3004/100000: episode: 171, duration: 0.037s, episode steps: 23, steps per second: 615, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.076 [-0.648, 1.463], mean_best_reward: --
  3036/100000: episode: 172, duration: 0.050s, episode steps: 32, steps per second: 643, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.072 [-0.947, 1.711], mean_best_reward: --
  3062/100000: episode: 173, duration: 0.040s, episode steps: 26, steps per second: 654, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.059 [-0.730, 1.026], mean_best_reward: --
  3110/100000: episode: 174, duration: 0.073s, episode steps: 48, steps per second: 654, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.027 [-0.834, 1.416], mean_best_reward: --
  3135/100000: episode: 175, duration: 0.040s, episode steps: 25, steps per second: 629, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.121 [-0.372, 0.828], mean_best_reward: --
  3159/100000: episode: 176, duration: 0.036s, episode steps: 24, steps per second: 659, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.128 [-1.097, 0.377], mean_best_reward: --
  3195/100000: episode: 177, duration: 0.056s, episode steps: 36, steps per second: 638, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.063 [-0.578, 1.173], mean_best_reward: --
  3218/100000: episode: 178, duration: 0.037s, episode steps: 23, steps per second: 624, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.107 [-0.606, 1.041], mean_best_reward: --
  3228/100000: episode: 179, duration: 0.016s, episode steps: 10, steps per second: 615, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.118 [-0.987, 1.670], mean_best_reward: --
  3253/100000: episode: 180, duration: 0.039s, episode steps: 25, steps per second: 642, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.066 [-0.784, 1.562], mean_best_reward: --
  3293/100000: episode: 181, duration: 0.060s, episode steps: 40, steps per second: 661, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.084 [-0.752, 1.117], mean_best_reward: --
  3324/100000: episode: 182, duration: 0.053s, episode steps: 31, steps per second: 585, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.074 [-0.620, 1.111], mean_best_reward: --
  3342/100000: episode: 183, duration: 0.027s, episode steps: 18, steps per second: 655, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.069 [-0.998, 1.614], mean_best_reward: --
  3367/100000: episode: 184, duration: 0.040s, episode steps: 25, steps per second: 624, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.136 [-1.216, 0.539], mean_best_reward: --
  3387/100000: episode: 185, duration: 0.030s, episode steps: 20, steps per second: 672, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.097 [-0.974, 1.834], mean_best_reward: --
  3396/100000: episode: 186, duration: 0.014s, episode steps: 9, steps per second: 641, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.122 [-1.375, 2.252], mean_best_reward: --
  3422/100000: episode: 187, duration: 0.041s, episode steps: 26, steps per second: 640, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.052 [-0.793, 1.609], mean_best_reward: --
  3465/100000: episode: 188, duration: 0.067s, episode steps: 43, steps per second: 643, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.055 [-1.648, 0.588], mean_best_reward: --
  3480/100000: episode: 189, duration: 0.026s, episode steps: 15, steps per second: 583, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.107 [-0.807, 1.367], mean_best_reward: --
  3510/100000: episode: 190, duration: 0.055s, episode steps: 30, steps per second: 549, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.378, 0.836], mean_best_reward: --
  3531/100000: episode: 191, duration: 0.039s, episode steps: 21, steps per second: 543, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.095 [-0.609, 1.457], mean_best_reward: --
  3571/100000: episode: 192, duration: 0.061s, episode steps: 40, steps per second: 651, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.582, 1.013], mean_best_reward: --
  3588/100000: episode: 193, duration: 0.027s, episode steps: 17, steps per second: 635, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.072 [-1.193, 1.881], mean_best_reward: --
  3603/100000: episode: 194, duration: 0.026s, episode steps: 15, steps per second: 579, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.079 [-0.809, 1.280], mean_best_reward: --
  3622/100000: episode: 195, duration: 0.030s, episode steps: 19, steps per second: 625, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.085 [-0.745, 1.440], mean_best_reward: --
  3654/100000: episode: 196, duration: 0.054s, episode steps: 32, steps per second: 598, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.809, 1.226], mean_best_reward: --
  3673/100000: episode: 197, duration: 0.028s, episode steps: 19, steps per second: 673, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.092 [-0.804, 1.174], mean_best_reward: --
  3700/100000: episode: 198, duration: 0.039s, episode steps: 27, steps per second: 701, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.073 [-1.135, 0.558], mean_best_reward: --
  3711/100000: episode: 199, duration: 0.018s, episode steps: 11, steps per second: 612, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.136 [-1.155, 2.019], mean_best_reward: --
  3722/100000: episode: 200, duration: 0.019s, episode steps: 11, steps per second: 583, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.207, 1.917], mean_best_reward: 80.000000
  3766/100000: episode: 201, duration: 0.067s, episode steps: 44, steps per second: 656, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.120 [-0.501, 0.903], mean_best_reward: --
  3834/100000: episode: 202, duration: 0.101s, episode steps: 68, steps per second: 672, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.128 [-0.746, 0.923], mean_best_reward: --
  3952/100000: episode: 203, duration: 0.202s, episode steps: 118, steps per second: 586, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.848, 1.382], mean_best_reward: --
  3974/100000: episode: 204, duration: 0.041s, episode steps: 22, steps per second: 537, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.540, 0.903], mean_best_reward: --
  4023/100000: episode: 205, duration: 0.089s, episode steps: 49, steps per second: 553, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.179 [-0.627, 1.295], mean_best_reward: --
  4105/100000: episode: 206, duration: 0.141s, episode steps: 82, steps per second: 581, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.177 [-0.750, 0.926], mean_best_reward: --
  4162/100000: episode: 207, duration: 0.095s, episode steps: 57, steps per second: 603, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.189 [-0.657, 1.577], mean_best_reward: --
  4237/100000: episode: 208, duration: 0.121s, episode steps: 75, steps per second: 621, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.099 [-0.639, 0.930], mean_best_reward: --
  4265/100000: episode: 209, duration: 0.041s, episode steps: 28, steps per second: 680, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.088 [-0.970, 0.633], mean_best_reward: --
  4332/100000: episode: 210, duration: 0.105s, episode steps: 67, steps per second: 639, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.091 [-0.792, 1.055], mean_best_reward: --
  4456/100000: episode: 211, duration: 0.217s, episode steps: 124, steps per second: 571, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.041 [-0.783, 1.069], mean_best_reward: --
  4483/100000: episode: 212, duration: 0.052s, episode steps: 27, steps per second: 516, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.104 [-0.438, 0.785], mean_best_reward: --
  4550/100000: episode: 213, duration: 0.115s, episode steps: 67, steps per second: 584, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.163 [-0.609, 1.151], mean_best_reward: --
  4630/100000: episode: 214, duration: 0.122s, episode steps: 80, steps per second: 658, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.113 [-0.594, 0.858], mean_best_reward: --
  4703/100000: episode: 215, duration: 0.110s, episode steps: 73, steps per second: 664, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.101 [-1.286, 0.640], mean_best_reward: --
  4782/100000: episode: 216, duration: 0.120s, episode steps: 79, steps per second: 660, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.177 [-0.661, 1.163], mean_best_reward: --
  4853/100000: episode: 217, duration: 0.121s, episode steps: 71, steps per second: 586, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.072 [-0.604, 0.951], mean_best_reward: --
  4900/100000: episode: 218, duration: 0.080s, episode steps: 47, steps per second: 586, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.096 [-0.897, 0.593], mean_best_reward: --
  5008/100000: episode: 219, duration: 0.173s, episode steps: 108, steps per second: 624, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.264 [-1.278, 0.941], mean_best_reward: --
  5075/100000: episode: 220, duration: 0.103s, episode steps: 67, steps per second: 649, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.141 [-1.200, 0.538], mean_best_reward: --
  5149/100000: episode: 221, duration: 0.121s, episode steps: 74, steps per second: 611, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.027 [-1.127, 1.083], mean_best_reward: --
  5213/100000: episode: 222, duration: 0.099s, episode steps: 64, steps per second: 645, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.084 [-0.597, 1.062], mean_best_reward: --
  5252/100000: episode: 223, duration: 0.059s, episode steps: 39, steps per second: 661, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.122 [-0.544, 0.902], mean_best_reward: --
  5294/100000: episode: 224, duration: 0.063s, episode steps: 42, steps per second: 669, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.533, 0.893], mean_best_reward: --
  5357/100000: episode: 225, duration: 0.091s, episode steps: 63, steps per second: 694, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.069 [-1.192, 0.646], mean_best_reward: --
  5377/100000: episode: 226, duration: 0.027s, episode steps: 20, steps per second: 732, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.398, 0.873], mean_best_reward: --
  5420/100000: episode: 227, duration: 0.062s, episode steps: 43, steps per second: 692, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.046 [-0.558, 0.880], mean_best_reward: --
  5458/100000: episode: 228, duration: 0.054s, episode steps: 38, steps per second: 707, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.122 [-0.605, 1.067], mean_best_reward: --
  5547/100000: episode: 229, duration: 0.147s, episode steps: 89, steps per second: 606, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.136 [-1.451, 0.797], mean_best_reward: --
  5618/100000: episode: 230, duration: 0.108s, episode steps: 71, steps per second: 659, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.229 [-1.366, 0.663], mean_best_reward: --
  5628/100000: episode: 231, duration: 0.017s, episode steps: 10, steps per second: 574, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.133 [-1.411, 0.740], mean_best_reward: --
  5675/100000: episode: 232, duration: 0.077s, episode steps: 47, steps per second: 613, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.574 [0.000, 1.000], mean observation: 0.230 [-0.488, 1.326], mean_best_reward: --
  5806/100000: episode: 233, duration: 0.213s, episode steps: 131, steps per second: 616, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.139 [-1.271, 0.848], mean_best_reward: --
  5842/100000: episode: 234, duration: 0.062s, episode steps: 36, steps per second: 578, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.154 [-0.600, 1.068], mean_best_reward: --
  5877/100000: episode: 235, duration: 0.058s, episode steps: 35, steps per second: 608, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.080 [-0.379, 0.748], mean_best_reward: --
  5909/100000: episode: 236, duration: 0.048s, episode steps: 32, steps per second: 669, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-0.918, 0.539], mean_best_reward: --
  5986/100000: episode: 237, duration: 0.110s, episode steps: 77, steps per second: 697, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.108 [-0.781, 1.090], mean_best_reward: --
  6019/100000: episode: 238, duration: 0.050s, episode steps: 33, steps per second: 655, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.104 [-0.396, 1.065], mean_best_reward: --
  6073/100000: episode: 239, duration: 0.073s, episode steps: 54, steps per second: 744, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.075 [-1.233, 1.108], mean_best_reward: --
  6141/100000: episode: 240, duration: 0.112s, episode steps: 68, steps per second: 608, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.238 [-1.123, 0.761], mean_best_reward: --
  6253/100000: episode: 241, duration: 0.185s, episode steps: 112, steps per second: 604, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.031 [-0.918, 0.813], mean_best_reward: --
  6275/100000: episode: 242, duration: 0.040s, episode steps: 22, steps per second: 553, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.054 [-0.830, 1.191], mean_best_reward: --
  6310/100000: episode: 243, duration: 0.059s, episode steps: 35, steps per second: 598, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.082 [-0.930, 1.372], mean_best_reward: --
  6412/100000: episode: 244, duration: 0.158s, episode steps: 102, steps per second: 648, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.086 [-0.656, 1.032], mean_best_reward: --
  6539/100000: episode: 245, duration: 0.197s, episode steps: 127, steps per second: 644, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.121 [-1.014, 1.203], mean_best_reward: --
  6578/100000: episode: 246, duration: 0.064s, episode steps: 39, steps per second: 613, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.071 [-0.627, 0.932], mean_best_reward: --
  6641/100000: episode: 247, duration: 0.094s, episode steps: 63, steps per second: 670, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.138 [-1.010, 0.567], mean_best_reward: --
  6692/100000: episode: 248, duration: 0.081s, episode steps: 51, steps per second: 632, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.190 [-0.705, 1.264], mean_best_reward: --
  6730/100000: episode: 249, duration: 0.063s, episode steps: 38, steps per second: 601, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.108 [-0.995, 0.760], mean_best_reward: --
  6770/100000: episode: 250, duration: 0.068s, episode steps: 40, steps per second: 589, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-0.949, 0.776], mean_best_reward: 121.000000
  6811/100000: episode: 251, duration: 0.062s, episode steps: 41, steps per second: 657, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.208 [-0.986, 1.295], mean_best_reward: --
  6873/100000: episode: 252, duration: 0.101s, episode steps: 62, steps per second: 611, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.222 [-1.131, 0.761], mean_best_reward: --
  6897/100000: episode: 253, duration: 0.043s, episode steps: 24, steps per second: 563, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.360, 0.999], mean_best_reward: --
  6947/100000: episode: 254, duration: 0.083s, episode steps: 50, steps per second: 604, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.698, 1.152], mean_best_reward: --
  6975/100000: episode: 255, duration: 0.050s, episode steps: 28, steps per second: 565, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.373, 0.964], mean_best_reward: --
  7095/100000: episode: 256, duration: 0.200s, episode steps: 120, steps per second: 600, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.137 [-0.771, 1.179], mean_best_reward: --
  7146/100000: episode: 257, duration: 0.088s, episode steps: 51, steps per second: 576, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.170 [-0.682, 0.935], mean_best_reward: --
  7235/100000: episode: 258, duration: 0.155s, episode steps: 89, steps per second: 575, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.150 [-0.766, 1.116], mean_best_reward: --
  7263/100000: episode: 259, duration: 0.049s, episode steps: 28, steps per second: 577, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.596, 0.928], mean_best_reward: --
  7318/100000: episode: 260, duration: 0.087s, episode steps: 55, steps per second: 629, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.077 [-0.602, 0.961], mean_best_reward: --
  7354/100000: episode: 261, duration: 0.051s, episode steps: 36, steps per second: 711, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.584, 1.160], mean_best_reward: --
  7422/100000: episode: 262, duration: 0.112s, episode steps: 68, steps per second: 606, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.150 [-1.251, 1.143], mean_best_reward: --
  7461/100000: episode: 263, duration: 0.064s, episode steps: 39, steps per second: 605, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.114 [-0.499, 0.889], mean_best_reward: --
  7511/100000: episode: 264, duration: 0.063s, episode steps: 50, steps per second: 797, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.103 [-1.259, 0.476], mean_best_reward: --
  7601/100000: episode: 265, duration: 0.122s, episode steps: 90, steps per second: 736, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.123 [-0.859, 1.037], mean_best_reward: --
  7767/100000: episode: 266, duration: 0.207s, episode steps: 166, steps per second: 803, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.007 [-1.456, 0.769], mean_best_reward: --
  7812/100000: episode: 267, duration: 0.067s, episode steps: 45, steps per second: 676, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.183 [-1.154, 1.013], mean_best_reward: --
  7856/100000: episode: 268, duration: 0.084s, episode steps: 44, steps per second: 526, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.150 [-0.594, 0.925], mean_best_reward: --
  7993/100000: episode: 269, duration: 0.219s, episode steps: 137, steps per second: 627, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.014 [-0.789, 0.960], mean_best_reward: --
  8082/100000: episode: 270, duration: 0.136s, episode steps: 89, steps per second: 656, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.155 [-1.324, 1.135], mean_best_reward: --
  8120/100000: episode: 271, duration: 0.061s, episode steps: 38, steps per second: 621, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.093 [-0.594, 0.947], mean_best_reward: --
  8168/100000: episode: 272, duration: 0.082s, episode steps: 48, steps per second: 585, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.070 [-1.413, 0.995], mean_best_reward: --
  8230/100000: episode: 273, duration: 0.093s, episode steps: 62, steps per second: 665, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.159 [-0.631, 1.082], mean_best_reward: --
  8292/100000: episode: 274, duration: 0.105s, episode steps: 62, steps per second: 589, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.108 [-0.525, 0.836], mean_best_reward: --
  8306/100000: episode: 275, duration: 0.026s, episode steps: 14, steps per second: 530, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.759, 1.346], mean_best_reward: --
  8365/100000: episode: 276, duration: 0.099s, episode steps: 59, steps per second: 596, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.123 [-0.947, 0.550], mean_best_reward: --
  8395/100000: episode: 277, duration: 0.054s, episode steps: 30, steps per second: 553, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.114 [-0.435, 0.824], mean_best_reward: --
  8450/100000: episode: 278, duration: 0.096s, episode steps: 55, steps per second: 572, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.136 [-0.781, 1.023], mean_best_reward: --
  8488/100000: episode: 279, duration: 0.059s, episode steps: 38, steps per second: 640, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.150 [-0.450, 1.029], mean_best_reward: --
  8504/100000: episode: 280, duration: 0.026s, episode steps: 16, steps per second: 622, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.766, 1.197], mean_best_reward: --
  8580/100000: episode: 281, duration: 0.130s, episode steps: 76, steps per second: 587, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.152 [-0.922, 1.304], mean_best_reward: --
  8628/100000: episode: 282, duration: 0.086s, episode steps: 48, steps per second: 561, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.117 [-0.485, 0.897], mean_best_reward: --
  8699/100000: episode: 283, duration: 0.127s, episode steps: 71, steps per second: 558, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.080 [-0.707, 0.833], mean_best_reward: --
  8727/100000: episode: 284, duration: 0.047s, episode steps: 28, steps per second: 596, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.794, 1.149], mean_best_reward: --
  8817/100000: episode: 285, duration: 0.144s, episode steps: 90, steps per second: 623, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.202 [-1.660, 0.865], mean_best_reward: --
  8865/100000: episode: 286, duration: 0.081s, episode steps: 48, steps per second: 593, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.521, 1.250], mean_best_reward: --
  8919/100000: episode: 287, duration: 0.085s, episode steps: 54, steps per second: 637, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.100 [-0.501, 0.841], mean_best_reward: --
  9008/100000: episode: 288, duration: 0.146s, episode steps: 89, steps per second: 608, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.111 [-1.163, 0.885], mean_best_reward: --
  9056/100000: episode: 289, duration: 0.077s, episode steps: 48, steps per second: 627, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.942, 1.340], mean_best_reward: --
  9091/100000: episode: 290, duration: 0.057s, episode steps: 35, steps per second: 617, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.052 [-0.805, 1.451], mean_best_reward: --
  9197/100000: episode: 291, duration: 0.183s, episode steps: 106, steps per second: 580, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.185 [-1.047, 1.544], mean_best_reward: --
  9251/100000: episode: 292, duration: 0.101s, episode steps: 54, steps per second: 535, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.604, 1.238], mean_best_reward: --
  9335/100000: episode: 293, duration: 0.139s, episode steps: 84, steps per second: 606, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.114 [-0.830, 1.139], mean_best_reward: --
  9469/100000: episode: 294, duration: 0.216s, episode steps: 134, steps per second: 619, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.156 [-0.804, 1.290], mean_best_reward: --
  9527/100000: episode: 295, duration: 0.103s, episode steps: 58, steps per second: 563, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.228, 0.927], mean_best_reward: --
  9549/100000: episode: 296, duration: 0.043s, episode steps: 22, steps per second: 508, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.795, 1.223], mean_best_reward: --
  9598/100000: episode: 297, duration: 0.081s, episode steps: 49, steps per second: 608, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.101 [-0.598, 1.169], mean_best_reward: --
  9668/100000: episode: 298, duration: 0.121s, episode steps: 70, steps per second: 579, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.067 [-0.842, 1.092], mean_best_reward: --
  9693/100000: episode: 299, duration: 0.041s, episode steps: 25, steps per second: 612, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.109 [-0.384, 1.265], mean_best_reward: --
  9761/100000: episode: 300, duration: 0.113s, episode steps: 68, steps per second: 602, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.583, 1.293], mean_best_reward: 106.000000
  9813/100000: episode: 301, duration: 0.083s, episode steps: 52, steps per second: 627, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.495, 1.231], mean_best_reward: --
 10013/100000: episode: 302, duration: 0.342s, episode steps: 200, steps per second: 585, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.101 [-1.125, 0.751], mean_best_reward: --
 10085/100000: episode: 303, duration: 0.125s, episode steps: 72, steps per second: 575, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.252 [-1.323, 0.717], mean_best_reward: --
 10160/100000: episode: 304, duration: 0.126s, episode steps: 75, steps per second: 593, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.199 [-0.767, 0.984], mean_best_reward: --
 10194/100000: episode: 305, duration: 0.054s, episode steps: 34, steps per second: 626, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.098 [-0.424, 0.912], mean_best_reward: --
 10239/100000: episode: 306, duration: 0.081s, episode steps: 45, steps per second: 559, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.107 [-0.553, 0.912], mean_best_reward: --
 10276/100000: episode: 307, duration: 0.059s, episode steps: 37, steps per second: 623, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.098 [-0.527, 1.273], mean_best_reward: --
 10317/100000: episode: 308, duration: 0.069s, episode steps: 41, steps per second: 593, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.009 [-0.796, 1.115], mean_best_reward: --
 10340/100000: episode: 309, duration: 0.041s, episode steps: 23, steps per second: 556, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.112 [-0.385, 1.170], mean_best_reward: --
 10406/100000: episode: 310, duration: 0.108s, episode steps: 66, steps per second: 612, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.171 [-1.630, 1.118], mean_best_reward: --
 10459/100000: episode: 311, duration: 0.083s, episode steps: 53, steps per second: 638, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.068 [-0.721, 1.062], mean_best_reward: --
 10485/100000: episode: 312, duration: 0.040s, episode steps: 26, steps per second: 645, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.089 [-1.071, 0.456], mean_best_reward: --
 10573/100000: episode: 313, duration: 0.135s, episode steps: 88, steps per second: 654, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.091 [-0.899, 0.885], mean_best_reward: --
 10620/100000: episode: 314, duration: 0.077s, episode steps: 47, steps per second: 607, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.098 [-0.588, 0.954], mean_best_reward: --
 10678/100000: episode: 315, duration: 0.099s, episode steps: 58, steps per second: 586, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.528, 1.183], mean_best_reward: --
 10705/100000: episode: 316, duration: 0.046s, episode steps: 27, steps per second: 586, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.080 [-0.564, 1.043], mean_best_reward: --
 10766/100000: episode: 317, duration: 0.092s, episode steps: 61, steps per second: 663, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.296 [-1.520, 0.695], mean_best_reward: --
 10814/100000: episode: 318, duration: 0.076s, episode steps: 48, steps per second: 632, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.156 [-0.698, 1.010], mean_best_reward: --
 10859/100000: episode: 319, duration: 0.071s, episode steps: 45, steps per second: 635, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.133 [-0.357, 0.953], mean_best_reward: --
 10917/100000: episode: 320, duration: 0.090s, episode steps: 58, steps per second: 643, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-1.189, 0.678], mean_best_reward: --
 10938/100000: episode: 321, duration: 0.032s, episode steps: 21, steps per second: 654, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.095 [-0.565, 1.243], mean_best_reward: --
 10991/100000: episode: 322, duration: 0.082s, episode steps: 53, steps per second: 643, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.184 [-0.652, 1.159], mean_best_reward: --
 11055/100000: episode: 323, duration: 0.099s, episode steps: 64, steps per second: 648, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.102 [-0.994, 0.590], mean_best_reward: --
 11082/100000: episode: 324, duration: 0.044s, episode steps: 27, steps per second: 612, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.122 [-0.602, 0.910], mean_best_reward: --
 11126/100000: episode: 325, duration: 0.073s, episode steps: 44, steps per second: 601, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.138 [-0.453, 0.803], mean_best_reward: --
 11186/100000: episode: 326, duration: 0.103s, episode steps: 60, steps per second: 582, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.092 [-1.099, 0.643], mean_best_reward: --
 11256/100000: episode: 327, duration: 0.111s, episode steps: 70, steps per second: 628, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.223 [-0.817, 1.563], mean_best_reward: --
 11360/100000: episode: 328, duration: 0.169s, episode steps: 104, steps per second: 616, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.032 [-1.079, 0.952], mean_best_reward: --
 11386/100000: episode: 329, duration: 0.049s, episode steps: 26, steps per second: 530, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.587, 1.132], mean_best_reward: --
 11461/100000: episode: 330, duration: 0.121s, episode steps: 75, steps per second: 618, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.016 [-0.832, 1.316], mean_best_reward: --
 11516/100000: episode: 331, duration: 0.094s, episode steps: 55, steps per second: 587, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.125 [-0.627, 1.079], mean_best_reward: --
 11569/100000: episode: 332, duration: 0.086s, episode steps: 53, steps per second: 614, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.087 [-0.476, 0.904], mean_best_reward: --
 11638/100000: episode: 333, duration: 0.114s, episode steps: 69, steps per second: 607, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.120 [-0.867, 1.157], mean_best_reward: --
 11679/100000: episode: 334, duration: 0.072s, episode steps: 41, steps per second: 572, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.066 [-0.810, 1.076], mean_best_reward: --
 11737/100000: episode: 335, duration: 0.106s, episode steps: 58, steps per second: 549, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.119 [-0.645, 0.943], mean_best_reward: --
 11771/100000: episode: 336, duration: 0.054s, episode steps: 34, steps per second: 631, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.072 [-0.510, 0.999], mean_best_reward: --
 11863/100000: episode: 337, duration: 0.143s, episode steps: 92, steps per second: 643, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.123 [-0.714, 0.973], mean_best_reward: --
 11926/100000: episode: 338, duration: 0.099s, episode steps: 63, steps per second: 637, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.172 [-0.718, 1.411], mean_best_reward: --
 12034/100000: episode: 339, duration: 0.165s, episode steps: 108, steps per second: 656, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.139 [-0.946, 0.845], mean_best_reward: --
 12071/100000: episode: 340, duration: 0.057s, episode steps: 37, steps per second: 646, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.131 [-0.745, 1.096], mean_best_reward: --
 12133/100000: episode: 341, duration: 0.098s, episode steps: 62, steps per second: 632, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.257 [-0.577, 1.299], mean_best_reward: --
 12199/100000: episode: 342, duration: 0.112s, episode steps: 66, steps per second: 591, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.136 [-0.541, 0.896], mean_best_reward: --
 12279/100000: episode: 343, duration: 0.120s, episode steps: 80, steps per second: 665, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.131 [-1.142, 0.633], mean_best_reward: --
 12316/100000: episode: 344, duration: 0.067s, episode steps: 37, steps per second: 551, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.142 [-0.864, 1.311], mean_best_reward: --
 12365/100000: episode: 345, duration: 0.087s, episode steps: 49, steps per second: 561, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.095 [-0.921, 0.684], mean_best_reward: --
 12479/100000: episode: 346, duration: 0.184s, episode steps: 114, steps per second: 619, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.213 [-1.770, 0.824], mean_best_reward: --
 12495/100000: episode: 347, duration: 0.030s, episode steps: 16, steps per second: 538, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.768, 1.314], mean_best_reward: --
 12553/100000: episode: 348, duration: 0.096s, episode steps: 58, steps per second: 603, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.114 [-0.367, 1.303], mean_best_reward: --
 12633/100000: episode: 349, duration: 0.140s, episode steps: 80, steps per second: 572, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.123 [-0.672, 0.858], mean_best_reward: --
 12709/100000: episode: 350, duration: 0.126s, episode steps: 76, steps per second: 601, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.225 [-1.329, 0.652], mean_best_reward: 98.000000
 12775/100000: episode: 351, duration: 0.109s, episode steps: 66, steps per second: 604, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.148 [-1.118, 0.718], mean_best_reward: --
 12856/100000: episode: 352, duration: 0.130s, episode steps: 81, steps per second: 624, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.173 [-1.359, 0.808], mean_best_reward: --
 12895/100000: episode: 353, duration: 0.067s, episode steps: 39, steps per second: 586, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.099 [-0.736, 1.241], mean_best_reward: --
 12934/100000: episode: 354, duration: 0.069s, episode steps: 39, steps per second: 568, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.106 [-0.737, 1.208], mean_best_reward: --
 13076/100000: episode: 355, duration: 0.247s, episode steps: 142, steps per second: 576, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.078 [-0.975, 0.743], mean_best_reward: --
 13143/100000: episode: 356, duration: 0.118s, episode steps: 67, steps per second: 569, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.118 [-0.425, 1.308], mean_best_reward: --
 13217/100000: episode: 357, duration: 0.126s, episode steps: 74, steps per second: 585, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.469, 1.240], mean_best_reward: --
 13268/100000: episode: 358, duration: 0.088s, episode steps: 51, steps per second: 583, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.160 [-0.558, 0.885], mean_best_reward: --
 13300/100000: episode: 359, duration: 0.055s, episode steps: 32, steps per second: 582, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.110 [-0.559, 0.886], mean_best_reward: --
 13354/100000: episode: 360, duration: 0.094s, episode steps: 54, steps per second: 572, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.125 [-0.543, 0.966], mean_best_reward: --
 13452/100000: episode: 361, duration: 0.168s, episode steps: 98, steps per second: 583, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-0.981, 0.994], mean_best_reward: --
 13553/100000: episode: 362, duration: 0.177s, episode steps: 101, steps per second: 572, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.228 [-1.530, 0.851], mean_best_reward: --
 13635/100000: episode: 363, duration: 0.134s, episode steps: 82, steps per second: 610, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.135 [-0.750, 1.101], mean_best_reward: --
 13673/100000: episode: 364, duration: 0.063s, episode steps: 38, steps per second: 601, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.353, 1.124], mean_best_reward: --
 13706/100000: episode: 365, duration: 0.049s, episode steps: 33, steps per second: 678, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.067 [-0.551, 1.114], mean_best_reward: --
 13766/100000: episode: 366, duration: 0.092s, episode steps: 60, steps per second: 649, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.024 [-0.567, 0.860], mean_best_reward: --
 13802/100000: episode: 367, duration: 0.057s, episode steps: 36, steps per second: 630, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.156 [-0.542, 0.982], mean_best_reward: --
 13886/100000: episode: 368, duration: 0.150s, episode steps: 84, steps per second: 559, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.110 [-0.996, 1.152], mean_best_reward: --
 13956/100000: episode: 369, duration: 0.119s, episode steps: 70, steps per second: 588, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.828, 0.939], mean_best_reward: --
 13999/100000: episode: 370, duration: 0.076s, episode steps: 43, steps per second: 563, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.054 [-0.619, 1.013], mean_best_reward: --
 14037/100000: episode: 371, duration: 0.061s, episode steps: 38, steps per second: 622, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.028 [-0.867, 1.083], mean_best_reward: --
 14095/100000: episode: 372, duration: 0.098s, episode steps: 58, steps per second: 590, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.040 [-0.911, 0.619], mean_best_reward: --
 14204/100000: episode: 373, duration: 0.181s, episode steps: 109, steps per second: 604, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.053 [-0.851, 0.951], mean_best_reward: --
 14256/100000: episode: 374, duration: 0.083s, episode steps: 52, steps per second: 629, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.150 [-0.858, 1.144], mean_best_reward: --
 14280/100000: episode: 375, duration: 0.042s, episode steps: 24, steps per second: 573, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.113 [-0.556, 0.947], mean_best_reward: --
 14309/100000: episode: 376, duration: 0.052s, episode steps: 29, steps per second: 554, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.089 [-0.641, 1.110], mean_best_reward: --
 14341/100000: episode: 377, duration: 0.062s, episode steps: 32, steps per second: 518, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.097 [-0.453, 1.034], mean_best_reward: --
 14365/100000: episode: 378, duration: 0.039s, episode steps: 24, steps per second: 622, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.137 [-0.601, 1.039], mean_best_reward: --
 14396/100000: episode: 379, duration: 0.052s, episode steps: 31, steps per second: 600, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.068 [-0.588, 1.007], mean_best_reward: --
 14472/100000: episode: 380, duration: 0.126s, episode steps: 76, steps per second: 601, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.123 [-0.588, 0.913], mean_best_reward: --
 14517/100000: episode: 381, duration: 0.074s, episode steps: 45, steps per second: 606, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.035 [-0.632, 1.201], mean_best_reward: --
 14550/100000: episode: 382, duration: 0.062s, episode steps: 33, steps per second: 531, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.121 [-0.976, 0.389], mean_best_reward: --
 14571/100000: episode: 383, duration: 0.044s, episode steps: 21, steps per second: 482, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.125 [-1.067, 0.395], mean_best_reward: --
 14659/100000: episode: 384, duration: 0.158s, episode steps: 88, steps per second: 559, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.240 [-1.529, 0.728], mean_best_reward: --
 14680/100000: episode: 385, duration: 0.041s, episode steps: 21, steps per second: 512, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.106 [-1.189, 0.417], mean_best_reward: --
 14765/100000: episode: 386, duration: 0.144s, episode steps: 85, steps per second: 592, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.153 [-0.644, 1.137], mean_best_reward: --
 14784/100000: episode: 387, duration: 0.035s, episode steps: 19, steps per second: 548, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.103 [-0.595, 1.019], mean_best_reward: --
 14848/100000: episode: 388, duration: 0.101s, episode steps: 64, steps per second: 631, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.132 [-0.562, 1.043], mean_best_reward: --
 14955/100000: episode: 389, duration: 0.190s, episode steps: 107, steps per second: 563, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.099 [-0.640, 1.454], mean_best_reward: --
 14980/100000: episode: 390, duration: 0.039s, episode steps: 25, steps per second: 637, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.114 [-1.059, 0.479], mean_best_reward: --
 15065/100000: episode: 391, duration: 0.133s, episode steps: 85, steps per second: 641, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.152 [-0.851, 1.273], mean_best_reward: --
 15112/100000: episode: 392, duration: 0.079s, episode steps: 47, steps per second: 597, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.163 [-0.518, 1.130], mean_best_reward: --
 15136/100000: episode: 393, duration: 0.042s, episode steps: 24, steps per second: 572, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.372, 0.986], mean_best_reward: --
 15157/100000: episode: 394, duration: 0.035s, episode steps: 21, steps per second: 606, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.068 [-0.826, 1.408], mean_best_reward: --
 15205/100000: episode: 395, duration: 0.078s, episode steps: 48, steps per second: 613, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.251 [-1.475, 0.579], mean_best_reward: --
 15247/100000: episode: 396, duration: 0.066s, episode steps: 42, steps per second: 633, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.117 [-0.645, 0.988], mean_best_reward: --
 15356/100000: episode: 397, duration: 0.181s, episode steps: 109, steps per second: 602, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.044 [-0.663, 0.961], mean_best_reward: --
 15476/100000: episode: 398, duration: 0.207s, episode steps: 120, steps per second: 580, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.108 [-1.225, 1.282], mean_best_reward: --
 15508/100000: episode: 399, duration: 0.058s, episode steps: 32, steps per second: 556, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.128 [-0.637, 0.929], mean_best_reward: --
 15542/100000: episode: 400, duration: 0.060s, episode steps: 34, steps per second: 563, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.056 [-0.761, 1.132], mean_best_reward: 96.000000
 15628/100000: episode: 401, duration: 0.145s, episode steps: 86, steps per second: 592, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.113 [-0.572, 1.064], mean_best_reward: --
 15704/100000: episode: 402, duration: 0.126s, episode steps: 76, steps per second: 602, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.185 [-1.034, 0.752], mean_best_reward: --
 15751/100000: episode: 403, duration: 0.083s, episode steps: 47, steps per second: 565, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.204 [-0.579, 1.122], mean_best_reward: --
 15822/100000: episode: 404, duration: 0.118s, episode steps: 71, steps per second: 603, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.134 [-1.049, 0.663], mean_best_reward: --
 15895/100000: episode: 405, duration: 0.125s, episode steps: 73, steps per second: 583, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.158 [-0.597, 1.312], mean_best_reward: --
 15958/100000: episode: 406, duration: 0.108s, episode steps: 63, steps per second: 585, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.148 [-0.792, 1.104], mean_best_reward: --
 15984/100000: episode: 407, duration: 0.051s, episode steps: 26, steps per second: 509, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.122 [-1.207, 0.384], mean_best_reward: --
 16036/100000: episode: 408, duration: 0.095s, episode steps: 52, steps per second: 546, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.590, 0.980], mean_best_reward: --
 16091/100000: episode: 409, duration: 0.092s, episode steps: 55, steps per second: 595, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.114 [-0.951, 0.558], mean_best_reward: --
 16131/100000: episode: 410, duration: 0.068s, episode steps: 40, steps per second: 588, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.031 [-0.636, 1.384], mean_best_reward: --
 16247/100000: episode: 411, duration: 0.189s, episode steps: 116, steps per second: 614, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.109 [-1.103, 0.861], mean_best_reward: --
 16300/100000: episode: 412, duration: 0.094s, episode steps: 53, steps per second: 567, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.185 [-0.562, 1.126], mean_best_reward: --
 16353/100000: episode: 413, duration: 0.090s, episode steps: 53, steps per second: 591, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.050 [-0.748, 1.027], mean_best_reward: --
 16371/100000: episode: 414, duration: 0.033s, episode steps: 18, steps per second: 549, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.547, 0.999], mean_best_reward: --
 16419/100000: episode: 415, duration: 0.081s, episode steps: 48, steps per second: 589, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.062 [-0.537, 0.877], mean_best_reward: --
 16469/100000: episode: 416, duration: 0.085s, episode steps: 50, steps per second: 591, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.069 [-0.600, 0.934], mean_best_reward: --
 16514/100000: episode: 417, duration: 0.076s, episode steps: 45, steps per second: 588, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.206 [-0.570, 1.499], mean_best_reward: --
 16555/100000: episode: 418, duration: 0.069s, episode steps: 41, steps per second: 594, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.111 [-0.710, 0.873], mean_best_reward: --
 16604/100000: episode: 419, duration: 0.076s, episode steps: 49, steps per second: 644, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.129 [-0.521, 0.809], mean_best_reward: --
 16718/100000: episode: 420, duration: 0.183s, episode steps: 114, steps per second: 622, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-0.846, 1.419], mean_best_reward: --
 16760/100000: episode: 421, duration: 0.068s, episode steps: 42, steps per second: 614, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.148 [-0.603, 0.973], mean_best_reward: --
 16825/100000: episode: 422, duration: 0.110s, episode steps: 65, steps per second: 592, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.028 [-0.848, 1.115], mean_best_reward: --
 16955/100000: episode: 423, duration: 0.213s, episode steps: 130, steps per second: 610, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.103 [-0.790, 1.286], mean_best_reward: --
 16989/100000: episode: 424, duration: 0.053s, episode steps: 34, steps per second: 645, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.423, 0.907], mean_best_reward: --
 17134/100000: episode: 425, duration: 0.229s, episode steps: 145, steps per second: 632, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.159 [-1.320, 1.079], mean_best_reward: --
 17305/100000: episode: 426, duration: 0.271s, episode steps: 171, steps per second: 630, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.135 [-0.962, 0.806], mean_best_reward: --
 17356/100000: episode: 427, duration: 0.089s, episode steps: 51, steps per second: 576, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.116 [-0.445, 1.009], mean_best_reward: --
 17375/100000: episode: 428, duration: 0.032s, episode steps: 19, steps per second: 594, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.105 [-0.596, 1.250], mean_best_reward: --
 17424/100000: episode: 429, duration: 0.073s, episode steps: 49, steps per second: 667, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.178 [-0.628, 1.341], mean_best_reward: --
 17497/100000: episode: 430, duration: 0.110s, episode steps: 73, steps per second: 662, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.029 [-1.051, 1.189], mean_best_reward: --
 17550/100000: episode: 431, duration: 0.098s, episode steps: 53, steps per second: 543, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.093 [-0.870, 0.576], mean_best_reward: --
 17601/100000: episode: 432, duration: 0.089s, episode steps: 51, steps per second: 573, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.165 [-0.808, 1.272], mean_best_reward: --
 17694/100000: episode: 433, duration: 0.152s, episode steps: 93, steps per second: 613, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.020 [-1.157, 1.126], mean_best_reward: --
 17720/100000: episode: 434, duration: 0.044s, episode steps: 26, steps per second: 596, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.101 [-1.229, 0.567], mean_best_reward: --
 17762/100000: episode: 435, duration: 0.067s, episode steps: 42, steps per second: 625, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.183 [-0.757, 0.951], mean_best_reward: --
 17793/100000: episode: 436, duration: 0.051s, episode steps: 31, steps per second: 607, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.024 [-0.771, 1.047], mean_best_reward: --
 17890/100000: episode: 437, duration: 0.151s, episode steps: 97, steps per second: 641, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.152 [-0.895, 1.679], mean_best_reward: --
 17972/100000: episode: 438, duration: 0.136s, episode steps: 82, steps per second: 605, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.196 [-0.729, 1.139], mean_best_reward: --
 18042/100000: episode: 439, duration: 0.107s, episode steps: 70, steps per second: 654, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.190 [-0.661, 1.699], mean_best_reward: --
 18106/100000: episode: 440, duration: 0.104s, episode steps: 64, steps per second: 613, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.220 [-1.504, 0.781], mean_best_reward: --
 18188/100000: episode: 441, duration: 0.133s, episode steps: 82, steps per second: 615, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.100 [-0.898, 1.093], mean_best_reward: --
 18239/100000: episode: 442, duration: 0.079s, episode steps: 51, steps per second: 646, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.098 [-0.902, 0.546], mean_best_reward: --
 18358/100000: episode: 443, duration: 0.194s, episode steps: 119, steps per second: 612, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.088 [-0.966, 1.326], mean_best_reward: --
 18405/100000: episode: 444, duration: 0.076s, episode steps: 47, steps per second: 616, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.042 [-0.904, 1.026], mean_best_reward: --
 18460/100000: episode: 445, duration: 0.092s, episode steps: 55, steps per second: 597, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.180 [-0.588, 0.982], mean_best_reward: --
 18512/100000: episode: 446, duration: 0.084s, episode steps: 52, steps per second: 619, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.120 [-0.636, 0.873], mean_best_reward: --
 18542/100000: episode: 447, duration: 0.052s, episode steps: 30, steps per second: 574, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.082, 0.610], mean_best_reward: --
 18606/100000: episode: 448, duration: 0.111s, episode steps: 64, steps per second: 578, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.158 [-0.492, 0.944], mean_best_reward: --
 18638/100000: episode: 449, duration: 0.060s, episode steps: 32, steps per second: 531, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.543, 1.135], mean_best_reward: --
 18693/100000: episode: 450, duration: 0.095s, episode steps: 55, steps per second: 577, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.137 [-1.072, 0.472], mean_best_reward: 185.500000
 18765/100000: episode: 451, duration: 0.115s, episode steps: 72, steps per second: 624, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.232 [-1.619, 0.625], mean_best_reward: --
 18801/100000: episode: 452, duration: 0.060s, episode steps: 36, steps per second: 595, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.137 [-0.990, 0.566], mean_best_reward: --
 18842/100000: episode: 453, duration: 0.065s, episode steps: 41, steps per second: 633, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.051 [-0.878, 0.617], mean_best_reward: --
 18871/100000: episode: 454, duration: 0.047s, episode steps: 29, steps per second: 619, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.065 [-0.714, 1.197], mean_best_reward: --
 18932/100000: episode: 455, duration: 0.105s, episode steps: 61, steps per second: 582, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.183 [-0.425, 1.160], mean_best_reward: --
 18966/100000: episode: 456, duration: 0.061s, episode steps: 34, steps per second: 553, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.605, 1.018], mean_best_reward: --
 18984/100000: episode: 457, duration: 0.034s, episode steps: 18, steps per second: 528, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.095 [-0.623, 1.017], mean_best_reward: --
 19028/100000: episode: 458, duration: 0.078s, episode steps: 44, steps per second: 563, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.106 [-0.638, 1.239], mean_best_reward: --
 19083/100000: episode: 459, duration: 0.099s, episode steps: 55, steps per second: 558, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.268 [-1.666, 0.755], mean_best_reward: --
 19124/100000: episode: 460, duration: 0.074s, episode steps: 41, steps per second: 551, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.027 [-0.812, 1.119], mean_best_reward: --
 19160/100000: episode: 461, duration: 0.064s, episode steps: 36, steps per second: 563, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.192 [-0.973, 0.518], mean_best_reward: --
 19191/100000: episode: 462, duration: 0.054s, episode steps: 31, steps per second: 574, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.142 [-0.403, 1.240], mean_best_reward: --
 19223/100000: episode: 463, duration: 0.057s, episode steps: 32, steps per second: 560, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.065, 0.558], mean_best_reward: --
 19286/100000: episode: 464, duration: 0.106s, episode steps: 63, steps per second: 592, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.070 [-0.663, 1.472], mean_best_reward: --
 19361/100000: episode: 465, duration: 0.111s, episode steps: 75, steps per second: 674, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.102 [-0.578, 0.941], mean_best_reward: --
 19418/100000: episode: 466, duration: 0.094s, episode steps: 57, steps per second: 607, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.048 [-0.555, 0.929], mean_best_reward: --
 19493/100000: episode: 467, duration: 0.118s, episode steps: 75, steps per second: 637, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.113 [-0.924, 1.036], mean_best_reward: --
 19580/100000: episode: 468, duration: 0.152s, episode steps: 87, steps per second: 573, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.031 [-0.751, 1.113], mean_best_reward: --
 19669/100000: episode: 469, duration: 0.155s, episode steps: 89, steps per second: 573, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.184 [-1.110, 0.914], mean_best_reward: --
 19749/100000: episode: 470, duration: 0.136s, episode steps: 80, steps per second: 588, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.039 [-0.618, 0.887], mean_best_reward: --
 19785/100000: episode: 471, duration: 0.063s, episode steps: 36, steps per second: 574, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.122 [-0.444, 1.611], mean_best_reward: --
 19844/100000: episode: 472, duration: 0.094s, episode steps: 59, steps per second: 625, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.090 [-0.539, 0.953], mean_best_reward: --
 19925/100000: episode: 473, duration: 0.141s, episode steps: 81, steps per second: 575, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.169 [-1.286, 0.586], mean_best_reward: --
 19979/100000: episode: 474, duration: 0.100s, episode steps: 54, steps per second: 542, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.592, 1.218], mean_best_reward: --
 20046/100000: episode: 475, duration: 0.106s, episode steps: 67, steps per second: 632, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.151 [-0.841, 0.601], mean_best_reward: --
 20106/100000: episode: 476, duration: 0.092s, episode steps: 60, steps per second: 650, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.128 [-0.598, 1.254], mean_best_reward: --
 20149/100000: episode: 477, duration: 0.065s, episode steps: 43, steps per second: 666, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.134 [-0.344, 1.033], mean_best_reward: --
 20220/100000: episode: 478, duration: 0.107s, episode steps: 71, steps per second: 661, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.122 [-0.486, 1.107], mean_best_reward: --
 20301/100000: episode: 479, duration: 0.125s, episode steps: 81, steps per second: 647, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.081 [-1.260, 0.823], mean_best_reward: --
 20397/100000: episode: 480, duration: 0.160s, episode steps: 96, steps per second: 598, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.169 [-0.885, 1.071], mean_best_reward: --
 20440/100000: episode: 481, duration: 0.073s, episode steps: 43, steps per second: 590, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.161 [-0.660, 1.110], mean_best_reward: --
 20478/100000: episode: 482, duration: 0.064s, episode steps: 38, steps per second: 595, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.108 [-0.455, 1.168], mean_best_reward: --
 20527/100000: episode: 483, duration: 0.086s, episode steps: 49, steps per second: 572, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.195 [-1.136, 0.409], mean_best_reward: --
 20572/100000: episode: 484, duration: 0.074s, episode steps: 45, steps per second: 607, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.063 [-0.549, 0.967], mean_best_reward: --
 20597/100000: episode: 485, duration: 0.040s, episode steps: 25, steps per second: 625, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.119 [-0.602, 1.056], mean_best_reward: --
 20752/100000: episode: 486, duration: 0.235s, episode steps: 155, steps per second: 660, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.183 [-0.996, 0.755], mean_best_reward: --
 20838/100000: episode: 487, duration: 0.134s, episode steps: 86, steps per second: 644, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.096 [-0.838, 1.124], mean_best_reward: --
 20884/100000: episode: 488, duration: 0.078s, episode steps: 46, steps per second: 586, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-0.534, 1.270], mean_best_reward: --
 20981/100000: episode: 489, duration: 0.154s, episode steps: 97, steps per second: 631, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.123 [-1.169, 0.852], mean_best_reward: --
 21055/100000: episode: 490, duration: 0.113s, episode steps: 74, steps per second: 654, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.253 [-1.669, 0.843], mean_best_reward: --
 21167/100000: episode: 491, duration: 0.178s, episode steps: 112, steps per second: 629, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.144 [-1.034, 0.870], mean_best_reward: --
 21251/100000: episode: 492, duration: 0.129s, episode steps: 84, steps per second: 649, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.075 [-0.967, 1.178], mean_best_reward: --
 21297/100000: episode: 493, duration: 0.070s, episode steps: 46, steps per second: 655, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-1.295, 0.836], mean_best_reward: --
 21321/100000: episode: 494, duration: 0.039s, episode steps: 24, steps per second: 618, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.552, 0.993], mean_best_reward: --
 21378/100000: episode: 495, duration: 0.090s, episode steps: 57, steps per second: 632, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.103 [-0.915, 0.669], mean_best_reward: --
 21458/100000: episode: 496, duration: 0.123s, episode steps: 80, steps per second: 652, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.021 [-0.905, 0.937], mean_best_reward: --
 21499/100000: episode: 497, duration: 0.067s, episode steps: 41, steps per second: 614, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.133 [-0.433, 0.928], mean_best_reward: --
 21536/100000: episode: 498, duration: 0.069s, episode steps: 37, steps per second: 535, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.084 [-0.430, 0.867], mean_best_reward: --
 21607/100000: episode: 499, duration: 0.119s, episode steps: 71, steps per second: 596, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.084 [-0.592, 0.915], mean_best_reward: --
 21665/100000: episode: 500, duration: 0.098s, episode steps: 58, steps per second: 592, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.133 [-0.656, 1.092], mean_best_reward: 134.000000
 21715/100000: episode: 501, duration: 0.076s, episode steps: 50, steps per second: 654, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.580, 1.365], mean_best_reward: --
 21759/100000: episode: 502, duration: 0.069s, episode steps: 44, steps per second: 642, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.769, 1.685], mean_best_reward: --
 21838/100000: episode: 503, duration: 0.133s, episode steps: 79, steps per second: 593, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.166 [-1.077, 0.572], mean_best_reward: --
 21896/100000: episode: 504, duration: 0.092s, episode steps: 58, steps per second: 630, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.194 [-1.195, 0.669], mean_best_reward: --
 21968/100000: episode: 505, duration: 0.111s, episode steps: 72, steps per second: 648, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.062 [-0.590, 0.949], mean_best_reward: --
 21998/100000: episode: 506, duration: 0.047s, episode steps: 30, steps per second: 644, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.061, 0.420], mean_best_reward: --
 22040/100000: episode: 507, duration: 0.062s, episode steps: 42, steps per second: 673, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.148 [-0.471, 1.083], mean_best_reward: --
 22094/100000: episode: 508, duration: 0.083s, episode steps: 54, steps per second: 648, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.058 [-0.838, 0.539], mean_best_reward: --
 22138/100000: episode: 509, duration: 0.066s, episode steps: 44, steps per second: 662, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.103 [-0.359, 0.775], mean_best_reward: --
 22312/100000: episode: 510, duration: 0.287s, episode steps: 174, steps per second: 607, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-1.217, 1.342], mean_best_reward: --
 22368/100000: episode: 511, duration: 0.100s, episode steps: 56, steps per second: 558, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.053 [-0.861, 1.112], mean_best_reward: --
 22421/100000: episode: 512, duration: 0.094s, episode steps: 53, steps per second: 566, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.229 [-0.597, 1.457], mean_best_reward: --
 22470/100000: episode: 513, duration: 0.081s, episode steps: 49, steps per second: 604, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.157 [-0.451, 1.016], mean_best_reward: --
 22506/100000: episode: 514, duration: 0.056s, episode steps: 36, steps per second: 643, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.652, 1.354], mean_best_reward: --
 22592/100000: episode: 515, duration: 0.144s, episode steps: 86, steps per second: 598, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.767, 1.297], mean_best_reward: --
 22642/100000: episode: 516, duration: 0.074s, episode steps: 50, steps per second: 675, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.065 [-0.575, 0.929], mean_best_reward: --
 22706/100000: episode: 517, duration: 0.104s, episode steps: 64, steps per second: 614, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.084 [-0.633, 1.082], mean_best_reward: --
 22785/100000: episode: 518, duration: 0.131s, episode steps: 79, steps per second: 601, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.160 [-0.680, 1.112], mean_best_reward: --
 22828/100000: episode: 519, duration: 0.063s, episode steps: 43, steps per second: 685, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.156 [-1.117, 0.546], mean_best_reward: --
 22860/100000: episode: 520, duration: 0.049s, episode steps: 32, steps per second: 653, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.133 [-0.966, 0.543], mean_best_reward: --
 22923/100000: episode: 521, duration: 0.109s, episode steps: 63, steps per second: 578, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.109 [-0.733, 1.120], mean_best_reward: --
 22991/100000: episode: 522, duration: 0.115s, episode steps: 68, steps per second: 591, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.041 [-0.588, 0.946], mean_best_reward: --
 23020/100000: episode: 523, duration: 0.050s, episode steps: 29, steps per second: 579, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.073 [-0.748, 1.169], mean_best_reward: --
 23089/100000: episode: 524, duration: 0.106s, episode steps: 69, steps per second: 649, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.173 [-0.726, 1.140], mean_best_reward: --
 23147/100000: episode: 525, duration: 0.091s, episode steps: 58, steps per second: 637, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.075 [-0.554, 0.715], mean_best_reward: --
 23167/100000: episode: 526, duration: 0.032s, episode steps: 20, steps per second: 621, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.753, 1.371], mean_best_reward: --
 23229/100000: episode: 527, duration: 0.102s, episode steps: 62, steps per second: 608, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.155 [-0.598, 1.265], mean_best_reward: --
 23246/100000: episode: 528, duration: 0.027s, episode steps: 17, steps per second: 619, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.075 [-0.640, 1.196], mean_best_reward: --
 23291/100000: episode: 529, duration: 0.065s, episode steps: 45, steps per second: 691, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.112 [-0.563, 0.950], mean_best_reward: --
 23352/100000: episode: 530, duration: 0.095s, episode steps: 61, steps per second: 640, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.139 [-0.550, 1.050], mean_best_reward: --
 23378/100000: episode: 531, duration: 0.047s, episode steps: 26, steps per second: 554, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.765, 1.079], mean_best_reward: --
 23440/100000: episode: 532, duration: 0.096s, episode steps: 62, steps per second: 647, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.219 [-1.158, 0.638], mean_best_reward: --
 23468/100000: episode: 533, duration: 0.054s, episode steps: 28, steps per second: 522, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.086 [-0.888, 0.544], mean_best_reward: --
 23542/100000: episode: 534, duration: 0.125s, episode steps: 74, steps per second: 592, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.070 [-0.737, 1.053], mean_best_reward: --
 23640/100000: episode: 535, duration: 0.162s, episode steps: 98, steps per second: 604, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.172 [-1.260, 0.624], mean_best_reward: --
 23688/100000: episode: 536, duration: 0.088s, episode steps: 48, steps per second: 547, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.202 [-1.121, 0.543], mean_best_reward: --
 23783/100000: episode: 537, duration: 0.162s, episode steps: 95, steps per second: 586, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.082 [-1.338, 0.780], mean_best_reward: --
 23819/100000: episode: 538, duration: 0.066s, episode steps: 36, steps per second: 548, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.776, 1.148], mean_best_reward: --
 23886/100000: episode: 539, duration: 0.105s, episode steps: 67, steps per second: 639, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.057 [-0.989, 0.563], mean_best_reward: --
 23936/100000: episode: 540, duration: 0.092s, episode steps: 50, steps per second: 545, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.180 [-0.638, 1.241], mean_best_reward: --
 23957/100000: episode: 541, duration: 0.033s, episode steps: 21, steps per second: 637, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.075 [-0.756, 1.260], mean_best_reward: --
 23995/100000: episode: 542, duration: 0.063s, episode steps: 38, steps per second: 603, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.129 [-1.070, 0.361], mean_best_reward: --
 24062/100000: episode: 543, duration: 0.120s, episode steps: 67, steps per second: 557, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.083 [-0.615, 0.970], mean_best_reward: --
 24129/100000: episode: 544, duration: 0.113s, episode steps: 67, steps per second: 593, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.169 [-0.811, 1.165], mean_best_reward: --
 24298/100000: episode: 545, duration: 0.272s, episode steps: 169, steps per second: 621, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.144 [-1.228, 0.742], mean_best_reward: --
 24407/100000: episode: 546, duration: 0.170s, episode steps: 109, steps per second: 640, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.026 [-0.876, 0.941], mean_best_reward: --
 24439/100000: episode: 547, duration: 0.058s, episode steps: 32, steps per second: 548, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.552, 1.245], mean_best_reward: --
 24483/100000: episode: 548, duration: 0.076s, episode steps: 44, steps per second: 582, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.145 [-0.388, 0.737], mean_best_reward: --
 24557/100000: episode: 549, duration: 0.106s, episode steps: 74, steps per second: 699, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.108 [-0.820, 1.102], mean_best_reward: --
 24583/100000: episode: 550, duration: 0.041s, episode steps: 26, steps per second: 634, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.595, 0.961], mean_best_reward: 139.000000
 24630/100000: episode: 551, duration: 0.073s, episode steps: 47, steps per second: 642, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.109 [-1.000, 0.609], mean_best_reward: --
 24682/100000: episode: 552, duration: 0.083s, episode steps: 52, steps per second: 624, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.787, 1.075], mean_best_reward: --
 24761/100000: episode: 553, duration: 0.119s, episode steps: 79, steps per second: 666, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.112 [-0.702, 1.135], mean_best_reward: --
 24886/100000: episode: 554, duration: 0.217s, episode steps: 125, steps per second: 576, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.074 [-0.848, 1.072], mean_best_reward: --
 24901/100000: episode: 555, duration: 0.024s, episode steps: 15, steps per second: 618, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.092 [-1.152, 0.606], mean_best_reward: --
 24973/100000: episode: 556, duration: 0.106s, episode steps: 72, steps per second: 677, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.051 [-0.956, 1.596], mean_best_reward: --
 25023/100000: episode: 557, duration: 0.075s, episode steps: 50, steps per second: 669, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.130 [-0.684, 1.402], mean_best_reward: --
 25113/100000: episode: 558, duration: 0.148s, episode steps: 90, steps per second: 609, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.267 [-1.091, 2.016], mean_best_reward: --
 25273/100000: episode: 559, duration: 0.263s, episode steps: 160, steps per second: 609, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.079 [-1.288, 1.021], mean_best_reward: --
 25337/100000: episode: 560, duration: 0.111s, episode steps: 64, steps per second: 576, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.134 [-0.668, 1.335], mean_best_reward: --
 25383/100000: episode: 561, duration: 0.068s, episode steps: 46, steps per second: 678, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.090 [-0.379, 0.848], mean_best_reward: --
 25528/100000: episode: 562, duration: 0.219s, episode steps: 145, steps per second: 662, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.061 [-0.955, 0.835], mean_best_reward: --
 25557/100000: episode: 563, duration: 0.055s, episode steps: 29, steps per second: 527, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.132 [-0.540, 0.906], mean_best_reward: --
 25663/100000: episode: 564, duration: 0.156s, episode steps: 106, steps per second: 678, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.148 [-1.324, 0.857], mean_best_reward: --
 25799/100000: episode: 565, duration: 0.200s, episode steps: 136, steps per second: 680, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.068 [-0.851, 1.511], mean_best_reward: --
 25859/100000: episode: 566, duration: 0.107s, episode steps: 60, steps per second: 560, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.123 [-0.628, 1.119], mean_best_reward: --
 25888/100000: episode: 567, duration: 0.048s, episode steps: 29, steps per second: 603, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.127 [-0.427, 0.815], mean_best_reward: --
 25939/100000: episode: 568, duration: 0.088s, episode steps: 51, steps per second: 578, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.184 [-1.084, 0.857], mean_best_reward: --
 26010/100000: episode: 569, duration: 0.122s, episode steps: 71, steps per second: 584, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.082 [-1.361, 0.631], mean_best_reward: --
 26114/100000: episode: 570, duration: 0.177s, episode steps: 104, steps per second: 586, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.044 [-0.619, 0.835], mean_best_reward: --
 26171/100000: episode: 571, duration: 0.083s, episode steps: 57, steps per second: 686, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.097 [-0.605, 1.169], mean_best_reward: --
 26272/100000: episode: 572, duration: 0.170s, episode steps: 101, steps per second: 595, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.057 [-0.799, 1.341], mean_best_reward: --
 26340/100000: episode: 573, duration: 0.109s, episode steps: 68, steps per second: 625, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.107 [-0.570, 0.931], mean_best_reward: --
 26391/100000: episode: 574, duration: 0.076s, episode steps: 51, steps per second: 671, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.100 [-0.561, 0.890], mean_best_reward: --
 26414/100000: episode: 575, duration: 0.036s, episode steps: 23, steps per second: 640, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.078 [-0.559, 1.126], mean_best_reward: --
 26468/100000: episode: 576, duration: 0.088s, episode steps: 54, steps per second: 616, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.138 [-0.457, 0.957], mean_best_reward: --
 26535/100000: episode: 577, duration: 0.119s, episode steps: 67, steps per second: 564, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.249 [-1.337, 0.605], mean_best_reward: --
 26606/100000: episode: 578, duration: 0.122s, episode steps: 71, steps per second: 582, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.270 [-1.716, 0.647], mean_best_reward: --
 26656/100000: episode: 579, duration: 0.087s, episode steps: 50, steps per second: 577, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.208 [-1.263, 0.608], mean_best_reward: --
 26680/100000: episode: 580, duration: 0.043s, episode steps: 24, steps per second: 561, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.393, 1.014], mean_best_reward: --
 26719/100000: episode: 581, duration: 0.071s, episode steps: 39, steps per second: 551, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.094 [-0.976, 0.465], mean_best_reward: --
 26750/100000: episode: 582, duration: 0.051s, episode steps: 31, steps per second: 605, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.104 [-0.375, 1.023], mean_best_reward: --
 26799/100000: episode: 583, duration: 0.073s, episode steps: 49, steps per second: 672, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.105 [-0.629, 0.897], mean_best_reward: --
 26825/100000: episode: 584, duration: 0.039s, episode steps: 26, steps per second: 665, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.585, 1.344], mean_best_reward: --
 26891/100000: episode: 585, duration: 0.101s, episode steps: 66, steps per second: 653, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.021 [-0.913, 1.103], mean_best_reward: --
 26939/100000: episode: 586, duration: 0.076s, episode steps: 48, steps per second: 634, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.940, 1.305], mean_best_reward: --
 26978/100000: episode: 587, duration: 0.058s, episode steps: 39, steps per second: 669, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.167 [-1.178, 0.754], mean_best_reward: --
 27035/100000: episode: 588, duration: 0.087s, episode steps: 57, steps per second: 655, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.167 [-0.427, 1.258], mean_best_reward: --
 27068/100000: episode: 589, duration: 0.041s, episode steps: 33, steps per second: 808, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.057 [-0.635, 0.970], mean_best_reward: --
 27162/100000: episode: 590, duration: 0.126s, episode steps: 94, steps per second: 745, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.095 [-0.741, 0.969], mean_best_reward: --
 27249/100000: episode: 591, duration: 0.112s, episode steps: 87, steps per second: 780, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.082 [-0.801, 1.107], mean_best_reward: --
 27290/100000: episode: 592, duration: 0.050s, episode steps: 41, steps per second: 823, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.098 [-0.783, 1.114], mean_best_reward: --
 27345/100000: episode: 593, duration: 0.067s, episode steps: 55, steps per second: 822, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.070 [-0.544, 1.058], mean_best_reward: --
 27403/100000: episode: 594, duration: 0.084s, episode steps: 58, steps per second: 691, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.115 [-0.720, 1.124], mean_best_reward: --
 27457/100000: episode: 595, duration: 0.083s, episode steps: 54, steps per second: 652, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.192 [-1.138, 0.485], mean_best_reward: --
 27540/100000: episode: 596, duration: 0.125s, episode steps: 83, steps per second: 662, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.167 [-1.090, 0.955], mean_best_reward: --
 27589/100000: episode: 597, duration: 0.074s, episode steps: 49, steps per second: 659, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.117 [-0.783, 0.899], mean_best_reward: --
 27658/100000: episode: 598, duration: 0.114s, episode steps: 69, steps per second: 603, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.248 [-1.448, 0.820], mean_best_reward: --
 27733/100000: episode: 599, duration: 0.127s, episode steps: 75, steps per second: 592, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.162 [-1.140, 1.047], mean_best_reward: --
 27776/100000: episode: 600, duration: 0.076s, episode steps: 43, steps per second: 569, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.147 [-0.402, 1.073], mean_best_reward: 128.000000
 27827/100000: episode: 601, duration: 0.080s, episode steps: 51, steps per second: 641, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.100 [-1.046, 0.560], mean_best_reward: --
 27946/100000: episode: 602, duration: 0.194s, episode steps: 119, steps per second: 613, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.143 [-1.129, 0.856], mean_best_reward: --
 28031/100000: episode: 603, duration: 0.139s, episode steps: 85, steps per second: 613, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.105 [-0.842, 1.085], mean_best_reward: --
 28149/100000: episode: 604, duration: 0.201s, episode steps: 118, steps per second: 587, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.192 [-1.222, 1.461], mean_best_reward: --
 28240/100000: episode: 605, duration: 0.153s, episode steps: 91, steps per second: 594, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.157 [-0.741, 1.152], mean_best_reward: --
 28342/100000: episode: 606, duration: 0.171s, episode steps: 102, steps per second: 597, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.189 [-1.314, 0.744], mean_best_reward: --
 28358/100000: episode: 607, duration: 0.030s, episode steps: 16, steps per second: 539, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.096 [-0.582, 1.236], mean_best_reward: --
 28413/100000: episode: 608, duration: 0.088s, episode steps: 55, steps per second: 626, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.108 [-0.418, 0.910], mean_best_reward: --
 28457/100000: episode: 609, duration: 0.074s, episode steps: 44, steps per second: 596, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.088 [-0.676, 0.862], mean_best_reward: --
 28546/100000: episode: 610, duration: 0.135s, episode steps: 89, steps per second: 658, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.152 [-1.159, 0.820], mean_best_reward: --
 28573/100000: episode: 611, duration: 0.042s, episode steps: 27, steps per second: 646, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.117 [-0.443, 1.025], mean_best_reward: --
 28617/100000: episode: 612, duration: 0.076s, episode steps: 44, steps per second: 579, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.032 [-0.564, 1.038], mean_best_reward: --
 28666/100000: episode: 613, duration: 0.081s, episode steps: 49, steps per second: 605, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.184 [-1.153, 0.635], mean_best_reward: --
 28690/100000: episode: 614, duration: 0.041s, episode steps: 24, steps per second: 581, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.103 [-0.443, 0.963], mean_best_reward: --
 28764/100000: episode: 615, duration: 0.123s, episode steps: 74, steps per second: 600, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.246 [-0.681, 1.502], mean_best_reward: --
 28810/100000: episode: 616, duration: 0.071s, episode steps: 46, steps per second: 649, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.155 [-0.502, 1.142], mean_best_reward: --
 28864/100000: episode: 617, duration: 0.099s, episode steps: 54, steps per second: 546, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.126 [-0.754, 1.123], mean_best_reward: --
 28982/100000: episode: 618, duration: 0.204s, episode steps: 118, steps per second: 577, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.072 [-0.785, 1.127], mean_best_reward: --
 29019/100000: episode: 619, duration: 0.058s, episode steps: 37, steps per second: 638, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.079 [-0.633, 1.168], mean_best_reward: --
 29076/100000: episode: 620, duration: 0.096s, episode steps: 57, steps per second: 593, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.036 [-1.622, 0.620], mean_best_reward: --
 29103/100000: episode: 621, duration: 0.049s, episode steps: 27, steps per second: 556, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.828, 1.169], mean_best_reward: --
 29177/100000: episode: 622, duration: 0.121s, episode steps: 74, steps per second: 611, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.166 [-0.613, 1.167], mean_best_reward: --
 29301/100000: episode: 623, duration: 0.197s, episode steps: 124, steps per second: 631, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.023 [-0.692, 0.989], mean_best_reward: --
 29363/100000: episode: 624, duration: 0.105s, episode steps: 62, steps per second: 589, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.179 [-0.566, 1.111], mean_best_reward: --
 29422/100000: episode: 625, duration: 0.100s, episode steps: 59, steps per second: 592, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.059 [-0.626, 1.081], mean_best_reward: --
 29562/100000: episode: 626, duration: 0.239s, episode steps: 140, steps per second: 587, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.194 [-1.162, 1.013], mean_best_reward: --
 29635/100000: episode: 627, duration: 0.128s, episode steps: 73, steps per second: 569, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.072 [-0.900, 0.719], mean_best_reward: --
 29792/100000: episode: 628, duration: 0.240s, episode steps: 157, steps per second: 653, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.143 [-1.251, 1.003], mean_best_reward: --
 29874/100000: episode: 629, duration: 0.123s, episode steps: 82, steps per second: 666, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.120 [-0.811, 1.097], mean_best_reward: --
 29921/100000: episode: 630, duration: 0.084s, episode steps: 47, steps per second: 563, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.168 [-1.136, 0.567], mean_best_reward: --
 30004/100000: episode: 631, duration: 0.140s, episode steps: 83, steps per second: 594, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.100 [-1.022, 0.614], mean_best_reward: --
 30096/100000: episode: 632, duration: 0.147s, episode steps: 92, steps per second: 627, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.084 [-0.613, 1.035], mean_best_reward: --
 30150/100000: episode: 633, duration: 0.092s, episode steps: 54, steps per second: 586, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.045 [-0.420, 0.759], mean_best_reward: --
 30230/100000: episode: 634, duration: 0.133s, episode steps: 80, steps per second: 599, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.075 [-0.952, 1.034], mean_best_reward: --
 30255/100000: episode: 635, duration: 0.044s, episode steps: 25, steps per second: 569, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.063 [-0.761, 1.245], mean_best_reward: --
 30333/100000: episode: 636, duration: 0.133s, episode steps: 78, steps per second: 586, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.184, 0.511], mean_best_reward: --
 30417/100000: episode: 637, duration: 0.125s, episode steps: 84, steps per second: 670, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.134 [-0.704, 1.086], mean_best_reward: --
 30460/100000: episode: 638, duration: 0.067s, episode steps: 43, steps per second: 646, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.109 [-0.618, 0.923], mean_best_reward: --
 30539/100000: episode: 639, duration: 0.124s, episode steps: 79, steps per second: 639, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.110 [-0.576, 0.943], mean_best_reward: --
 30648/100000: episode: 640, duration: 0.193s, episode steps: 109, steps per second: 564, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.176 [-1.074, 0.685], mean_best_reward: --
 30722/100000: episode: 641, duration: 0.131s, episode steps: 74, steps per second: 563, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.118 [-0.621, 1.123], mean_best_reward: --
 30749/100000: episode: 642, duration: 0.050s, episode steps: 27, steps per second: 542, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.082 [-1.248, 0.631], mean_best_reward: --
 30793/100000: episode: 643, duration: 0.079s, episode steps: 44, steps per second: 560, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.063 [-0.629, 0.826], mean_best_reward: --
 30880/100000: episode: 644, duration: 0.147s, episode steps: 87, steps per second: 593, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.160 [-0.913, 1.844], mean_best_reward: --
 30951/100000: episode: 645, duration: 0.118s, episode steps: 71, steps per second: 604, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.153 [-0.758, 0.980], mean_best_reward: --
 31011/100000: episode: 646, duration: 0.087s, episode steps: 60, steps per second: 691, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.112 [-0.771, 1.106], mean_best_reward: --
 31030/100000: episode: 647, duration: 0.031s, episode steps: 19, steps per second: 619, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.084 [-0.430, 1.001], mean_best_reward: --
 31133/100000: episode: 648, duration: 0.161s, episode steps: 103, steps per second: 641, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.029 [-1.003, 1.159], mean_best_reward: --
 31174/100000: episode: 649, duration: 0.064s, episode steps: 41, steps per second: 640, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.056 [-0.783, 1.201], mean_best_reward: --
 31204/100000: episode: 650, duration: 0.052s, episode steps: 30, steps per second: 577, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.421, 1.167], mean_best_reward: 112.000000
 31259/100000: episode: 651, duration: 0.082s, episode steps: 55, steps per second: 670, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.213 [-0.629, 1.424], mean_best_reward: --
 31346/100000: episode: 652, duration: 0.142s, episode steps: 87, steps per second: 615, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.129 [-0.704, 1.285], mean_best_reward: --
 31420/100000: episode: 653, duration: 0.131s, episode steps: 74, steps per second: 566, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.101 [-1.274, 0.787], mean_best_reward: --
 31474/100000: episode: 654, duration: 0.089s, episode steps: 54, steps per second: 605, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.184 [-0.846, 1.330], mean_best_reward: --
 31558/100000: episode: 655, duration: 0.137s, episode steps: 84, steps per second: 613, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.149 [-0.794, 1.126], mean_best_reward: --
 31626/100000: episode: 656, duration: 0.106s, episode steps: 68, steps per second: 640, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.254 [-1.456, 0.549], mean_best_reward: --
 31675/100000: episode: 657, duration: 0.076s, episode steps: 49, steps per second: 647, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.111 [-0.495, 1.068], mean_best_reward: --
 31706/100000: episode: 658, duration: 0.049s, episode steps: 31, steps per second: 633, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.120 [-0.395, 1.251], mean_best_reward: --
 31820/100000: episode: 659, duration: 0.174s, episode steps: 114, steps per second: 656, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.082 [-0.955, 0.922], mean_best_reward: --
 31861/100000: episode: 660, duration: 0.074s, episode steps: 41, steps per second: 550, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.058 [-0.602, 1.029], mean_best_reward: --
 31988/100000: episode: 661, duration: 0.205s, episode steps: 127, steps per second: 620, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.027 [-0.746, 1.141], mean_best_reward: --
 32037/100000: episode: 662, duration: 0.091s, episode steps: 49, steps per second: 541, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.143 [-0.936, 0.708], mean_best_reward: --
 32077/100000: episode: 663, duration: 0.071s, episode steps: 40, steps per second: 564, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.068 [-0.559, 1.278], mean_best_reward: --
 32135/100000: episode: 664, duration: 0.101s, episode steps: 58, steps per second: 572, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.071 [-0.794, 1.213], mean_best_reward: --
 32209/100000: episode: 665, duration: 0.122s, episode steps: 74, steps per second: 605, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.155 [-0.559, 1.260], mean_best_reward: --
 32243/100000: episode: 666, duration: 0.062s, episode steps: 34, steps per second: 545, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.130 [-0.477, 0.840], mean_best_reward: --
 32302/100000: episode: 667, duration: 0.098s, episode steps: 59, steps per second: 603, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.037 [-0.955, 1.159], mean_best_reward: --
 32360/100000: episode: 668, duration: 0.102s, episode steps: 58, steps per second: 568, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.223 [-1.153, 0.586], mean_best_reward: --
 32443/100000: episode: 669, duration: 0.144s, episode steps: 83, steps per second: 576, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.065 [-0.865, 1.064], mean_best_reward: --
 32480/100000: episode: 670, duration: 0.057s, episode steps: 37, steps per second: 646, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.056 [-0.591, 1.014], mean_best_reward: --
 32558/100000: episode: 671, duration: 0.117s, episode steps: 78, steps per second: 664, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.151 [-0.718, 1.089], mean_best_reward: --
 32610/100000: episode: 672, duration: 0.078s, episode steps: 52, steps per second: 667, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.162 [-0.614, 1.134], mean_best_reward: --
 32747/100000: episode: 673, duration: 0.244s, episode steps: 137, steps per second: 562, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.081 [-0.796, 1.265], mean_best_reward: --
 32798/100000: episode: 674, duration: 0.085s, episode steps: 51, steps per second: 598, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.194 [-0.576, 1.026], mean_best_reward: --
 32884/100000: episode: 675, duration: 0.134s, episode steps: 86, steps per second: 642, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.163 [-1.072, 1.469], mean_best_reward: --
 32940/100000: episode: 676, duration: 0.085s, episode steps: 56, steps per second: 659, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.061 [-0.954, 1.341], mean_best_reward: --
 32978/100000: episode: 677, duration: 0.055s, episode steps: 38, steps per second: 694, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.106 [-0.622, 0.928], mean_best_reward: --
 33028/100000: episode: 678, duration: 0.090s, episode steps: 50, steps per second: 555, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.113 [-0.859, 1.023], mean_best_reward: --
 33051/100000: episode: 679, duration: 0.040s, episode steps: 23, steps per second: 569, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.126 [-0.758, 1.304], mean_best_reward: --
 33156/100000: episode: 680, duration: 0.173s, episode steps: 105, steps per second: 605, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.201 [-1.473, 0.901], mean_best_reward: --
 33218/100000: episode: 681, duration: 0.106s, episode steps: 62, steps per second: 587, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.182 [-0.529, 1.330], mean_best_reward: --
 33276/100000: episode: 682, duration: 0.093s, episode steps: 58, steps per second: 622, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.952, 0.803], mean_best_reward: --
 33308/100000: episode: 683, duration: 0.048s, episode steps: 32, steps per second: 661, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.744, 1.096], mean_best_reward: --
 33328/100000: episode: 684, duration: 0.033s, episode steps: 20, steps per second: 597, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.126 [-0.560, 1.244], mean_best_reward: --
 33386/100000: episode: 685, duration: 0.103s, episode steps: 58, steps per second: 564, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.043 [-1.007, 0.752], mean_best_reward: --
 33445/100000: episode: 686, duration: 0.097s, episode steps: 59, steps per second: 610, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.190 [-1.152, 0.497], mean_best_reward: --
 33489/100000: episode: 687, duration: 0.066s, episode steps: 44, steps per second: 668, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.115 [-1.118, 0.808], mean_best_reward: --
 33552/100000: episode: 688, duration: 0.100s, episode steps: 63, steps per second: 629, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.137 [-0.982, 0.467], mean_best_reward: --
 33593/100000: episode: 689, duration: 0.062s, episode steps: 41, steps per second: 662, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.015 [-0.771, 1.188], mean_best_reward: --
 33636/100000: episode: 690, duration: 0.078s, episode steps: 43, steps per second: 552, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.139 [-0.417, 1.100], mean_best_reward: --
 33658/100000: episode: 691, duration: 0.036s, episode steps: 22, steps per second: 616, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.065 [-0.792, 1.261], mean_best_reward: --
 33762/100000: episode: 692, duration: 0.163s, episode steps: 104, steps per second: 636, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.188 [-1.055, 1.335], mean_best_reward: --
 33797/100000: episode: 693, duration: 0.066s, episode steps: 35, steps per second: 530, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.119 [-0.589, 0.891], mean_best_reward: --
 33903/100000: episode: 694, duration: 0.171s, episode steps: 106, steps per second: 622, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.824, 1.024], mean_best_reward: --
 34091/100000: episode: 695, duration: 0.315s, episode steps: 188, steps per second: 597, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.039 [-1.094, 1.346], mean_best_reward: --
 34127/100000: episode: 696, duration: 0.062s, episode steps: 36, steps per second: 580, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.130 [-0.948, 0.606], mean_best_reward: --
 34157/100000: episode: 697, duration: 0.052s, episode steps: 30, steps per second: 573, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.092 [-0.848, 1.131], mean_best_reward: --
 34295/100000: episode: 698, duration: 0.207s, episode steps: 138, steps per second: 666, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.183 [-1.238, 0.841], mean_best_reward: --
 34342/100000: episode: 699, duration: 0.072s, episode steps: 47, steps per second: 655, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.122 [-0.751, 1.098], mean_best_reward: --
 34401/100000: episode: 700, duration: 0.096s, episode steps: 59, steps per second: 616, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.038 [-1.174, 0.628], mean_best_reward: 120.500000
 34533/100000: episode: 701, duration: 0.227s, episode steps: 132, steps per second: 582, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.053 [-1.045, 1.036], mean_best_reward: --
 34567/100000: episode: 702, duration: 0.056s, episode steps: 34, steps per second: 604, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.129 [-0.576, 0.979], mean_best_reward: --
 34634/100000: episode: 703, duration: 0.117s, episode steps: 67, steps per second: 573, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.158 [-0.970, 0.527], mean_best_reward: --
 34746/100000: episode: 704, duration: 0.190s, episode steps: 112, steps per second: 589, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.120 [-0.810, 1.141], mean_best_reward: --
 34774/100000: episode: 705, duration: 0.048s, episode steps: 28, steps per second: 584, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.366, 0.856], mean_best_reward: --
 34799/100000: episode: 706, duration: 0.048s, episode steps: 25, steps per second: 524, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.086 [-0.862, 0.593], mean_best_reward: --
 34861/100000: episode: 707, duration: 0.099s, episode steps: 62, steps per second: 627, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.115 [-0.956, 0.510], mean_best_reward: --
 34938/100000: episode: 708, duration: 0.117s, episode steps: 77, steps per second: 661, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.215 [-1.695, 0.544], mean_best_reward: --
 35064/100000: episode: 709, duration: 0.203s, episode steps: 126, steps per second: 621, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.176 [-1.130, 0.959], mean_best_reward: --
 35100/100000: episode: 710, duration: 0.063s, episode steps: 36, steps per second: 573, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.083 [-1.420, 0.745], mean_best_reward: --
 35149/100000: episode: 711, duration: 0.086s, episode steps: 49, steps per second: 572, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.095 [-0.754, 1.033], mean_best_reward: --
 35198/100000: episode: 712, duration: 0.083s, episode steps: 49, steps per second: 591, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.111 [-0.741, 0.780], mean_best_reward: --
 35272/100000: episode: 713, duration: 0.129s, episode steps: 74, steps per second: 574, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.234 [-0.800, 1.330], mean_best_reward: --
 35339/100000: episode: 714, duration: 0.142s, episode steps: 67, steps per second: 470, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.137 [-1.122, 0.578], mean_best_reward: --
 35376/100000: episode: 715, duration: 0.057s, episode steps: 37, steps per second: 650, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.119 [-0.471, 1.199], mean_best_reward: --
 35486/100000: episode: 716, duration: 0.177s, episode steps: 110, steps per second: 621, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.038 [-0.923, 1.110], mean_best_reward: --
 35515/100000: episode: 717, duration: 0.048s, episode steps: 29, steps per second: 602, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.133 [-0.576, 1.257], mean_best_reward: --
 35572/100000: episode: 718, duration: 0.087s, episode steps: 57, steps per second: 654, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.079 [-0.858, 0.952], mean_best_reward: --
 35626/100000: episode: 719, duration: 0.084s, episode steps: 54, steps per second: 644, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.188 [-1.111, 0.669], mean_best_reward: --
 35710/100000: episode: 720, duration: 0.131s, episode steps: 84, steps per second: 640, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.134 [-0.731, 1.129], mean_best_reward: --
 35749/100000: episode: 721, duration: 0.063s, episode steps: 39, steps per second: 622, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.085 [-0.501, 1.174], mean_best_reward: --
 35815/100000: episode: 722, duration: 0.103s, episode steps: 66, steps per second: 642, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.230 [-1.452, 0.982], mean_best_reward: --
 35966/100000: episode: 723, duration: 0.246s, episode steps: 151, steps per second: 613, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.113 [-1.215, 1.408], mean_best_reward: --
 36026/100000: episode: 724, duration: 0.101s, episode steps: 60, steps per second: 592, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.244 [-1.340, 0.600], mean_best_reward: --
 36099/100000: episode: 725, duration: 0.113s, episode steps: 73, steps per second: 647, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.137 [-0.638, 1.128], mean_best_reward: --
 36151/100000: episode: 726, duration: 0.079s, episode steps: 52, steps per second: 656, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.139 [-0.649, 1.164], mean_best_reward: --
 36222/100000: episode: 727, duration: 0.107s, episode steps: 71, steps per second: 662, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.084 [-0.933, 0.890], mean_best_reward: --
 36396/100000: episode: 728, duration: 0.265s, episode steps: 174, steps per second: 656, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.073 [-1.005, 0.741], mean_best_reward: --
 36435/100000: episode: 729, duration: 0.059s, episode steps: 39, steps per second: 665, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.111 [-0.472, 1.089], mean_best_reward: --
 36525/100000: episode: 730, duration: 0.139s, episode steps: 90, steps per second: 647, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.078 [-0.698, 1.198], mean_best_reward: --
 36556/100000: episode: 731, duration: 0.049s, episode steps: 31, steps per second: 638, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.056 [-0.557, 0.932], mean_best_reward: --
 36585/100000: episode: 732, duration: 0.050s, episode steps: 29, steps per second: 584, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.092 [-0.583, 1.088], mean_best_reward: --
 36660/100000: episode: 733, duration: 0.114s, episode steps: 75, steps per second: 659, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.099 [-0.839, 0.907], mean_best_reward: --
 36727/100000: episode: 734, duration: 0.105s, episode steps: 67, steps per second: 636, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.215 [-1.320, 0.819], mean_best_reward: --
 36789/100000: episode: 735, duration: 0.106s, episode steps: 62, steps per second: 583, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.107 [-0.595, 0.895], mean_best_reward: --
 36830/100000: episode: 736, duration: 0.075s, episode steps: 41, steps per second: 549, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.115 [-0.771, 1.158], mean_best_reward: --
 36888/100000: episode: 737, duration: 0.092s, episode steps: 58, steps per second: 634, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.161 [-0.574, 1.178], mean_best_reward: --
 36960/100000: episode: 738, duration: 0.106s, episode steps: 72, steps per second: 678, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.159 [-0.574, 1.245], mean_best_reward: --
 37029/100000: episode: 739, duration: 0.106s, episode steps: 69, steps per second: 649, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.217 [-0.861, 1.341], mean_best_reward: --
 37128/100000: episode: 740, duration: 0.175s, episode steps: 99, steps per second: 565, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.170 [-1.333, 0.884], mean_best_reward: --
 37227/100000: episode: 741, duration: 0.172s, episode steps: 99, steps per second: 575, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.153 [-1.224, 0.800], mean_best_reward: --
 37311/100000: episode: 742, duration: 0.133s, episode steps: 84, steps per second: 631, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.130 [-0.586, 1.380], mean_best_reward: --
 37339/100000: episode: 743, duration: 0.053s, episode steps: 28, steps per second: 525, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.558, 1.147], mean_best_reward: --
 37372/100000: episode: 744, duration: 0.062s, episode steps: 33, steps per second: 533, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: 0.157 [-0.575, 1.017], mean_best_reward: --
 37405/100000: episode: 745, duration: 0.059s, episode steps: 33, steps per second: 559, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.092 [-1.300, 0.626], mean_best_reward: --
 37445/100000: episode: 746, duration: 0.062s, episode steps: 40, steps per second: 650, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.141 [-0.364, 0.919], mean_best_reward: --
 37485/100000: episode: 747, duration: 0.071s, episode steps: 40, steps per second: 567, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.153 [-0.743, 1.230], mean_best_reward: --
 37539/100000: episode: 748, duration: 0.083s, episode steps: 54, steps per second: 653, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.789, 1.171], mean_best_reward: --
 37645/100000: episode: 749, duration: 0.173s, episode steps: 106, steps per second: 612, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.020 [-1.174, 0.790], mean_best_reward: --
 37685/100000: episode: 750, duration: 0.064s, episode steps: 40, steps per second: 624, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.059 [-0.784, 0.977], mean_best_reward: 175.500000
 37762/100000: episode: 751, duration: 0.124s, episode steps: 77, steps per second: 620, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.143 [-0.639, 1.501], mean_best_reward: --
 37819/100000: episode: 752, duration: 0.102s, episode steps: 57, steps per second: 559, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.003 [-0.793, 0.928], mean_best_reward: --
 37881/100000: episode: 753, duration: 0.115s, episode steps: 62, steps per second: 540, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.193 [-1.020, 0.896], mean_best_reward: --
 37916/100000: episode: 754, duration: 0.062s, episode steps: 35, steps per second: 567, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.099 [-0.267, 1.147], mean_best_reward: --
 37969/100000: episode: 755, duration: 0.090s, episode steps: 53, steps per second: 592, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.179 [-0.605, 1.149], mean_best_reward: --
 38006/100000: episode: 756, duration: 0.061s, episode steps: 37, steps per second: 605, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.106 [-0.503, 1.366], mean_best_reward: --
 38071/100000: episode: 757, duration: 0.105s, episode steps: 65, steps per second: 617, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.141 [-0.480, 1.129], mean_best_reward: --
 38148/100000: episode: 758, duration: 0.124s, episode steps: 77, steps per second: 619, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.085 [-0.749, 0.919], mean_best_reward: --
 38206/100000: episode: 759, duration: 0.099s, episode steps: 58, steps per second: 588, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.654, 1.397], mean_best_reward: --
 38293/100000: episode: 760, duration: 0.147s, episode steps: 87, steps per second: 593, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.160 [-0.847, 1.263], mean_best_reward: --
 38325/100000: episode: 761, duration: 0.055s, episode steps: 32, steps per second: 585, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.781, 1.181], mean_best_reward: --
 38454/100000: episode: 762, duration: 0.212s, episode steps: 129, steps per second: 608, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.073 [-0.736, 0.984], mean_best_reward: --
 38480/100000: episode: 763, duration: 0.050s, episode steps: 26, steps per second: 520, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.047 [-0.630, 1.150], mean_best_reward: --
 38513/100000: episode: 764, duration: 0.054s, episode steps: 33, steps per second: 614, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.119 [-0.404, 0.942], mean_best_reward: --
 38543/100000: episode: 765, duration: 0.054s, episode steps: 30, steps per second: 556, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.878, 0.387], mean_best_reward: --
 38622/100000: episode: 766, duration: 0.132s, episode steps: 79, steps per second: 597, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.210 [-1.029, 1.547], mean_best_reward: --
 38771/100000: episode: 767, duration: 0.236s, episode steps: 149, steps per second: 632, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.030 [-1.185, 1.112], mean_best_reward: --
 38811/100000: episode: 768, duration: 0.070s, episode steps: 40, steps per second: 571, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.129 [-0.544, 1.092], mean_best_reward: --
 38889/100000: episode: 769, duration: 0.137s, episode steps: 78, steps per second: 571, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.074 [-0.602, 0.936], mean_best_reward: --
 38940/100000: episode: 770, duration: 0.083s, episode steps: 51, steps per second: 615, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.095 [-0.608, 1.148], mean_best_reward: --
 38982/100000: episode: 771, duration: 0.070s, episode steps: 42, steps per second: 599, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.575, 0.829], mean_best_reward: --
 39029/100000: episode: 772, duration: 0.081s, episode steps: 47, steps per second: 581, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.114 [-0.602, 0.942], mean_best_reward: --
 39090/100000: episode: 773, duration: 0.109s, episode steps: 61, steps per second: 561, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.098 [-0.803, 0.974], mean_best_reward: --
 39207/100000: episode: 774, duration: 0.209s, episode steps: 117, steps per second: 560, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.228 [-0.658, 1.702], mean_best_reward: --
 39250/100000: episode: 775, duration: 0.077s, episode steps: 43, steps per second: 560, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.040 [-0.516, 0.900], mean_best_reward: --
 39298/100000: episode: 776, duration: 0.071s, episode steps: 48, steps per second: 675, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.070 [-0.586, 0.958], mean_best_reward: --
 39364/100000: episode: 777, duration: 0.110s, episode steps: 66, steps per second: 599, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.129 [-0.539, 1.185], mean_best_reward: --
 39452/100000: episode: 778, duration: 0.153s, episode steps: 88, steps per second: 575, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.219 [-0.991, 0.702], mean_best_reward: --
 39487/100000: episode: 779, duration: 0.061s, episode steps: 35, steps per second: 571, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.111 [-0.304, 1.114], mean_best_reward: --
 39529/100000: episode: 780, duration: 0.072s, episode steps: 42, steps per second: 580, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.038 [-0.628, 0.830], mean_best_reward: --
 39562/100000: episode: 781, duration: 0.056s, episode steps: 33, steps per second: 591, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.101 [-0.367, 1.140], mean_best_reward: --
 39619/100000: episode: 782, duration: 0.090s, episode steps: 57, steps per second: 636, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.155 [-0.454, 0.974], mean_best_reward: --
 39657/100000: episode: 783, duration: 0.055s, episode steps: 38, steps per second: 688, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.019 [-1.013, 1.353], mean_best_reward: --
 39716/100000: episode: 784, duration: 0.097s, episode steps: 59, steps per second: 607, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: 0.222 [-0.659, 1.613], mean_best_reward: --
 39755/100000: episode: 785, duration: 0.069s, episode steps: 39, steps per second: 567, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.059 [-0.618, 1.044], mean_best_reward: --
 39785/100000: episode: 786, duration: 0.045s, episode steps: 30, steps per second: 662, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.295, 0.767], mean_best_reward: --
 39814/100000: episode: 787, duration: 0.044s, episode steps: 29, steps per second: 665, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.140 [-0.820, 0.364], mean_best_reward: --
 39868/100000: episode: 788, duration: 0.094s, episode steps: 54, steps per second: 574, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.150 [-0.647, 1.445], mean_best_reward: --
 39973/100000: episode: 789, duration: 0.173s, episode steps: 105, steps per second: 607, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.225 [-1.691, 0.689], mean_best_reward: --
 40053/100000: episode: 790, duration: 0.140s, episode steps: 80, steps per second: 570, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.187 [-1.491, 0.939], mean_best_reward: --
 40122/100000: episode: 791, duration: 0.127s, episode steps: 69, steps per second: 545, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.186 [-0.535, 1.110], mean_best_reward: --
 40237/100000: episode: 792, duration: 0.199s, episode steps: 115, steps per second: 577, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.043 [-0.821, 1.051], mean_best_reward: --
 40315/100000: episode: 793, duration: 0.138s, episode steps: 78, steps per second: 567, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-1.003, 1.517], mean_best_reward: --
 40382/100000: episode: 794, duration: 0.117s, episode steps: 67, steps per second: 571, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.189 [-0.617, 1.325], mean_best_reward: --
 40456/100000: episode: 795, duration: 0.123s, episode steps: 74, steps per second: 600, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.157 [-0.432, 1.307], mean_best_reward: --
 40530/100000: episode: 796, duration: 0.125s, episode steps: 74, steps per second: 593, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.169 [-0.596, 1.286], mean_best_reward: --
 40568/100000: episode: 797, duration: 0.061s, episode steps: 38, steps per second: 622, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.043 [-0.612, 1.137], mean_best_reward: --
 40635/100000: episode: 798, duration: 0.116s, episode steps: 67, steps per second: 580, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.198 [-1.119, 0.572], mean_best_reward: --
 40688/100000: episode: 799, duration: 0.093s, episode steps: 53, steps per second: 569, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.195 [-0.667, 1.132], mean_best_reward: --
 40718/100000: episode: 800, duration: 0.054s, episode steps: 30, steps per second: 561, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.453, 1.034], mean_best_reward: 148.000000
 40749/100000: episode: 801, duration: 0.053s, episode steps: 31, steps per second: 580, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.098 [-0.403, 0.819], mean_best_reward: --
 40825/100000: episode: 802, duration: 0.128s, episode steps: 76, steps per second: 594, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.053 [-0.987, 0.939], mean_best_reward: --
 40912/100000: episode: 803, duration: 0.151s, episode steps: 87, steps per second: 578, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.216 [-1.401, 1.002], mean_best_reward: --
 40944/100000: episode: 804, duration: 0.053s, episode steps: 32, steps per second: 601, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.261, 1.021], mean_best_reward: --
 41111/100000: episode: 805, duration: 0.284s, episode steps: 167, steps per second: 588, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.106 [-1.134, 0.735], mean_best_reward: --
 41202/100000: episode: 806, duration: 0.146s, episode steps: 91, steps per second: 621, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.126 [-1.266, 0.606], mean_best_reward: --
 41280/100000: episode: 807, duration: 0.125s, episode steps: 78, steps per second: 623, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.063 [-0.711, 1.168], mean_best_reward: --
 41452/100000: episode: 808, duration: 0.264s, episode steps: 172, steps per second: 652, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.111 [-0.901, 0.829], mean_best_reward: --
 41466/100000: episode: 809, duration: 0.023s, episode steps: 14, steps per second: 601, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.737, 1.243], mean_best_reward: --
 41532/100000: episode: 810, duration: 0.106s, episode steps: 66, steps per second: 621, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.111 [-0.958, 0.628], mean_best_reward: --
 41586/100000: episode: 811, duration: 0.082s, episode steps: 54, steps per second: 660, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.117 [-0.601, 0.917], mean_best_reward: --
 41715/100000: episode: 812, duration: 0.208s, episode steps: 129, steps per second: 620, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.043 [-0.739, 0.942], mean_best_reward: --
 41766/100000: episode: 813, duration: 0.091s, episode steps: 51, steps per second: 558, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.159 [-0.770, 0.850], mean_best_reward: --
 41829/100000: episode: 814, duration: 0.105s, episode steps: 63, steps per second: 600, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.145 [-0.945, 1.398], mean_best_reward: --
 42000/100000: episode: 815, duration: 0.296s, episode steps: 171, steps per second: 577, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.041 [-1.137, 0.943], mean_best_reward: --
 42044/100000: episode: 816, duration: 0.081s, episode steps: 44, steps per second: 543, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.206 [-1.108, 0.590], mean_best_reward: --
 42113/100000: episode: 817, duration: 0.120s, episode steps: 69, steps per second: 577, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.082 [-0.567, 0.884], mean_best_reward: --
 42142/100000: episode: 818, duration: 0.047s, episode steps: 29, steps per second: 617, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.116 [-0.615, 0.942], mean_best_reward: --
 42213/100000: episode: 819, duration: 0.107s, episode steps: 71, steps per second: 662, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.078 [-1.003, 1.428], mean_best_reward: --
 42248/100000: episode: 820, duration: 0.053s, episode steps: 35, steps per second: 661, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.132 [-0.877, 0.433], mean_best_reward: --
 42265/100000: episode: 821, duration: 0.028s, episode steps: 17, steps per second: 617, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.118 [-0.741, 1.288], mean_best_reward: --
 42301/100000: episode: 822, duration: 0.055s, episode steps: 36, steps per second: 654, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.130 [-0.588, 0.903], mean_best_reward: --
 42329/100000: episode: 823, duration: 0.043s, episode steps: 28, steps per second: 656, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.573, 1.029], mean_best_reward: --
 42353/100000: episode: 824, duration: 0.038s, episode steps: 24, steps per second: 630, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.076 [-0.577, 1.375], mean_best_reward: --
 42375/100000: episode: 825, duration: 0.036s, episode steps: 22, steps per second: 618, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-0.902, 0.403], mean_best_reward: --
 42494/100000: episode: 826, duration: 0.183s, episode steps: 119, steps per second: 650, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.098 [-0.727, 0.972], mean_best_reward: --
 42564/100000: episode: 827, duration: 0.120s, episode steps: 70, steps per second: 582, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.176 [-0.998, 1.236], mean_best_reward: --
 42601/100000: episode: 828, duration: 0.068s, episode steps: 37, steps per second: 542, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.091 [-1.014, 0.610], mean_best_reward: --
 42674/100000: episode: 829, duration: 0.115s, episode steps: 73, steps per second: 634, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.102 [-0.971, 1.251], mean_best_reward: --
 42721/100000: episode: 830, duration: 0.072s, episode steps: 47, steps per second: 657, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.166 [-1.045, 0.564], mean_best_reward: --
 42765/100000: episode: 831, duration: 0.069s, episode steps: 44, steps per second: 641, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.892, 1.270], mean_best_reward: --
 42837/100000: episode: 832, duration: 0.118s, episode steps: 72, steps per second: 610, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.297 [-0.923, 1.525], mean_best_reward: --
 42884/100000: episode: 833, duration: 0.091s, episode steps: 47, steps per second: 517, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.083 [-0.948, 1.126], mean_best_reward: --
 42932/100000: episode: 834, duration: 0.080s, episode steps: 48, steps per second: 599, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.128 [-1.446, 0.696], mean_best_reward: --
 43014/100000: episode: 835, duration: 0.141s, episode steps: 82, steps per second: 582, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.243 [-1.451, 0.790], mean_best_reward: --
 43040/100000: episode: 836, duration: 0.041s, episode steps: 26, steps per second: 628, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.140 [-1.032, 0.582], mean_best_reward: --
 43132/100000: episode: 837, duration: 0.149s, episode steps: 92, steps per second: 617, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.093 [-0.992, 0.986], mean_best_reward: --
 43154/100000: episode: 838, duration: 0.036s, episode steps: 22, steps per second: 614, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.634, 1.054], mean_best_reward: --
 43211/100000: episode: 839, duration: 0.097s, episode steps: 57, steps per second: 589, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.099 [-0.786, 1.088], mean_best_reward: --
 43264/100000: episode: 840, duration: 0.087s, episode steps: 53, steps per second: 609, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.195 [-1.275, 0.607], mean_best_reward: --
 43370/100000: episode: 841, duration: 0.171s, episode steps: 106, steps per second: 620, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-0.832, 1.530], mean_best_reward: --
 43420/100000: episode: 842, duration: 0.087s, episode steps: 50, steps per second: 574, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.884, 1.030], mean_best_reward: --
 43533/100000: episode: 843, duration: 0.181s, episode steps: 113, steps per second: 623, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.146 [-0.704, 0.860], mean_best_reward: --
 43568/100000: episode: 844, duration: 0.067s, episode steps: 35, steps per second: 523, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.151 [-0.625, 1.111], mean_best_reward: --
 43628/100000: episode: 845, duration: 0.106s, episode steps: 60, steps per second: 566, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.040 [-0.600, 0.824], mean_best_reward: --
 43659/100000: episode: 846, duration: 0.055s, episode steps: 31, steps per second: 568, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.127 [-0.403, 1.069], mean_best_reward: --
 43720/100000: episode: 847, duration: 0.101s, episode steps: 61, steps per second: 601, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.138 [-1.333, 0.732], mean_best_reward: --
 43770/100000: episode: 848, duration: 0.083s, episode steps: 50, steps per second: 602, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-1.155, 1.540], mean_best_reward: --
 43802/100000: episode: 849, duration: 0.051s, episode steps: 32, steps per second: 630, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.575, 1.384], mean_best_reward: --
 43831/100000: episode: 850, duration: 0.047s, episode steps: 29, steps per second: 611, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.097 [-0.750, 1.169], mean_best_reward: 164.000000
 43960/100000: episode: 851, duration: 0.206s, episode steps: 129, steps per second: 627, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.126 [-0.942, 1.288], mean_best_reward: --
 44023/100000: episode: 852, duration: 0.097s, episode steps: 63, steps per second: 648, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.167 [-1.280, 0.846], mean_best_reward: --
 44089/100000: episode: 853, duration: 0.105s, episode steps: 66, steps per second: 631, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.160 [-1.215, 0.647], mean_best_reward: --
 44159/100000: episode: 854, duration: 0.119s, episode steps: 70, steps per second: 588, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.123 [-0.659, 0.983], mean_best_reward: --
 44268/100000: episode: 855, duration: 0.176s, episode steps: 109, steps per second: 621, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.243 [-1.302, 0.892], mean_best_reward: --
 44339/100000: episode: 856, duration: 0.120s, episode steps: 71, steps per second: 590, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.074 [-0.661, 0.874], mean_best_reward: --
 44374/100000: episode: 857, duration: 0.061s, episode steps: 35, steps per second: 575, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.087 [-0.807, 1.331], mean_best_reward: --
 44402/100000: episode: 858, duration: 0.050s, episode steps: 28, steps per second: 557, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.418, 0.936], mean_best_reward: --
 44451/100000: episode: 859, duration: 0.079s, episode steps: 49, steps per second: 621, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.104 [-0.614, 0.856], mean_best_reward: --
 44507/100000: episode: 860, duration: 0.083s, episode steps: 56, steps per second: 671, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.148 [-0.573, 1.103], mean_best_reward: --
 44593/100000: episode: 861, duration: 0.127s, episode steps: 86, steps per second: 676, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.088 [-0.876, 0.729], mean_best_reward: --
 44714/100000: episode: 862, duration: 0.192s, episode steps: 121, steps per second: 629, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.162 [-0.947, 0.847], mean_best_reward: --
 44842/100000: episode: 863, duration: 0.195s, episode steps: 128, steps per second: 655, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.100 [-0.604, 1.245], mean_best_reward: --
 44953/100000: episode: 864, duration: 0.177s, episode steps: 111, steps per second: 626, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.179 [-0.724, 1.425], mean_best_reward: --
 45055/100000: episode: 865, duration: 0.161s, episode steps: 102, steps per second: 633, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-0.974, 1.082], mean_best_reward: --
 45094/100000: episode: 866, duration: 0.062s, episode steps: 39, steps per second: 633, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.082 [-0.601, 1.135], mean_best_reward: --
 45113/100000: episode: 867, duration: 0.032s, episode steps: 19, steps per second: 593, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.105 [-0.599, 1.129], mean_best_reward: --
 45169/100000: episode: 868, duration: 0.088s, episode steps: 56, steps per second: 637, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.206 [-1.286, 0.402], mean_best_reward: --
 45239/100000: episode: 869, duration: 0.110s, episode steps: 70, steps per second: 639, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.196 [-1.134, 0.705], mean_best_reward: --
 45505/100000: episode: 870, duration: 0.415s, episode steps: 266, steps per second: 642, episode reward: 266.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.023 [-0.942, 0.975], mean_best_reward: --
 45550/100000: episode: 871, duration: 0.064s, episode steps: 45, steps per second: 705, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.083 [-0.815, 0.979], mean_best_reward: --
 45639/100000: episode: 872, duration: 0.158s, episode steps: 89, steps per second: 562, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.119 [-0.937, 0.903], mean_best_reward: --
 45733/100000: episode: 873, duration: 0.149s, episode steps: 94, steps per second: 631, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.153 [-0.779, 0.725], mean_best_reward: --
 45751/100000: episode: 874, duration: 0.028s, episode steps: 18, steps per second: 648, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-1.013, 1.358], mean_best_reward: --
 45828/100000: episode: 875, duration: 0.134s, episode steps: 77, steps per second: 576, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.064 [-0.780, 0.972], mean_best_reward: --
 45849/100000: episode: 876, duration: 0.038s, episode steps: 21, steps per second: 548, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.113 [-0.540, 0.947], mean_best_reward: --
 45965/100000: episode: 877, duration: 0.164s, episode steps: 116, steps per second: 706, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.000 [-0.985, 1.082], mean_best_reward: --
 46030/100000: episode: 878, duration: 0.102s, episode steps: 65, steps per second: 635, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.031 [-1.031, 1.116], mean_best_reward: --
 46105/100000: episode: 879, duration: 0.113s, episode steps: 75, steps per second: 664, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.087 [-0.595, 0.910], mean_best_reward: --
 46132/100000: episode: 880, duration: 0.048s, episode steps: 27, steps per second: 559, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.141 [-0.598, 1.020], mean_best_reward: --
 46164/100000: episode: 881, duration: 0.051s, episode steps: 32, steps per second: 623, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.392, 1.129], mean_best_reward: --
 46214/100000: episode: 882, duration: 0.074s, episode steps: 50, steps per second: 680, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.165 [-0.636, 0.898], mean_best_reward: --
 46322/100000: episode: 883, duration: 0.183s, episode steps: 108, steps per second: 591, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.185 [-0.712, 1.703], mean_best_reward: --
 46358/100000: episode: 884, duration: 0.062s, episode steps: 36, steps per second: 584, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.150 [-0.542, 0.923], mean_best_reward: --
 46398/100000: episode: 885, duration: 0.069s, episode steps: 40, steps per second: 581, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.077 [-0.431, 0.866], mean_best_reward: --
 46445/100000: episode: 886, duration: 0.085s, episode steps: 47, steps per second: 551, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.094 [-0.481, 0.914], mean_best_reward: --
 46490/100000: episode: 887, duration: 0.073s, episode steps: 45, steps per second: 621, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.122 [-0.569, 1.208], mean_best_reward: --
 46558/100000: episode: 888, duration: 0.117s, episode steps: 68, steps per second: 580, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.225 [-1.079, 0.746], mean_best_reward: --
 46667/100000: episode: 889, duration: 0.179s, episode steps: 109, steps per second: 608, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.162 [-0.966, 0.867], mean_best_reward: --
 46744/100000: episode: 890, duration: 0.111s, episode steps: 77, steps per second: 696, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.184 [-0.610, 0.919], mean_best_reward: --
 46851/100000: episode: 891, duration: 0.162s, episode steps: 107, steps per second: 661, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.220 [-1.661, 0.947], mean_best_reward: --
 46901/100000: episode: 892, duration: 0.062s, episode steps: 50, steps per second: 810, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.093 [-0.479, 1.094], mean_best_reward: --
 46938/100000: episode: 893, duration: 0.045s, episode steps: 37, steps per second: 815, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.085 [-0.391, 0.942], mean_best_reward: --
 46972/100000: episode: 894, duration: 0.042s, episode steps: 34, steps per second: 801, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.802, 1.051], mean_best_reward: --
 47012/100000: episode: 895, duration: 0.049s, episode steps: 40, steps per second: 809, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.128 [-0.468, 1.454], mean_best_reward: --
 47043/100000: episode: 896, duration: 0.048s, episode steps: 31, steps per second: 640, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.100 [-0.526, 1.116], mean_best_reward: --
 47115/100000: episode: 897, duration: 0.121s, episode steps: 72, steps per second: 593, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.247 [-1.641, 0.864], mean_best_reward: --
 47200/100000: episode: 898, duration: 0.139s, episode steps: 85, steps per second: 612, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.319 [-1.691, 0.725], mean_best_reward: --
 47259/100000: episode: 899, duration: 0.081s, episode steps: 59, steps per second: 730, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.191 [-1.148, 0.535], mean_best_reward: --
 47303/100000: episode: 900, duration: 0.069s, episode steps: 44, steps per second: 641, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.075 [-0.757, 1.083], mean_best_reward: 114.000000
 47425/100000: episode: 901, duration: 0.198s, episode steps: 122, steps per second: 617, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.117 [-0.843, 1.278], mean_best_reward: --
 47467/100000: episode: 902, duration: 0.077s, episode steps: 42, steps per second: 548, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.124 [-0.593, 1.185], mean_best_reward: --
 47526/100000: episode: 903, duration: 0.086s, episode steps: 59, steps per second: 683, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.116 [-1.309, 0.608], mean_best_reward: --
 47583/100000: episode: 904, duration: 0.095s, episode steps: 57, steps per second: 603, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.084 [-0.585, 0.942], mean_best_reward: --
 47650/100000: episode: 905, duration: 0.103s, episode steps: 67, steps per second: 648, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.065 [-1.350, 0.747], mean_best_reward: --
 47772/100000: episode: 906, duration: 0.184s, episode steps: 122, steps per second: 663, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.063 [-0.898, 0.925], mean_best_reward: --
 47835/100000: episode: 907, duration: 0.104s, episode steps: 63, steps per second: 606, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.221 [-1.449, 0.418], mean_best_reward: --
 47909/100000: episode: 908, duration: 0.124s, episode steps: 74, steps per second: 598, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.667, 1.282], mean_best_reward: --
 48053/100000: episode: 909, duration: 0.223s, episode steps: 144, steps per second: 645, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.029 [-1.103, 0.932], mean_best_reward: --
 48120/100000: episode: 910, duration: 0.112s, episode steps: 67, steps per second: 599, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.184 [-1.110, 0.700], mean_best_reward: --
 48151/100000: episode: 911, duration: 0.054s, episode steps: 31, steps per second: 573, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.049 [-0.759, 1.134], mean_best_reward: --
 48221/100000: episode: 912, duration: 0.116s, episode steps: 70, steps per second: 602, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.142 [-0.493, 1.073], mean_best_reward: --
 48243/100000: episode: 913, duration: 0.036s, episode steps: 22, steps per second: 607, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.070 [-1.522, 0.959], mean_best_reward: --
 48316/100000: episode: 914, duration: 0.127s, episode steps: 73, steps per second: 575, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.149 [-0.617, 0.901], mean_best_reward: --
 48397/100000: episode: 915, duration: 0.124s, episode steps: 81, steps per second: 654, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.129 [-0.741, 1.087], mean_best_reward: --
 48537/100000: episode: 916, duration: 0.217s, episode steps: 140, steps per second: 644, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.037 [-1.002, 0.999], mean_best_reward: --
 48617/100000: episode: 917, duration: 0.135s, episode steps: 80, steps per second: 591, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.230 [-1.119, 0.903], mean_best_reward: --
 48665/100000: episode: 918, duration: 0.088s, episode steps: 48, steps per second: 547, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.752, 1.482], mean_best_reward: --
 48758/100000: episode: 919, duration: 0.147s, episode steps: 93, steps per second: 635, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.230 [-1.457, 0.858], mean_best_reward: --
 48827/100000: episode: 920, duration: 0.118s, episode steps: 69, steps per second: 584, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.240 [-0.846, 1.243], mean_best_reward: --
 48886/100000: episode: 921, duration: 0.105s, episode steps: 59, steps per second: 560, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.218 [-1.040, 1.276], mean_best_reward: --
 48940/100000: episode: 922, duration: 0.100s, episode steps: 54, steps per second: 538, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.067 [-0.884, 0.440], mean_best_reward: --
 48987/100000: episode: 923, duration: 0.074s, episode steps: 47, steps per second: 635, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.132 [-1.094, 0.954], mean_best_reward: --
 49021/100000: episode: 924, duration: 0.060s, episode steps: 34, steps per second: 565, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.070 [-0.601, 1.160], mean_best_reward: --
 49107/100000: episode: 925, duration: 0.131s, episode steps: 86, steps per second: 657, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.167 [-1.473, 0.784], mean_best_reward: --
 49199/100000: episode: 926, duration: 0.142s, episode steps: 92, steps per second: 649, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.198 [-1.287, 0.684], mean_best_reward: --
 49268/100000: episode: 927, duration: 0.106s, episode steps: 69, steps per second: 648, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.109 [-0.729, 1.082], mean_best_reward: --
 49334/100000: episode: 928, duration: 0.119s, episode steps: 66, steps per second: 554, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.125 [-1.133, 0.680], mean_best_reward: --
 49396/100000: episode: 929, duration: 0.109s, episode steps: 62, steps per second: 571, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.095 [-0.829, 1.204], mean_best_reward: --
 49502/100000: episode: 930, duration: 0.169s, episode steps: 106, steps per second: 627, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.105 [-0.919, 0.996], mean_best_reward: --
 49559/100000: episode: 931, duration: 0.091s, episode steps: 57, steps per second: 629, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.122 [-0.712, 1.044], mean_best_reward: --
 49607/100000: episode: 932, duration: 0.090s, episode steps: 48, steps per second: 534, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.196 [-1.266, 0.546], mean_best_reward: --
 49682/100000: episode: 933, duration: 0.127s, episode steps: 75, steps per second: 588, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.093 [-0.945, 1.082], mean_best_reward: --
 49707/100000: episode: 934, duration: 0.047s, episode steps: 25, steps per second: 529, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.109 [-0.399, 1.276], mean_best_reward: --
 49765/100000: episode: 935, duration: 0.094s, episode steps: 58, steps per second: 620, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.046 [-0.582, 0.732], mean_best_reward: --
 49805/100000: episode: 936, duration: 0.069s, episode steps: 40, steps per second: 582, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.122 [-0.772, 1.150], mean_best_reward: --
 49847/100000: episode: 937, duration: 0.069s, episode steps: 42, steps per second: 608, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.130 [-0.379, 0.846], mean_best_reward: --
 49901/100000: episode: 938, duration: 0.089s, episode steps: 54, steps per second: 609, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.141 [-0.519, 1.149], mean_best_reward: --
 50024/100000: episode: 939, duration: 0.190s, episode steps: 123, steps per second: 646, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.057 [-1.296, 0.813], mean_best_reward: --
 50069/100000: episode: 940, duration: 0.077s, episode steps: 45, steps per second: 584, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.148 [-0.443, 0.978], mean_best_reward: --
 50159/100000: episode: 941, duration: 0.142s, episode steps: 90, steps per second: 636, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.198 [-0.897, 1.136], mean_best_reward: --
 50181/100000: episode: 942, duration: 0.040s, episode steps: 22, steps per second: 547, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.758, 1.244], mean_best_reward: --
 50212/100000: episode: 943, duration: 0.049s, episode steps: 31, steps per second: 634, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.081 [-1.275, 0.590], mean_best_reward: --
 50240/100000: episode: 944, duration: 0.046s, episode steps: 28, steps per second: 610, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.815, 1.199], mean_best_reward: --
 50313/100000: episode: 945, duration: 0.116s, episode steps: 73, steps per second: 627, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.052 [-0.921, 1.434], mean_best_reward: --
 50352/100000: episode: 946, duration: 0.061s, episode steps: 39, steps per second: 634, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.105 [-1.001, 0.575], mean_best_reward: --
 50374/100000: episode: 947, duration: 0.042s, episode steps: 22, steps per second: 524, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.073 [-0.759, 1.293], mean_best_reward: --
 50418/100000: episode: 948, duration: 0.073s, episode steps: 44, steps per second: 603, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.049 [-1.028, 0.597], mean_best_reward: --
 50495/100000: episode: 949, duration: 0.113s, episode steps: 77, steps per second: 680, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.108 [-0.798, 1.080], mean_best_reward: --
 50545/100000: episode: 950, duration: 0.087s, episode steps: 50, steps per second: 574, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.235 [-1.458, 0.601], mean_best_reward: 166.500000
 50706/100000: episode: 951, duration: 0.248s, episode steps: 161, steps per second: 650, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.088 [-1.060, 1.018], mean_best_reward: --
 50784/100000: episode: 952, duration: 0.127s, episode steps: 78, steps per second: 613, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.130 [-0.672, 1.003], mean_best_reward: --
 50834/100000: episode: 953, duration: 0.077s, episode steps: 50, steps per second: 653, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.112 [-0.973, 0.597], mean_best_reward: --
 50883/100000: episode: 954, duration: 0.074s, episode steps: 49, steps per second: 666, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.098 [-0.436, 0.920], mean_best_reward: --
 50918/100000: episode: 955, duration: 0.058s, episode steps: 35, steps per second: 599, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.095 [-0.461, 1.116], mean_best_reward: --
 51011/100000: episode: 956, duration: 0.163s, episode steps: 93, steps per second: 569, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.124 [-1.304, 0.792], mean_best_reward: --
 51026/100000: episode: 957, duration: 0.026s, episode steps: 15, steps per second: 577, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.105 [-0.622, 1.083], mean_best_reward: --
 51104/100000: episode: 958, duration: 0.130s, episode steps: 78, steps per second: 600, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.600, 1.197], mean_best_reward: --
 51184/100000: episode: 959, duration: 0.132s, episode steps: 80, steps per second: 606, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.161 [-1.166, 0.728], mean_best_reward: --
 51243/100000: episode: 960, duration: 0.095s, episode steps: 59, steps per second: 623, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.118 [-1.086, 0.756], mean_best_reward: --
 51296/100000: episode: 961, duration: 0.094s, episode steps: 53, steps per second: 562, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.068 [-0.901, 0.550], mean_best_reward: --
 51361/100000: episode: 962, duration: 0.099s, episode steps: 65, steps per second: 660, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.075 [-0.772, 1.318], mean_best_reward: --
 51389/100000: episode: 963, duration: 0.044s, episode steps: 28, steps per second: 639, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.623, 0.971], mean_best_reward: --
 51423/100000: episode: 964, duration: 0.052s, episode steps: 34, steps per second: 655, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.103 [-0.399, 0.902], mean_best_reward: --
 51455/100000: episode: 965, duration: 0.050s, episode steps: 32, steps per second: 635, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.574, 1.083], mean_best_reward: --
 51490/100000: episode: 966, duration: 0.056s, episode steps: 35, steps per second: 628, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.109 [-1.191, 0.788], mean_best_reward: --
 51600/100000: episode: 967, duration: 0.182s, episode steps: 110, steps per second: 605, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.215, 0.659], mean_best_reward: --
 51623/100000: episode: 968, duration: 0.040s, episode steps: 23, steps per second: 572, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.087 [-0.585, 0.923], mean_best_reward: --
 51648/100000: episode: 969, duration: 0.043s, episode steps: 25, steps per second: 583, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.082 [-0.748, 1.359], mean_best_reward: --
 51724/100000: episode: 970, duration: 0.119s, episode steps: 76, steps per second: 639, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.116 [-0.611, 0.869], mean_best_reward: --
 51793/100000: episode: 971, duration: 0.105s, episode steps: 69, steps per second: 656, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.101 [-0.837, 1.045], mean_best_reward: --
 51880/100000: episode: 972, duration: 0.135s, episode steps: 87, steps per second: 645, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.092 [-0.555, 1.058], mean_best_reward: --
 51974/100000: episode: 973, duration: 0.160s, episode steps: 94, steps per second: 587, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.263 [-0.708, 1.848], mean_best_reward: --
 52065/100000: episode: 974, duration: 0.156s, episode steps: 91, steps per second: 583, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.092 [-0.636, 1.027], mean_best_reward: --
 52129/100000: episode: 975, duration: 0.104s, episode steps: 64, steps per second: 615, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.112 [-0.623, 0.981], mean_best_reward: --
 52203/100000: episode: 976, duration: 0.133s, episode steps: 74, steps per second: 557, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.242 [-0.945, 1.490], mean_best_reward: --
 52239/100000: episode: 977, duration: 0.059s, episode steps: 36, steps per second: 613, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.595, 1.278], mean_best_reward: --
 52285/100000: episode: 978, duration: 0.083s, episode steps: 46, steps per second: 556, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.067 [-0.593, 1.376], mean_best_reward: --
 52351/100000: episode: 979, duration: 0.116s, episode steps: 66, steps per second: 571, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.072 [-0.809, 0.619], mean_best_reward: --
 52446/100000: episode: 980, duration: 0.158s, episode steps: 95, steps per second: 601, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.281 [-1.810, 0.641], mean_best_reward: --
 52515/100000: episode: 981, duration: 0.122s, episode steps: 69, steps per second: 563, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.173 [-0.748, 1.157], mean_best_reward: --
 52577/100000: episode: 982, duration: 0.101s, episode steps: 62, steps per second: 615, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.169 [-1.427, 0.771], mean_best_reward: --
 52604/100000: episode: 983, duration: 0.051s, episode steps: 27, steps per second: 533, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.051 [-0.628, 0.859], mean_best_reward: --
 52701/100000: episode: 984, duration: 0.157s, episode steps: 97, steps per second: 619, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.094 [-0.734, 1.178], mean_best_reward: --
 52739/100000: episode: 985, duration: 0.063s, episode steps: 38, steps per second: 599, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.051 [-0.783, 1.105], mean_best_reward: --
 52845/100000: episode: 986, duration: 0.180s, episode steps: 106, steps per second: 590, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.207 [-1.112, 0.921], mean_best_reward: --
 52896/100000: episode: 987, duration: 0.079s, episode steps: 51, steps per second: 649, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.117 [-0.894, 0.749], mean_best_reward: --
 52944/100000: episode: 988, duration: 0.079s, episode steps: 48, steps per second: 608, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.086 [-0.580, 0.843], mean_best_reward: --
 53019/100000: episode: 989, duration: 0.123s, episode steps: 75, steps per second: 608, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.184 [-1.146, 0.566], mean_best_reward: --
 53109/100000: episode: 990, duration: 0.158s, episode steps: 90, steps per second: 570, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.056 [-0.797, 1.039], mean_best_reward: --
 53156/100000: episode: 991, duration: 0.078s, episode steps: 47, steps per second: 600, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.062 [-0.418, 0.804], mean_best_reward: --
 53213/100000: episode: 992, duration: 0.097s, episode steps: 57, steps per second: 585, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.113 [-0.637, 0.923], mean_best_reward: --
 53239/100000: episode: 993, duration: 0.041s, episode steps: 26, steps per second: 632, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.137 [-0.545, 0.948], mean_best_reward: --
 53298/100000: episode: 994, duration: 0.091s, episode steps: 59, steps per second: 648, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.110 [-0.387, 0.928], mean_best_reward: --
 53386/100000: episode: 995, duration: 0.134s, episode steps: 88, steps per second: 656, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.194 [-1.261, 0.611], mean_best_reward: --
 53446/100000: episode: 996, duration: 0.098s, episode steps: 60, steps per second: 614, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.105 [-1.284, 0.625], mean_best_reward: --
 53558/100000: episode: 997, duration: 0.191s, episode steps: 112, steps per second: 588, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.161 [-1.071, 1.475], mean_best_reward: --
 53623/100000: episode: 998, duration: 0.104s, episode steps: 65, steps per second: 622, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.003 [-0.782, 0.951], mean_best_reward: --
 53670/100000: episode: 999, duration: 0.074s, episode steps: 47, steps per second: 636, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.122 [-0.636, 1.001], mean_best_reward: --
 53716/100000: episode: 1000, duration: 0.079s, episode steps: 46, steps per second: 583, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.121, 0.588], mean_best_reward: 181.000000
 53790/100000: episode: 1001, duration: 0.116s, episode steps: 74, steps per second: 639, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.124 [-1.079, 0.896], mean_best_reward: --
 53877/100000: episode: 1002, duration: 0.143s, episode steps: 87, steps per second: 610, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.198 [-0.868, 1.323], mean_best_reward: --
 53933/100000: episode: 1003, duration: 0.096s, episode steps: 56, steps per second: 584, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.071 [-1.000, 0.584], mean_best_reward: --
 53985/100000: episode: 1004, duration: 0.085s, episode steps: 52, steps per second: 611, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.160 [-0.625, 1.117], mean_best_reward: --
 54011/100000: episode: 1005, duration: 0.043s, episode steps: 26, steps per second: 603, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.509, 1.120], mean_best_reward: --
 54056/100000: episode: 1006, duration: 0.072s, episode steps: 45, steps per second: 624, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.091 [-0.605, 0.893], mean_best_reward: --
 54204/100000: episode: 1007, duration: 0.251s, episode steps: 148, steps per second: 591, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.154 [-1.382, 0.817], mean_best_reward: --
 54237/100000: episode: 1008, duration: 0.059s, episode steps: 33, steps per second: 562, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.136 [-0.393, 0.907], mean_best_reward: --
 54269/100000: episode: 1009, duration: 0.055s, episode steps: 32, steps per second: 582, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.612, 0.926], mean_best_reward: --
 54338/100000: episode: 1010, duration: 0.118s, episode steps: 69, steps per second: 586, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.068 [-1.525, 0.632], mean_best_reward: --
 54386/100000: episode: 1011, duration: 0.084s, episode steps: 48, steps per second: 573, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.031 [-0.875, 1.083], mean_best_reward: --
 54467/100000: episode: 1012, duration: 0.128s, episode steps: 81, steps per second: 635, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.000 [-0.716, 0.773], mean_best_reward: --
 54523/100000: episode: 1013, duration: 0.096s, episode steps: 56, steps per second: 582, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.158 [-0.663, 1.106], mean_best_reward: --
 54609/100000: episode: 1014, duration: 0.131s, episode steps: 86, steps per second: 657, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.083 [-1.059, 0.597], mean_best_reward: --
 54754/100000: episode: 1015, duration: 0.249s, episode steps: 145, steps per second: 583, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.136 [-0.928, 0.884], mean_best_reward: --
 54828/100000: episode: 1016, duration: 0.115s, episode steps: 74, steps per second: 641, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.063 [-0.849, 0.967], mean_best_reward: --
 54854/100000: episode: 1017, duration: 0.049s, episode steps: 26, steps per second: 530, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.111 [-0.922, 0.382], mean_best_reward: --
 54897/100000: episode: 1018, duration: 0.072s, episode steps: 43, steps per second: 599, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.086 [-0.453, 1.170], mean_best_reward: --
 54962/100000: episode: 1019, duration: 0.112s, episode steps: 65, steps per second: 580, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.203 [-1.012, 0.800], mean_best_reward: --
 55081/100000: episode: 1020, duration: 0.182s, episode steps: 119, steps per second: 653, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.022 [-1.316, 1.116], mean_best_reward: --
 55163/100000: episode: 1021, duration: 0.142s, episode steps: 82, steps per second: 578, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.110 [-0.700, 1.167], mean_best_reward: --
 55230/100000: episode: 1022, duration: 0.109s, episode steps: 67, steps per second: 615, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.102 [-0.943, 0.969], mean_best_reward: --
 55286/100000: episode: 1023, duration: 0.084s, episode steps: 56, steps per second: 665, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.110 [-0.577, 0.992], mean_best_reward: --
 55336/100000: episode: 1024, duration: 0.079s, episode steps: 50, steps per second: 635, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.126 [-1.097, 0.573], mean_best_reward: --
 55394/100000: episode: 1025, duration: 0.106s, episode steps: 58, steps per second: 549, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.118 [-0.762, 1.055], mean_best_reward: --
 55440/100000: episode: 1026, duration: 0.074s, episode steps: 46, steps per second: 623, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.122 [-0.476, 1.108], mean_best_reward: --
 55481/100000: episode: 1027, duration: 0.077s, episode steps: 41, steps per second: 535, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.107 [-0.418, 0.685], mean_best_reward: --
 55609/100000: episode: 1028, duration: 0.199s, episode steps: 128, steps per second: 643, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.148 [-0.840, 1.384], mean_best_reward: --
 55682/100000: episode: 1029, duration: 0.111s, episode steps: 73, steps per second: 657, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.211 [-0.675, 1.312], mean_best_reward: --
 55770/100000: episode: 1030, duration: 0.134s, episode steps: 88, steps per second: 655, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.130 [-0.732, 1.017], mean_best_reward: --
 55819/100000: episode: 1031, duration: 0.089s, episode steps: 49, steps per second: 552, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.084 [-1.159, 1.019], mean_best_reward: --
 55863/100000: episode: 1032, duration: 0.068s, episode steps: 44, steps per second: 643, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.106 [-0.552, 0.955], mean_best_reward: --
 55977/100000: episode: 1033, duration: 0.194s, episode steps: 114, steps per second: 589, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.096 [-0.883, 1.246], mean_best_reward: --
 56007/100000: episode: 1034, duration: 0.045s, episode steps: 30, steps per second: 663, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.182 [-0.545, 1.036], mean_best_reward: --
 56078/100000: episode: 1035, duration: 0.107s, episode steps: 71, steps per second: 662, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.200 [-1.307, 0.608], mean_best_reward: --
 56171/100000: episode: 1036, duration: 0.144s, episode steps: 93, steps per second: 648, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.204 [-1.306, 0.801], mean_best_reward: --
 56236/100000: episode: 1037, duration: 0.105s, episode steps: 65, steps per second: 618, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.073 [-0.609, 1.137], mean_best_reward: --
 56272/100000: episode: 1038, duration: 0.057s, episode steps: 36, steps per second: 636, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.107 [-0.556, 0.990], mean_best_reward: --
 56403/100000: episode: 1039, duration: 0.205s, episode steps: 131, steps per second: 638, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.105 [-0.731, 1.077], mean_best_reward: --
 56505/100000: episode: 1040, duration: 0.171s, episode steps: 102, steps per second: 598, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.032 [-0.770, 1.092], mean_best_reward: --
 56571/100000: episode: 1041, duration: 0.115s, episode steps: 66, steps per second: 574, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.140 [-0.909, 0.951], mean_best_reward: --
 56665/100000: episode: 1042, duration: 0.142s, episode steps: 94, steps per second: 663, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.154 [-0.693, 1.297], mean_best_reward: --
 56769/100000: episode: 1043, duration: 0.160s, episode steps: 104, steps per second: 651, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.135 [-0.862, 1.166], mean_best_reward: --
 56809/100000: episode: 1044, duration: 0.059s, episode steps: 40, steps per second: 683, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.130 [-0.853, 0.584], mean_best_reward: --
 56893/100000: episode: 1045, duration: 0.124s, episode steps: 84, steps per second: 675, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.243 [-1.108, 0.878], mean_best_reward: --
 56934/100000: episode: 1046, duration: 0.062s, episode steps: 41, steps per second: 656, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.145 [-0.908, 0.367], mean_best_reward: --
 56989/100000: episode: 1047, duration: 0.083s, episode steps: 55, steps per second: 659, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.071 [-1.004, 0.393], mean_best_reward: --
 57037/100000: episode: 1048, duration: 0.081s, episode steps: 48, steps per second: 593, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.031 [-0.575, 0.900], mean_best_reward: --
 57156/100000: episode: 1049, duration: 0.198s, episode steps: 119, steps per second: 602, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.028 [-0.837, 1.021], mean_best_reward: --
 57217/100000: episode: 1050, duration: 0.103s, episode steps: 61, steps per second: 591, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.178 [-0.958, 0.661], mean_best_reward: 152.000000
 57243/100000: episode: 1051, duration: 0.043s, episode steps: 26, steps per second: 610, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.121 [-0.903, 0.392], mean_best_reward: --
 57298/100000: episode: 1052, duration: 0.099s, episode steps: 55, steps per second: 554, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.157 [-1.260, 0.766], mean_best_reward: --
 57385/100000: episode: 1053, duration: 0.139s, episode steps: 87, steps per second: 628, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.030 [-0.622, 1.186], mean_best_reward: --
 57444/100000: episode: 1054, duration: 0.103s, episode steps: 59, steps per second: 575, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.092 [-1.369, 0.614], mean_best_reward: --
 57529/100000: episode: 1055, duration: 0.146s, episode steps: 85, steps per second: 583, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.242 [-1.303, 0.832], mean_best_reward: --
 57592/100000: episode: 1056, duration: 0.112s, episode steps: 63, steps per second: 560, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.114 [-0.452, 1.000], mean_best_reward: --
 57643/100000: episode: 1057, duration: 0.081s, episode steps: 51, steps per second: 631, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.216 [-1.145, 0.549], mean_best_reward: --
 57705/100000: episode: 1058, duration: 0.106s, episode steps: 62, steps per second: 586, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.145 [-1.064, 0.711], mean_best_reward: --
 57760/100000: episode: 1059, duration: 0.099s, episode steps: 55, steps per second: 554, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.187 [-0.561, 1.294], mean_best_reward: --
 57819/100000: episode: 1060, duration: 0.103s, episode steps: 59, steps per second: 574, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.027 [-0.941, 1.152], mean_best_reward: --
 57873/100000: episode: 1061, duration: 0.082s, episode steps: 54, steps per second: 658, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.092 [-0.688, 0.865], mean_best_reward: --
 57915/100000: episode: 1062, duration: 0.061s, episode steps: 42, steps per second: 694, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.135 [-1.316, 0.641], mean_best_reward: --
 57991/100000: episode: 1063, duration: 0.116s, episode steps: 76, steps per second: 653, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.192 [-1.331, 0.701], mean_best_reward: --
 58014/100000: episode: 1064, duration: 0.040s, episode steps: 23, steps per second: 578, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.089 [-0.422, 1.068], mean_best_reward: --
 58090/100000: episode: 1065, duration: 0.137s, episode steps: 76, steps per second: 555, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.539, 1.133], mean_best_reward: --
 58134/100000: episode: 1066, duration: 0.079s, episode steps: 44, steps per second: 554, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.032 [-0.814, 1.074], mean_best_reward: --
 58176/100000: episode: 1067, duration: 0.075s, episode steps: 42, steps per second: 558, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.131 [-0.641, 0.985], mean_best_reward: --
 58214/100000: episode: 1068, duration: 0.068s, episode steps: 38, steps per second: 561, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.140 [-0.390, 1.004], mean_best_reward: --
 58241/100000: episode: 1069, duration: 0.048s, episode steps: 27, steps per second: 563, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.079 [-0.579, 1.302], mean_best_reward: --
 58283/100000: episode: 1070, duration: 0.072s, episode steps: 42, steps per second: 580, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.123 [-0.570, 0.994], mean_best_reward: --
 58299/100000: episode: 1071, duration: 0.028s, episode steps: 16, steps per second: 564, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.112 [-0.761, 1.403], mean_best_reward: --
 58328/100000: episode: 1072, duration: 0.055s, episode steps: 29, steps per second: 529, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.035 [-0.936, 1.319], mean_best_reward: --
 58399/100000: episode: 1073, duration: 0.112s, episode steps: 71, steps per second: 633, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.207 [-0.659, 1.277], mean_best_reward: --
 58448/100000: episode: 1074, duration: 0.074s, episode steps: 49, steps per second: 658, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.144 [-1.194, 0.433], mean_best_reward: --
 58515/100000: episode: 1075, duration: 0.109s, episode steps: 67, steps per second: 615, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.186 [-0.671, 1.103], mean_best_reward: --
 58548/100000: episode: 1076, duration: 0.059s, episode steps: 33, steps per second: 564, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.092 [-1.100, 0.640], mean_best_reward: --
 58589/100000: episode: 1077, duration: 0.074s, episode steps: 41, steps per second: 551, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.196 [-1.098, 0.597], mean_best_reward: --
 58651/100000: episode: 1078, duration: 0.109s, episode steps: 62, steps per second: 570, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.183 [-1.444, 0.674], mean_best_reward: --
 58688/100000: episode: 1079, duration: 0.067s, episode steps: 37, steps per second: 548, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.078 [-0.836, 1.601], mean_best_reward: --
 58715/100000: episode: 1080, duration: 0.046s, episode steps: 27, steps per second: 592, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.038 [-0.579, 1.130], mean_best_reward: --
 58802/100000: episode: 1081, duration: 0.150s, episode steps: 87, steps per second: 582, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.172 [-1.176, 1.175], mean_best_reward: --
 58931/100000: episode: 1082, duration: 0.225s, episode steps: 129, steps per second: 573, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.115 [-0.907, 0.896], mean_best_reward: --
 59032/100000: episode: 1083, duration: 0.156s, episode steps: 101, steps per second: 647, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.110 [-0.841, 0.578], mean_best_reward: --
 59056/100000: episode: 1084, duration: 0.037s, episode steps: 24, steps per second: 643, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.761, 1.386], mean_best_reward: --
 59110/100000: episode: 1085, duration: 0.085s, episode steps: 54, steps per second: 639, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.189 [-0.564, 1.113], mean_best_reward: --
 59168/100000: episode: 1086, duration: 0.095s, episode steps: 58, steps per second: 612, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.114 [-0.790, 1.101], mean_best_reward: --
 59266/100000: episode: 1087, duration: 0.156s, episode steps: 98, steps per second: 628, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.094 [-0.961, 0.927], mean_best_reward: --
 59331/100000: episode: 1088, duration: 0.109s, episode steps: 65, steps per second: 594, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.177 [-0.808, 1.161], mean_best_reward: --
 59384/100000: episode: 1089, duration: 0.085s, episode steps: 53, steps per second: 623, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.114 [-0.970, 0.546], mean_best_reward: --
 59412/100000: episode: 1090, duration: 0.042s, episode steps: 28, steps per second: 671, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.526, 1.053], mean_best_reward: --
 59519/100000: episode: 1091, duration: 0.159s, episode steps: 107, steps per second: 671, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.136 [-1.515, 0.885], mean_best_reward: --
 59621/100000: episode: 1092, duration: 0.170s, episode steps: 102, steps per second: 599, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.097 [-1.086, 0.732], mean_best_reward: --
 59723/100000: episode: 1093, duration: 0.165s, episode steps: 102, steps per second: 618, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.082 [-0.785, 1.386], mean_best_reward: --
 59813/100000: episode: 1094, duration: 0.137s, episode steps: 90, steps per second: 658, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.202 [-1.194, 0.688], mean_best_reward: --
 59868/100000: episode: 1095, duration: 0.084s, episode steps: 55, steps per second: 652, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.156 [-0.850, 1.141], mean_best_reward: --
 59952/100000: episode: 1096, duration: 0.130s, episode steps: 84, steps per second: 646, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.052 [-0.673, 0.935], mean_best_reward: --
 59973/100000: episode: 1097, duration: 0.033s, episode steps: 21, steps per second: 636, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.111 [-1.317, 0.574], mean_best_reward: --
 60092/100000: episode: 1098, duration: 0.206s, episode steps: 119, steps per second: 579, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.143 [-0.998, 0.789], mean_best_reward: --
 60141/100000: episode: 1099, duration: 0.087s, episode steps: 49, steps per second: 560, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.157 [-0.553, 0.969], mean_best_reward: --
 60206/100000: episode: 1100, duration: 0.106s, episode steps: 65, steps per second: 615, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.188 [-1.265, 0.648], mean_best_reward: 155.500000
 60252/100000: episode: 1101, duration: 0.079s, episode steps: 46, steps per second: 583, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.922, 1.079], mean_best_reward: --
 60301/100000: episode: 1102, duration: 0.078s, episode steps: 49, steps per second: 631, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.122 [-0.707, 0.988], mean_best_reward: --
 60335/100000: episode: 1103, duration: 0.055s, episode steps: 34, steps per second: 613, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.060 [-0.625, 0.896], mean_best_reward: --
 60398/100000: episode: 1104, duration: 0.111s, episode steps: 63, steps per second: 570, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.143 [-0.895, 1.161], mean_best_reward: --
 60454/100000: episode: 1105, duration: 0.101s, episode steps: 56, steps per second: 556, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.051 [-0.866, 1.116], mean_best_reward: --
 60516/100000: episode: 1106, duration: 0.096s, episode steps: 62, steps per second: 646, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.214 [-0.567, 1.274], mean_best_reward: --
 60609/100000: episode: 1107, duration: 0.155s, episode steps: 93, steps per second: 601, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.200 [-1.076, 0.825], mean_best_reward: --
 60641/100000: episode: 1108, duration: 0.049s, episode steps: 32, steps per second: 653, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.133 [-1.017, 1.495], mean_best_reward: --
 60687/100000: episode: 1109, duration: 0.081s, episode steps: 46, steps per second: 570, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.064 [-0.857, 1.150], mean_best_reward: --
 60783/100000: episode: 1110, duration: 0.148s, episode steps: 96, steps per second: 647, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.039 [-0.884, 1.089], mean_best_reward: --
 60858/100000: episode: 1111, duration: 0.111s, episode steps: 75, steps per second: 678, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.195 [-1.471, 0.802], mean_best_reward: --
 60944/100000: episode: 1112, duration: 0.145s, episode steps: 86, steps per second: 592, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.148 [-1.297, 0.615], mean_best_reward: --
 60973/100000: episode: 1113, duration: 0.055s, episode steps: 29, steps per second: 528, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.054 [-0.559, 0.985], mean_best_reward: --
 61048/100000: episode: 1114, duration: 0.128s, episode steps: 75, steps per second: 588, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.282 [-0.487, 1.499], mean_best_reward: --
 61100/100000: episode: 1115, duration: 0.080s, episode steps: 52, steps per second: 649, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.095 [-0.602, 0.870], mean_best_reward: --
 61167/100000: episode: 1116, duration: 0.113s, episode steps: 67, steps per second: 593, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.049 [-0.526, 1.198], mean_best_reward: --
 61258/100000: episode: 1117, duration: 0.159s, episode steps: 91, steps per second: 572, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.161 [-0.941, 0.622], mean_best_reward: --
 61301/100000: episode: 1118, duration: 0.066s, episode steps: 43, steps per second: 649, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.085 [-0.722, 1.046], mean_best_reward: --
 61340/100000: episode: 1119, duration: 0.066s, episode steps: 39, steps per second: 589, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.082 [-0.596, 1.252], mean_best_reward: --
 61438/100000: episode: 1120, duration: 0.152s, episode steps: 98, steps per second: 646, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.230 [-1.165, 0.855], mean_best_reward: --
 61494/100000: episode: 1121, duration: 0.101s, episode steps: 56, steps per second: 553, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.178 [-0.551, 1.144], mean_best_reward: --
 61567/100000: episode: 1122, duration: 0.124s, episode steps: 73, steps per second: 589, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.183 [-1.135, 0.720], mean_best_reward: --
 61709/100000: episode: 1123, duration: 0.255s, episode steps: 142, steps per second: 557, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.083 [-1.116, 0.817], mean_best_reward: --
 61775/100000: episode: 1124, duration: 0.118s, episode steps: 66, steps per second: 558, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.094 [-0.575, 1.047], mean_best_reward: --
 61812/100000: episode: 1125, duration: 0.063s, episode steps: 37, steps per second: 584, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.126 [-0.389, 1.003], mean_best_reward: --
 61886/100000: episode: 1126, duration: 0.117s, episode steps: 74, steps per second: 632, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.090 [-0.864, 1.320], mean_best_reward: --
 61953/100000: episode: 1127, duration: 0.104s, episode steps: 67, steps per second: 644, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.132 [-0.701, 1.046], mean_best_reward: --
 62039/100000: episode: 1128, duration: 0.152s, episode steps: 86, steps per second: 565, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.164 [-0.727, 0.984], mean_best_reward: --
 62080/100000: episode: 1129, duration: 0.069s, episode steps: 41, steps per second: 592, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.146 [-0.453, 0.755], mean_best_reward: --
 62127/100000: episode: 1130, duration: 0.078s, episode steps: 47, steps per second: 603, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.131 [-0.429, 0.889], mean_best_reward: --
 62188/100000: episode: 1131, duration: 0.095s, episode steps: 61, steps per second: 644, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.136 [-1.059, 0.634], mean_best_reward: --
 62231/100000: episode: 1132, duration: 0.070s, episode steps: 43, steps per second: 616, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.056 [-0.745, 1.162], mean_best_reward: --
 62286/100000: episode: 1133, duration: 0.084s, episode steps: 55, steps per second: 656, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.111 [-0.512, 1.274], mean_best_reward: --
 62332/100000: episode: 1134, duration: 0.079s, episode steps: 46, steps per second: 585, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.978, 1.176], mean_best_reward: --
 62399/100000: episode: 1135, duration: 0.107s, episode steps: 67, steps per second: 626, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.157 [-1.268, 0.790], mean_best_reward: --
 62463/100000: episode: 1136, duration: 0.111s, episode steps: 64, steps per second: 578, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.196 [-1.137, 0.639], mean_best_reward: --
 62496/100000: episode: 1137, duration: 0.060s, episode steps: 33, steps per second: 550, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.094 [-0.511, 0.963], mean_best_reward: --
 62590/100000: episode: 1138, duration: 0.159s, episode steps: 94, steps per second: 591, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.186 [-1.166, 0.852], mean_best_reward: --
 62647/100000: episode: 1139, duration: 0.098s, episode steps: 57, steps per second: 584, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.153 [-1.049, 0.665], mean_best_reward: --
 62696/100000: episode: 1140, duration: 0.090s, episode steps: 49, steps per second: 547, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.194 [-0.549, 1.140], mean_best_reward: --
 62778/100000: episode: 1141, duration: 0.136s, episode steps: 82, steps per second: 604, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.071 [-0.973, 1.108], mean_best_reward: --
 62820/100000: episode: 1142, duration: 0.073s, episode steps: 42, steps per second: 576, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.073 [-0.956, 0.621], mean_best_reward: --
 62868/100000: episode: 1143, duration: 0.084s, episode steps: 48, steps per second: 575, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.085 [-0.549, 0.889], mean_best_reward: --
 63013/100000: episode: 1144, duration: 0.247s, episode steps: 145, steps per second: 587, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.162 [-1.235, 1.006], mean_best_reward: --
 63118/100000: episode: 1145, duration: 0.180s, episode steps: 105, steps per second: 582, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.227 [-0.890, 1.720], mean_best_reward: --
 63186/100000: episode: 1146, duration: 0.115s, episode steps: 68, steps per second: 591, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.959, 1.106], mean_best_reward: --
 63257/100000: episode: 1147, duration: 0.114s, episode steps: 71, steps per second: 620, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.095 [-0.953, 0.948], mean_best_reward: --
 63291/100000: episode: 1148, duration: 0.063s, episode steps: 34, steps per second: 538, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.118 [-0.905, 0.359], mean_best_reward: --
 63364/100000: episode: 1149, duration: 0.119s, episode steps: 73, steps per second: 614, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.124 [-0.760, 1.297], mean_best_reward: --
 63439/100000: episode: 1150, duration: 0.114s, episode steps: 75, steps per second: 658, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.146 [-0.754, 0.916], mean_best_reward: 148.000000
 63481/100000: episode: 1151, duration: 0.067s, episode steps: 42, steps per second: 629, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.143 [-0.918, 0.500], mean_best_reward: --
 63521/100000: episode: 1152, duration: 0.061s, episode steps: 40, steps per second: 650, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.122 [-0.510, 0.921], mean_best_reward: --
 63547/100000: episode: 1153, duration: 0.039s, episode steps: 26, steps per second: 661, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.045 [-0.947, 1.343], mean_best_reward: --
 63594/100000: episode: 1154, duration: 0.069s, episode steps: 47, steps per second: 683, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.095 [-0.477, 1.304], mean_best_reward: --
 63641/100000: episode: 1155, duration: 0.078s, episode steps: 47, steps per second: 601, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.164 [-0.443, 0.947], mean_best_reward: --
 63686/100000: episode: 1156, duration: 0.074s, episode steps: 45, steps per second: 604, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.081 [-0.564, 1.101], mean_best_reward: --
 63768/100000: episode: 1157, duration: 0.122s, episode steps: 82, steps per second: 670, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.592, 1.049], mean_best_reward: --
 63796/100000: episode: 1158, duration: 0.042s, episode steps: 28, steps per second: 660, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.070 [-0.585, 1.005], mean_best_reward: --
 63824/100000: episode: 1159, duration: 0.045s, episode steps: 28, steps per second: 627, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.357, 0.963], mean_best_reward: --
 63859/100000: episode: 1160, duration: 0.056s, episode steps: 35, steps per second: 628, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.107 [-0.556, 0.868], mean_best_reward: --
 63910/100000: episode: 1161, duration: 0.074s, episode steps: 51, steps per second: 687, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.042 [-0.554, 1.128], mean_best_reward: --
 64033/100000: episode: 1162, duration: 0.183s, episode steps: 123, steps per second: 672, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.172 [-1.324, 0.749], mean_best_reward: --
 64122/100000: episode: 1163, duration: 0.136s, episode steps: 89, steps per second: 656, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.249 [-1.491, 0.737], mean_best_reward: --
 64174/100000: episode: 1164, duration: 0.078s, episode steps: 52, steps per second: 665, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.161 [-0.677, 1.072], mean_best_reward: --
 64211/100000: episode: 1165, duration: 0.058s, episode steps: 37, steps per second: 642, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.135 [-0.399, 0.942], mean_best_reward: --
 64263/100000: episode: 1166, duration: 0.081s, episode steps: 52, steps per second: 643, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.220 [-0.720, 1.324], mean_best_reward: --
 64355/100000: episode: 1167, duration: 0.134s, episode steps: 92, steps per second: 686, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.119 [-1.088, 0.692], mean_best_reward: --
 64431/100000: episode: 1168, duration: 0.114s, episode steps: 76, steps per second: 664, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.153 [-0.590, 1.091], mean_best_reward: --
 64462/100000: episode: 1169, duration: 0.049s, episode steps: 31, steps per second: 632, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.113 [-1.089, 0.443], mean_best_reward: --
 64530/100000: episode: 1170, duration: 0.110s, episode steps: 68, steps per second: 617, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.858, 1.498], mean_best_reward: --
 64627/100000: episode: 1171, duration: 0.166s, episode steps: 97, steps per second: 583, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.161 [-1.040, 0.816], mean_best_reward: --
 64644/100000: episode: 1172, duration: 0.026s, episode steps: 17, steps per second: 643, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.082 [-0.589, 1.065], mean_best_reward: --
 64686/100000: episode: 1173, duration: 0.062s, episode steps: 42, steps per second: 673, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.072 [-0.614, 1.137], mean_best_reward: --
 64713/100000: episode: 1174, duration: 0.046s, episode steps: 27, steps per second: 585, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.055 [-0.631, 1.008], mean_best_reward: --
 64772/100000: episode: 1175, duration: 0.096s, episode steps: 59, steps per second: 617, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.063 [-0.788, 1.004], mean_best_reward: --
 64859/100000: episode: 1176, duration: 0.139s, episode steps: 87, steps per second: 624, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.030 [-0.949, 1.228], mean_best_reward: --
 64912/100000: episode: 1177, duration: 0.080s, episode steps: 53, steps per second: 663, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.177 [-0.864, 0.962], mean_best_reward: --
 64956/100000: episode: 1178, duration: 0.069s, episode steps: 44, steps per second: 639, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.044, 0.721], mean_best_reward: --
 65011/100000: episode: 1179, duration: 0.090s, episode steps: 55, steps per second: 613, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.093 [-0.814, 1.259], mean_best_reward: --
 65041/100000: episode: 1180, duration: 0.049s, episode steps: 30, steps per second: 611, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.784, 1.202], mean_best_reward: --
 65080/100000: episode: 1181, duration: 0.071s, episode steps: 39, steps per second: 550, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.090], mean_best_reward: --
 65268/100000: episode: 1182, duration: 0.279s, episode steps: 188, steps per second: 673, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.024 [-1.143, 1.154], mean_best_reward: --
 65405/100000: episode: 1183, duration: 0.228s, episode steps: 137, steps per second: 600, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.129 [-0.954, 0.867], mean_best_reward: --
 65448/100000: episode: 1184, duration: 0.071s, episode steps: 43, steps per second: 608, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.142 [-0.991, 0.611], mean_best_reward: --
 65495/100000: episode: 1185, duration: 0.076s, episode steps: 47, steps per second: 614, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.082 [-0.610, 1.031], mean_best_reward: --
 65531/100000: episode: 1186, duration: 0.053s, episode steps: 36, steps per second: 677, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.037 [-0.811, 1.094], mean_best_reward: --
 65602/100000: episode: 1187, duration: 0.115s, episode steps: 71, steps per second: 616, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.217 [-0.740, 1.494], mean_best_reward: --
 65642/100000: episode: 1188, duration: 0.067s, episode steps: 40, steps per second: 599, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.066 [-0.603, 1.014], mean_best_reward: --
 65696/100000: episode: 1189, duration: 0.092s, episode steps: 54, steps per second: 585, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.116 [-0.585, 0.897], mean_best_reward: --
 65792/100000: episode: 1190, duration: 0.138s, episode steps: 96, steps per second: 698, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.095 [-0.938, 0.672], mean_best_reward: --
 65919/100000: episode: 1191, duration: 0.206s, episode steps: 127, steps per second: 617, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.158 [-1.306, 1.105], mean_best_reward: --
 65949/100000: episode: 1192, duration: 0.048s, episode steps: 30, steps per second: 631, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.089 [-0.643, 1.085], mean_best_reward: --
 66010/100000: episode: 1193, duration: 0.095s, episode steps: 61, steps per second: 639, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.216 [-0.865, 1.095], mean_best_reward: --
 66052/100000: episode: 1194, duration: 0.072s, episode steps: 42, steps per second: 583, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.441, 1.025], mean_best_reward: --
 66093/100000: episode: 1195, duration: 0.073s, episode steps: 41, steps per second: 560, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.083 [-1.113, 0.584], mean_best_reward: --
 66161/100000: episode: 1196, duration: 0.102s, episode steps: 68, steps per second: 667, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.223 [-1.260, 0.591], mean_best_reward: --
 66202/100000: episode: 1197, duration: 0.068s, episode steps: 41, steps per second: 605, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.163 [-0.633, 1.072], mean_best_reward: --
 66240/100000: episode: 1198, duration: 0.061s, episode steps: 38, steps per second: 623, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.118 [-0.578, 0.924], mean_best_reward: --
 66342/100000: episode: 1199, duration: 0.165s, episode steps: 102, steps per second: 618, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.098 [-0.921, 0.768], mean_best_reward: --
 66390/100000: episode: 1200, duration: 0.077s, episode steps: 48, steps per second: 622, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.065 [-0.785, 1.005], mean_best_reward: 171.500000
 66431/100000: episode: 1201, duration: 0.074s, episode steps: 41, steps per second: 551, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.068 [-0.513, 1.182], mean_best_reward: --
 66479/100000: episode: 1202, duration: 0.074s, episode steps: 48, steps per second: 645, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.131 [-0.554, 0.962], mean_best_reward: --
 66510/100000: episode: 1203, duration: 0.039s, episode steps: 31, steps per second: 802, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.138 [-0.925, 0.635], mean_best_reward: --
 66546/100000: episode: 1204, duration: 0.044s, episode steps: 36, steps per second: 814, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.820, 1.083], mean_best_reward: --
 66640/100000: episode: 1205, duration: 0.158s, episode steps: 94, steps per second: 596, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.167 [-1.401, 0.971], mean_best_reward: --
 66715/100000: episode: 1206, duration: 0.126s, episode steps: 75, steps per second: 593, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.243 [-0.716, 1.498], mean_best_reward: --
 66763/100000: episode: 1207, duration: 0.071s, episode steps: 48, steps per second: 674, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.165 [-0.410, 1.085], mean_best_reward: --
 66811/100000: episode: 1208, duration: 0.072s, episode steps: 48, steps per second: 668, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.223 [-0.583, 1.319], mean_best_reward: --
 66948/100000: episode: 1209, duration: 0.223s, episode steps: 137, steps per second: 616, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.160 [-1.044, 1.036], mean_best_reward: --
 66991/100000: episode: 1210, duration: 0.066s, episode steps: 43, steps per second: 653, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.028 [-0.750, 1.120], mean_best_reward: --
 67051/100000: episode: 1211, duration: 0.111s, episode steps: 60, steps per second: 540, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.108 [-0.535, 0.987], mean_best_reward: --
 67127/100000: episode: 1212, duration: 0.133s, episode steps: 76, steps per second: 572, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.127 [-0.880, 1.008], mean_best_reward: --
 67205/100000: episode: 1213, duration: 0.133s, episode steps: 78, steps per second: 587, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.128 [-0.890, 1.159], mean_best_reward: --
 67270/100000: episode: 1214, duration: 0.121s, episode steps: 65, steps per second: 538, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.153 [-0.662, 1.004], mean_best_reward: --
 67375/100000: episode: 1215, duration: 0.178s, episode steps: 105, steps per second: 590, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.156 [-0.704, 1.106], mean_best_reward: --
 67447/100000: episode: 1216, duration: 0.119s, episode steps: 72, steps per second: 603, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.104 [-0.745, 1.084], mean_best_reward: --
 67512/100000: episode: 1217, duration: 0.108s, episode steps: 65, steps per second: 603, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.126 [-0.719, 1.141], mean_best_reward: --
 67530/100000: episode: 1218, duration: 0.032s, episode steps: 18, steps per second: 560, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.737, 1.294], mean_best_reward: --
 67622/100000: episode: 1219, duration: 0.146s, episode steps: 92, steps per second: 630, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.194 [-0.509, 1.457], mean_best_reward: --
 67654/100000: episode: 1220, duration: 0.054s, episode steps: 32, steps per second: 588, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.125 [-0.547, 1.066], mean_best_reward: --
 67674/100000: episode: 1221, duration: 0.039s, episode steps: 20, steps per second: 518, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.803, 1.242], mean_best_reward: --
 67749/100000: episode: 1222, duration: 0.122s, episode steps: 75, steps per second: 615, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.067 [-0.907, 0.837], mean_best_reward: --
 67817/100000: episode: 1223, duration: 0.112s, episode steps: 68, steps per second: 610, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.137 [-1.117, 0.594], mean_best_reward: --
 67848/100000: episode: 1224, duration: 0.058s, episode steps: 31, steps per second: 531, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.058 [-0.562, 0.957], mean_best_reward: --
 67907/100000: episode: 1225, duration: 0.102s, episode steps: 59, steps per second: 579, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.143 [-0.879, 0.646], mean_best_reward: --
 67948/100000: episode: 1226, duration: 0.071s, episode steps: 41, steps per second: 577, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.109 [-0.447, 0.982], mean_best_reward: --
 67993/100000: episode: 1227, duration: 0.080s, episode steps: 45, steps per second: 563, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.174 [-0.986, 0.503], mean_best_reward: --
 68093/100000: episode: 1228, duration: 0.176s, episode steps: 100, steps per second: 568, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.212 [-1.080, 0.829], mean_best_reward: --
 68130/100000: episode: 1229, duration: 0.066s, episode steps: 37, steps per second: 562, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.071 [-0.810, 1.351], mean_best_reward: --
 68155/100000: episode: 1230, duration: 0.039s, episode steps: 25, steps per second: 640, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.101 [-0.610, 0.945], mean_best_reward: --
 68254/100000: episode: 1231, duration: 0.171s, episode steps: 99, steps per second: 577, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.078 [-1.715, 0.840], mean_best_reward: --
 68322/100000: episode: 1232, duration: 0.114s, episode steps: 68, steps per second: 597, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.178 [-0.618, 1.256], mean_best_reward: --
 68363/100000: episode: 1233, duration: 0.062s, episode steps: 41, steps per second: 666, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.180 [-0.568, 0.988], mean_best_reward: --
 68407/100000: episode: 1234, duration: 0.073s, episode steps: 44, steps per second: 601, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.059 [-0.421, 1.053], mean_best_reward: --
 68452/100000: episode: 1235, duration: 0.083s, episode steps: 45, steps per second: 542, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.173 [-0.592, 1.107], mean_best_reward: --
 68503/100000: episode: 1236, duration: 0.077s, episode steps: 51, steps per second: 660, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.107 [-0.567, 1.097], mean_best_reward: --
 68611/100000: episode: 1237, duration: 0.171s, episode steps: 108, steps per second: 632, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.186 [-1.484, 0.770], mean_best_reward: --
 68640/100000: episode: 1238, duration: 0.044s, episode steps: 29, steps per second: 659, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.094 [-0.412, 0.830], mean_best_reward: --
 68729/100000: episode: 1239, duration: 0.156s, episode steps: 89, steps per second: 571, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.063 [-1.064, 1.045], mean_best_reward: --
 68793/100000: episode: 1240, duration: 0.112s, episode steps: 64, steps per second: 573, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.059 [-0.786, 1.498], mean_best_reward: --
 68851/100000: episode: 1241, duration: 0.098s, episode steps: 58, steps per second: 589, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.142 [-0.474, 0.983], mean_best_reward: --
 68903/100000: episode: 1242, duration: 0.089s, episode steps: 52, steps per second: 587, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.156 [-1.130, 0.767], mean_best_reward: --
 68951/100000: episode: 1243, duration: 0.082s, episode steps: 48, steps per second: 586, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.122 [-0.764, 0.911], mean_best_reward: --
 69056/100000: episode: 1244, duration: 0.175s, episode steps: 105, steps per second: 601, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.080 [-1.040, 0.817], mean_best_reward: --
 69115/100000: episode: 1245, duration: 0.113s, episode steps: 59, steps per second: 522, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.029 [-1.014, 0.949], mean_best_reward: --
 69165/100000: episode: 1246, duration: 0.077s, episode steps: 50, steps per second: 653, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.142 [-0.358, 0.963], mean_best_reward: --
 69200/100000: episode: 1247, duration: 0.062s, episode steps: 35, steps per second: 565, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.132 [-0.441, 0.828], mean_best_reward: --
 69239/100000: episode: 1248, duration: 0.066s, episode steps: 39, steps per second: 590, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.076 [-0.849, 0.474], mean_best_reward: --
 69286/100000: episode: 1249, duration: 0.082s, episode steps: 47, steps per second: 573, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.079 [-0.432, 1.146], mean_best_reward: --
 69332/100000: episode: 1250, duration: 0.075s, episode steps: 46, steps per second: 615, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.953, 1.312], mean_best_reward: 179.500000
 69386/100000: episode: 1251, duration: 0.098s, episode steps: 54, steps per second: 550, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-0.797, 1.106], mean_best_reward: --
 69488/100000: episode: 1252, duration: 0.167s, episode steps: 102, steps per second: 612, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.101 [-0.787, 1.052], mean_best_reward: --
 69506/100000: episode: 1253, duration: 0.029s, episode steps: 18, steps per second: 627, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.357, 0.765], mean_best_reward: --
 69548/100000: episode: 1254, duration: 0.064s, episode steps: 42, steps per second: 654, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.056 [-0.563, 1.246], mean_best_reward: --
 69591/100000: episode: 1255, duration: 0.067s, episode steps: 43, steps per second: 639, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.178 [-1.105, 0.396], mean_best_reward: --
 69665/100000: episode: 1256, duration: 0.125s, episode steps: 74, steps per second: 591, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.194 [-0.650, 0.952], mean_best_reward: --
 69732/100000: episode: 1257, duration: 0.110s, episode steps: 67, steps per second: 610, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.083 [-0.832, 1.107], mean_best_reward: --
 69785/100000: episode: 1258, duration: 0.078s, episode steps: 53, steps per second: 680, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.123 [-1.252, 0.918], mean_best_reward: --
 69845/100000: episode: 1259, duration: 0.091s, episode steps: 60, steps per second: 658, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.095 [-0.842, 0.629], mean_best_reward: --
 69892/100000: episode: 1260, duration: 0.082s, episode steps: 47, steps per second: 575, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.204 [-0.489, 1.285], mean_best_reward: --
 69960/100000: episode: 1261, duration: 0.117s, episode steps: 68, steps per second: 580, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.074 [-1.086, 0.456], mean_best_reward: --
 70019/100000: episode: 1262, duration: 0.102s, episode steps: 59, steps per second: 577, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.112 [-0.502, 0.985], mean_best_reward: --
 70154/100000: episode: 1263, duration: 0.229s, episode steps: 135, steps per second: 588, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.015 [-0.808, 1.071], mean_best_reward: --
 70179/100000: episode: 1264, duration: 0.042s, episode steps: 25, steps per second: 592, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.117 [-0.596, 1.116], mean_best_reward: --
 70216/100000: episode: 1265, duration: 0.064s, episode steps: 37, steps per second: 581, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.087 [-0.596, 1.030], mean_best_reward: --
 70260/100000: episode: 1266, duration: 0.070s, episode steps: 44, steps per second: 633, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.962, 1.197], mean_best_reward: --
 70319/100000: episode: 1267, duration: 0.099s, episode steps: 59, steps per second: 594, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.066 [-1.187, 0.755], mean_best_reward: --
 70395/100000: episode: 1268, duration: 0.129s, episode steps: 76, steps per second: 587, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.085 [-0.615, 0.902], mean_best_reward: --
 70449/100000: episode: 1269, duration: 0.095s, episode steps: 54, steps per second: 567, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.123 [-0.958, 0.560], mean_best_reward: --
 70562/100000: episode: 1270, duration: 0.191s, episode steps: 113, steps per second: 593, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.002 [-1.056, 1.198], mean_best_reward: --
 70577/100000: episode: 1271, duration: 0.028s, episode steps: 15, steps per second: 540, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.086 [-0.825, 1.364], mean_best_reward: --
 70618/100000: episode: 1272, duration: 0.077s, episode steps: 41, steps per second: 530, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.062 [-0.620, 1.299], mean_best_reward: --
 70709/100000: episode: 1273, duration: 0.153s, episode steps: 91, steps per second: 597, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.126 [-1.299, 0.858], mean_best_reward: --
 70824/100000: episode: 1274, duration: 0.178s, episode steps: 115, steps per second: 646, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.182 [-1.186, 0.955], mean_best_reward: --
 70929/100000: episode: 1275, duration: 0.181s, episode steps: 105, steps per second: 581, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.018 [-1.327, 1.125], mean_best_reward: --
 70958/100000: episode: 1276, duration: 0.050s, episode steps: 29, steps per second: 583, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.067 [-0.438, 0.864], mean_best_reward: --
 71076/100000: episode: 1277, duration: 0.202s, episode steps: 118, steps per second: 583, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.139 [-1.546, 1.153], mean_best_reward: --
 71116/100000: episode: 1278, duration: 0.066s, episode steps: 40, steps per second: 603, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.046 [-0.801, 1.461], mean_best_reward: --
 71169/100000: episode: 1279, duration: 0.097s, episode steps: 53, steps per second: 545, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.093 [-0.795, 0.705], mean_best_reward: --
 71282/100000: episode: 1280, duration: 0.198s, episode steps: 113, steps per second: 570, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.172 [-1.277, 0.937], mean_best_reward: --
 71333/100000: episode: 1281, duration: 0.083s, episode steps: 51, steps per second: 618, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.171 [-0.440, 0.908], mean_best_reward: --
 71375/100000: episode: 1282, duration: 0.074s, episode steps: 42, steps per second: 568, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.112 [-0.889, 0.545], mean_best_reward: --
 71475/100000: episode: 1283, duration: 0.164s, episode steps: 100, steps per second: 608, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.182 [-1.076, 0.684], mean_best_reward: --
 71529/100000: episode: 1284, duration: 0.091s, episode steps: 54, steps per second: 592, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.027 [-0.834, 0.582], mean_best_reward: --
 71588/100000: episode: 1285, duration: 0.091s, episode steps: 59, steps per second: 648, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.142 [-0.682, 1.369], mean_best_reward: --
 71685/100000: episode: 1286, duration: 0.160s, episode steps: 97, steps per second: 606, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.090 [-1.080, 0.802], mean_best_reward: --
 71748/100000: episode: 1287, duration: 0.097s, episode steps: 63, steps per second: 647, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.101 [-1.135, 0.669], mean_best_reward: --
 71857/100000: episode: 1288, duration: 0.166s, episode steps: 109, steps per second: 656, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.115 [-1.487, 1.030], mean_best_reward: --
 71877/100000: episode: 1289, duration: 0.036s, episode steps: 20, steps per second: 551, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.109 [-0.791, 1.235], mean_best_reward: --
 71935/100000: episode: 1290, duration: 0.103s, episode steps: 58, steps per second: 562, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.195 [-1.183, 0.996], mean_best_reward: --
 72055/100000: episode: 1291, duration: 0.209s, episode steps: 120, steps per second: 574, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.117 [-1.120, 0.881], mean_best_reward: --
 72109/100000: episode: 1292, duration: 0.103s, episode steps: 54, steps per second: 526, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.166 [-0.508, 1.161], mean_best_reward: --
 72142/100000: episode: 1293, duration: 0.052s, episode steps: 33, steps per second: 641, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.154 [-0.553, 1.050], mean_best_reward: --
 72223/100000: episode: 1294, duration: 0.127s, episode steps: 81, steps per second: 636, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.137 [-1.041, 1.257], mean_best_reward: --
 72356/100000: episode: 1295, duration: 0.224s, episode steps: 133, steps per second: 594, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.074 [-1.287, 0.770], mean_best_reward: --
 72411/100000: episode: 1296, duration: 0.090s, episode steps: 55, steps per second: 610, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.099 [-0.702, 1.179], mean_best_reward: --
 72550/100000: episode: 1297, duration: 0.241s, episode steps: 139, steps per second: 576, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.153 [-1.533, 0.871], mean_best_reward: --
 72637/100000: episode: 1298, duration: 0.144s, episode steps: 87, steps per second: 606, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.164 [-1.270, 0.711], mean_best_reward: --
 72708/100000: episode: 1299, duration: 0.120s, episode steps: 71, steps per second: 593, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.135 [-0.862, 0.589], mean_best_reward: --
 72844/100000: episode: 1300, duration: 0.232s, episode steps: 136, steps per second: 585, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.049 [-0.744, 1.120], mean_best_reward: 153.000000
 72921/100000: episode: 1301, duration: 0.127s, episode steps: 77, steps per second: 608, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.008 [-0.781, 1.113], mean_best_reward: --
 72938/100000: episode: 1302, duration: 0.023s, episode steps: 17, steps per second: 733, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.088 [-0.606, 1.137], mean_best_reward: --
 72997/100000: episode: 1303, duration: 0.091s, episode steps: 59, steps per second: 652, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.110 [-0.568, 0.930], mean_best_reward: --
 73112/100000: episode: 1304, duration: 0.200s, episode steps: 115, steps per second: 576, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.123 [-0.739, 1.344], mean_best_reward: --
 73182/100000: episode: 1305, duration: 0.122s, episode steps: 70, steps per second: 572, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.211 [-0.684, 1.473], mean_best_reward: --
 73201/100000: episode: 1306, duration: 0.033s, episode steps: 19, steps per second: 573, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.118 [-0.378, 1.011], mean_best_reward: --
 73278/100000: episode: 1307, duration: 0.134s, episode steps: 77, steps per second: 576, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.019 [-0.792, 0.914], mean_best_reward: --
 73305/100000: episode: 1308, duration: 0.046s, episode steps: 27, steps per second: 589, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.098 [-0.957, 0.637], mean_best_reward: --
 73367/100000: episode: 1309, duration: 0.092s, episode steps: 62, steps per second: 677, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.093 [-0.728, 0.872], mean_best_reward: --
 73399/100000: episode: 1310, duration: 0.050s, episode steps: 32, steps per second: 642, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.136 [-0.549, 1.242], mean_best_reward: --
 73443/100000: episode: 1311, duration: 0.071s, episode steps: 44, steps per second: 623, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.095 [-0.361, 0.898], mean_best_reward: --
 73472/100000: episode: 1312, duration: 0.050s, episode steps: 29, steps per second: 579, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.053 [-0.551, 1.158], mean_best_reward: --
 73603/100000: episode: 1313, duration: 0.220s, episode steps: 131, steps per second: 596, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.087 [-0.982, 0.861], mean_best_reward: --
 73696/100000: episode: 1314, duration: 0.155s, episode steps: 93, steps per second: 599, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.244 [-1.647, 1.037], mean_best_reward: --
 73739/100000: episode: 1315, duration: 0.075s, episode steps: 43, steps per second: 573, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.123 [-0.373, 0.735], mean_best_reward: --
 73773/100000: episode: 1316, duration: 0.052s, episode steps: 34, steps per second: 659, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.237, 0.553], mean_best_reward: --
 73884/100000: episode: 1317, duration: 0.165s, episode steps: 111, steps per second: 671, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.010 [-0.738, 1.111], mean_best_reward: --
 73946/100000: episode: 1318, duration: 0.102s, episode steps: 62, steps per second: 609, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.080 [-0.708, 1.089], mean_best_reward: --
 73982/100000: episode: 1319, duration: 0.064s, episode steps: 36, steps per second: 566, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.116 [-0.821, 1.199], mean_best_reward: --
 74021/100000: episode: 1320, duration: 0.062s, episode steps: 39, steps per second: 633, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.125 [-0.764, 1.189], mean_best_reward: --
 74074/100000: episode: 1321, duration: 0.098s, episode steps: 53, steps per second: 542, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.160 [-0.954, 0.475], mean_best_reward: --
 74144/100000: episode: 1322, duration: 0.122s, episode steps: 70, steps per second: 573, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.176 [-1.300, 0.547], mean_best_reward: --
 74177/100000: episode: 1323, duration: 0.057s, episode steps: 33, steps per second: 576, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.169 [-0.558, 1.003], mean_best_reward: --
 74227/100000: episode: 1324, duration: 0.083s, episode steps: 50, steps per second: 604, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.212 [-0.818, 1.293], mean_best_reward: --
 74335/100000: episode: 1325, duration: 0.166s, episode steps: 108, steps per second: 649, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.108 [-1.062, 0.717], mean_best_reward: --
 74389/100000: episode: 1326, duration: 0.090s, episode steps: 54, steps per second: 599, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.114 [-0.758, 1.066], mean_best_reward: --
 74423/100000: episode: 1327, duration: 0.053s, episode steps: 34, steps per second: 641, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-0.937, 0.549], mean_best_reward: --
 74489/100000: episode: 1328, duration: 0.105s, episode steps: 66, steps per second: 626, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.142 [-0.376, 1.119], mean_best_reward: --
 74560/100000: episode: 1329, duration: 0.119s, episode steps: 71, steps per second: 597, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.024 [-1.135, 0.932], mean_best_reward: --
 74635/100000: episode: 1330, duration: 0.115s, episode steps: 75, steps per second: 650, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.098 [-1.299, 0.611], mean_best_reward: --
 74677/100000: episode: 1331, duration: 0.075s, episode steps: 42, steps per second: 563, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.220 [-1.300, 0.385], mean_best_reward: --
 74706/100000: episode: 1332, duration: 0.050s, episode steps: 29, steps per second: 580, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.106 [-0.390, 1.271], mean_best_reward: --
 74754/100000: episode: 1333, duration: 0.080s, episode steps: 48, steps per second: 600, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.098 [-0.686, 0.950], mean_best_reward: --
 74774/100000: episode: 1334, duration: 0.037s, episode steps: 20, steps per second: 538, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.636, 1.229], mean_best_reward: --
 74828/100000: episode: 1335, duration: 0.091s, episode steps: 54, steps per second: 592, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.988, 1.035], mean_best_reward: --
 74875/100000: episode: 1336, duration: 0.080s, episode steps: 47, steps per second: 585, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.175 [-0.574, 1.310], mean_best_reward: --
 74914/100000: episode: 1337, duration: 0.065s, episode steps: 39, steps per second: 597, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.112 [-0.689, 0.921], mean_best_reward: --
 74952/100000: episode: 1338, duration: 0.071s, episode steps: 38, steps per second: 533, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.149 [-0.553, 0.950], mean_best_reward: --
 74998/100000: episode: 1339, duration: 0.074s, episode steps: 46, steps per second: 621, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.129 [-0.462, 0.983], mean_best_reward: --
 75068/100000: episode: 1340, duration: 0.108s, episode steps: 70, steps per second: 647, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.185 [-1.623, 1.013], mean_best_reward: --
 75108/100000: episode: 1341, duration: 0.062s, episode steps: 40, steps per second: 641, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.087 [-0.516, 0.947], mean_best_reward: --
 75153/100000: episode: 1342, duration: 0.068s, episode steps: 45, steps per second: 663, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.191 [-0.603, 0.984], mean_best_reward: --
 75193/100000: episode: 1343, duration: 0.062s, episode steps: 40, steps per second: 647, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.159 [-0.554, 1.090], mean_best_reward: --
 75231/100000: episode: 1344, duration: 0.055s, episode steps: 38, steps per second: 696, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.092 [-0.648, 1.507], mean_best_reward: --
 75355/100000: episode: 1345, duration: 0.190s, episode steps: 124, steps per second: 652, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.133 [-0.953, 0.960], mean_best_reward: --
 75408/100000: episode: 1346, duration: 0.088s, episode steps: 53, steps per second: 605, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.122 [-0.782, 0.789], mean_best_reward: --
 75475/100000: episode: 1347, duration: 0.102s, episode steps: 67, steps per second: 654, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.094 [-0.734, 1.090], mean_best_reward: --
 75515/100000: episode: 1348, duration: 0.061s, episode steps: 40, steps per second: 658, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.150 [-0.585, 0.865], mean_best_reward: --
 75576/100000: episode: 1349, duration: 0.109s, episode steps: 61, steps per second: 559, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.060 [-0.622, 1.037], mean_best_reward: --
 75642/100000: episode: 1350, duration: 0.122s, episode steps: 66, steps per second: 539, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.095 [-1.114, 0.777], mean_best_reward: 142.500000
 75680/100000: episode: 1351, duration: 0.067s, episode steps: 38, steps per second: 564, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.105 [-0.755, 1.050], mean_best_reward: --
 75751/100000: episode: 1352, duration: 0.115s, episode steps: 71, steps per second: 620, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.168 [-1.122, 1.127], mean_best_reward: --
 75821/100000: episode: 1353, duration: 0.112s, episode steps: 70, steps per second: 626, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.095 [-0.541, 0.888], mean_best_reward: --
 75883/100000: episode: 1354, duration: 0.099s, episode steps: 62, steps per second: 628, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.128 [-0.825, 1.221], mean_best_reward: --
 75942/100000: episode: 1355, duration: 0.090s, episode steps: 59, steps per second: 657, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.010 [-0.808, 1.071], mean_best_reward: --
 75957/100000: episode: 1356, duration: 0.025s, episode steps: 15, steps per second: 593, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.124 [-1.271, 0.752], mean_best_reward: --
 76085/100000: episode: 1357, duration: 0.193s, episode steps: 128, steps per second: 665, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.539, 0.735], mean_best_reward: --
 76148/100000: episode: 1358, duration: 0.105s, episode steps: 63, steps per second: 602, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.105 [-0.967, 0.835], mean_best_reward: --
 76184/100000: episode: 1359, duration: 0.055s, episode steps: 36, steps per second: 653, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.128 [-0.503, 1.155], mean_best_reward: --
 76202/100000: episode: 1360, duration: 0.033s, episode steps: 18, steps per second: 544, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.088 [-0.597, 1.022], mean_best_reward: --
 76259/100000: episode: 1361, duration: 0.098s, episode steps: 57, steps per second: 581, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.148 [-0.641, 1.047], mean_best_reward: --
 76315/100000: episode: 1362, duration: 0.096s, episode steps: 56, steps per second: 583, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.052 [-0.623, 1.010], mean_best_reward: --
 76357/100000: episode: 1363, duration: 0.068s, episode steps: 42, steps per second: 616, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.234 [-1.346, 0.739], mean_best_reward: --
 76386/100000: episode: 1364, duration: 0.045s, episode steps: 29, steps per second: 638, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.072 [-0.435, 0.949], mean_best_reward: --
 76472/100000: episode: 1365, duration: 0.147s, episode steps: 86, steps per second: 584, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.239 [-0.703, 1.472], mean_best_reward: --
 76506/100000: episode: 1366, duration: 0.055s, episode steps: 34, steps per second: 623, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.079 [-0.737, 1.242], mean_best_reward: --
 76565/100000: episode: 1367, duration: 0.104s, episode steps: 59, steps per second: 569, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.187 [-0.561, 1.132], mean_best_reward: --
 76638/100000: episode: 1368, duration: 0.115s, episode steps: 73, steps per second: 637, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.165 [-0.709, 1.138], mean_best_reward: --
 76702/100000: episode: 1369, duration: 0.102s, episode steps: 64, steps per second: 626, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.220 [-1.620, 0.671], mean_best_reward: --
 76734/100000: episode: 1370, duration: 0.049s, episode steps: 32, steps per second: 660, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.161 [-0.544, 0.879], mean_best_reward: --
 76789/100000: episode: 1371, duration: 0.087s, episode steps: 55, steps per second: 630, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.144 [-0.595, 0.875], mean_best_reward: --
 76926/100000: episode: 1372, duration: 0.218s, episode steps: 137, steps per second: 628, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.132 [-1.118, 0.715], mean_best_reward: --
 76981/100000: episode: 1373, duration: 0.096s, episode steps: 55, steps per second: 573, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.122 [-0.605, 1.381], mean_best_reward: --
 77008/100000: episode: 1374, duration: 0.046s, episode steps: 27, steps per second: 592, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.082 [-0.409, 0.972], mean_best_reward: --
 77095/100000: episode: 1375, duration: 0.129s, episode steps: 87, steps per second: 674, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.181 [-1.185, 0.658], mean_best_reward: --
 77140/100000: episode: 1376, duration: 0.075s, episode steps: 45, steps per second: 602, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.087 [-0.636, 0.875], mean_best_reward: --
 77223/100000: episode: 1377, duration: 0.134s, episode steps: 83, steps per second: 618, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.183 [-0.961, 0.689], mean_best_reward: --
 77260/100000: episode: 1378, duration: 0.063s, episode steps: 37, steps per second: 584, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.144 [-0.437, 0.886], mean_best_reward: --
 77326/100000: episode: 1379, duration: 0.112s, episode steps: 66, steps per second: 587, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.156 [-0.583, 0.945], mean_best_reward: --
 77398/100000: episode: 1380, duration: 0.116s, episode steps: 72, steps per second: 621, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.097 [-0.791, 1.278], mean_best_reward: --
 77463/100000: episode: 1381, duration: 0.114s, episode steps: 65, steps per second: 570, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.205 [-0.825, 1.622], mean_best_reward: --
 77499/100000: episode: 1382, duration: 0.066s, episode steps: 36, steps per second: 549, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.127 [-0.833, 0.456], mean_best_reward: --
 77548/100000: episode: 1383, duration: 0.079s, episode steps: 49, steps per second: 621, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.124 [-1.166, 0.630], mean_best_reward: --
 77605/100000: episode: 1384, duration: 0.092s, episode steps: 57, steps per second: 621, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.070 [-1.075, 0.892], mean_best_reward: --
 77665/100000: episode: 1385, duration: 0.092s, episode steps: 60, steps per second: 649, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.053 [-0.887, 1.024], mean_best_reward: --
 77732/100000: episode: 1386, duration: 0.102s, episode steps: 67, steps per second: 658, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.164 [-0.600, 1.100], mean_best_reward: --
 77750/100000: episode: 1387, duration: 0.033s, episode steps: 18, steps per second: 553, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.127 [-0.572, 0.929], mean_best_reward: --
 77781/100000: episode: 1388, duration: 0.049s, episode steps: 31, steps per second: 634, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.128 [-0.589, 0.903], mean_best_reward: --
 77856/100000: episode: 1389, duration: 0.116s, episode steps: 75, steps per second: 644, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.266 [-1.652, 0.636], mean_best_reward: --
 77953/100000: episode: 1390, duration: 0.149s, episode steps: 97, steps per second: 649, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.175 [-0.886, 1.283], mean_best_reward: --
 78015/100000: episode: 1391, duration: 0.105s, episode steps: 62, steps per second: 593, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.153 [-0.877, 1.222], mean_best_reward: --
 78035/100000: episode: 1392, duration: 0.033s, episode steps: 20, steps per second: 612, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.649, 1.255], mean_best_reward: --
 78120/100000: episode: 1393, duration: 0.130s, episode steps: 85, steps per second: 653, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.089 [-1.346, 0.982], mean_best_reward: --
 78146/100000: episode: 1394, duration: 0.040s, episode steps: 26, steps per second: 647, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.813, 1.175], mean_best_reward: --
 78216/100000: episode: 1395, duration: 0.107s, episode steps: 70, steps per second: 652, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.141 [-0.726, 1.340], mean_best_reward: --
 78252/100000: episode: 1396, duration: 0.054s, episode steps: 36, steps per second: 661, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.138 [-0.546, 0.970], mean_best_reward: --
 78302/100000: episode: 1397, duration: 0.078s, episode steps: 50, steps per second: 639, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.084 [-0.576, 0.838], mean_best_reward: --
 78374/100000: episode: 1398, duration: 0.117s, episode steps: 72, steps per second: 617, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.072 [-0.599, 0.886], mean_best_reward: --
 78504/100000: episode: 1399, duration: 0.204s, episode steps: 130, steps per second: 637, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.083 [-1.122, 0.717], mean_best_reward: --
 78567/100000: episode: 1400, duration: 0.120s, episode steps: 63, steps per second: 527, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.060 [-0.754, 0.724], mean_best_reward: 172.500000
 78606/100000: episode: 1401, duration: 0.066s, episode steps: 39, steps per second: 595, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.170 [-0.543, 1.258], mean_best_reward: --
 78637/100000: episode: 1402, duration: 0.046s, episode steps: 31, steps per second: 668, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.097 [-0.430, 1.014], mean_best_reward: --
 78662/100000: episode: 1403, duration: 0.045s, episode steps: 25, steps per second: 551, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.123 [-0.594, 0.970], mean_best_reward: --
 78722/100000: episode: 1404, duration: 0.096s, episode steps: 60, steps per second: 625, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.058 [-0.701, 0.927], mean_best_reward: --
 78795/100000: episode: 1405, duration: 0.124s, episode steps: 73, steps per second: 588, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.192 [-0.658, 1.635], mean_best_reward: --
 78901/100000: episode: 1406, duration: 0.191s, episode steps: 106, steps per second: 555, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.148 [-0.892, 1.637], mean_best_reward: --
 78974/100000: episode: 1407, duration: 0.130s, episode steps: 73, steps per second: 560, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.296 [-1.875, 0.596], mean_best_reward: --
 79041/100000: episode: 1408, duration: 0.118s, episode steps: 67, steps per second: 567, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.055 [-0.596, 0.989], mean_best_reward: --
 79128/100000: episode: 1409, duration: 0.140s, episode steps: 87, steps per second: 621, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.176 [-1.234, 0.755], mean_best_reward: --
 79221/100000: episode: 1410, duration: 0.144s, episode steps: 93, steps per second: 644, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.229 [-1.305, 0.661], mean_best_reward: --
 79312/100000: episode: 1411, duration: 0.142s, episode steps: 91, steps per second: 642, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.224 [-1.308, 0.617], mean_best_reward: --
 79360/100000: episode: 1412, duration: 0.074s, episode steps: 48, steps per second: 649, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.105 [-0.912, 0.519], mean_best_reward: --
 79403/100000: episode: 1413, duration: 0.066s, episode steps: 43, steps per second: 653, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.114 [-0.636, 0.876], mean_best_reward: --
 79440/100000: episode: 1414, duration: 0.056s, episode steps: 37, steps per second: 662, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.116 [-0.602, 0.997], mean_best_reward: --
 79482/100000: episode: 1415, duration: 0.067s, episode steps: 42, steps per second: 628, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.128 [-0.546, 1.316], mean_best_reward: --
 79515/100000: episode: 1416, duration: 0.052s, episode steps: 33, steps per second: 639, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.099 [-0.588, 1.044], mean_best_reward: --
 79704/100000: episode: 1417, duration: 0.322s, episode steps: 189, steps per second: 587, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.012 [-0.890, 0.912], mean_best_reward: --
 79728/100000: episode: 1418, duration: 0.042s, episode steps: 24, steps per second: 574, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.567, 1.154], mean_best_reward: --
 79759/100000: episode: 1419, duration: 0.048s, episode steps: 31, steps per second: 639, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.049 [-0.813, 1.386], mean_best_reward: --
 79772/100000: episode: 1420, duration: 0.024s, episode steps: 13, steps per second: 542, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.127 [-0.786, 1.236], mean_best_reward: --
 79853/100000: episode: 1421, duration: 0.141s, episode steps: 81, steps per second: 573, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.126 [-0.537, 1.003], mean_best_reward: --
 79929/100000: episode: 1422, duration: 0.120s, episode steps: 76, steps per second: 633, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.799, 1.061], mean_best_reward: --
 79955/100000: episode: 1423, duration: 0.043s, episode steps: 26, steps per second: 603, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.099 [-0.628, 1.051], mean_best_reward: --
 79990/100000: episode: 1424, duration: 0.056s, episode steps: 35, steps per second: 626, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.128 [-0.413, 0.857], mean_best_reward: --
 80009/100000: episode: 1425, duration: 0.037s, episode steps: 19, steps per second: 520, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.070 [-0.781, 1.297], mean_best_reward: --
 80041/100000: episode: 1426, duration: 0.058s, episode steps: 32, steps per second: 548, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.064 [-0.639, 1.160], mean_best_reward: --
 80196/100000: episode: 1427, duration: 0.260s, episode steps: 155, steps per second: 597, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.135 [-1.026, 0.737], mean_best_reward: --
 80239/100000: episode: 1428, duration: 0.070s, episode steps: 43, steps per second: 614, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.098 [-1.131, 0.535], mean_best_reward: --
 80325/100000: episode: 1429, duration: 0.146s, episode steps: 86, steps per second: 588, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.169 [-0.612, 1.136], mean_best_reward: --
 80369/100000: episode: 1430, duration: 0.077s, episode steps: 44, steps per second: 572, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.183 [-0.989, 0.871], mean_best_reward: --
 80412/100000: episode: 1431, duration: 0.077s, episode steps: 43, steps per second: 556, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.104 [-0.770, 1.084], mean_best_reward: --
 80499/100000: episode: 1432, duration: 0.157s, episode steps: 87, steps per second: 553, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.203 [-1.291, 0.560], mean_best_reward: --
 80541/100000: episode: 1433, duration: 0.075s, episode steps: 42, steps per second: 560, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.112 [-0.587, 0.886], mean_best_reward: --
 80592/100000: episode: 1434, duration: 0.084s, episode steps: 51, steps per second: 610, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.105 [-0.589, 0.984], mean_best_reward: --
 80617/100000: episode: 1435, duration: 0.045s, episode steps: 25, steps per second: 561, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.120 [-1.076, 0.515], mean_best_reward: --
 80658/100000: episode: 1436, duration: 0.075s, episode steps: 41, steps per second: 544, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.070 [-0.579, 0.882], mean_best_reward: --
 80720/100000: episode: 1437, duration: 0.108s, episode steps: 62, steps per second: 572, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.229 [-0.677, 1.280], mean_best_reward: --
 80772/100000: episode: 1438, duration: 0.083s, episode steps: 52, steps per second: 630, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.141 [-0.473, 0.922], mean_best_reward: --
 80856/100000: episode: 1439, duration: 0.129s, episode steps: 84, steps per second: 651, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.126 [-0.813, 1.297], mean_best_reward: --
 81036/100000: episode: 1440, duration: 0.292s, episode steps: 180, steps per second: 617, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.050 [-0.929, 1.001], mean_best_reward: --
 81072/100000: episode: 1441, duration: 0.061s, episode steps: 36, steps per second: 587, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.115 [-0.740, 1.106], mean_best_reward: --
 81118/100000: episode: 1442, duration: 0.078s, episode steps: 46, steps per second: 592, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.073 [-0.563, 0.893], mean_best_reward: --
 81168/100000: episode: 1443, duration: 0.083s, episode steps: 50, steps per second: 604, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.067 [-0.878, 0.697], mean_best_reward: --
 81215/100000: episode: 1444, duration: 0.072s, episode steps: 47, steps per second: 650, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.070 [-0.736, 1.119], mean_best_reward: --
 81324/100000: episode: 1445, duration: 0.169s, episode steps: 109, steps per second: 646, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.127 [-1.420, 0.870], mean_best_reward: --
 81389/100000: episode: 1446, duration: 0.099s, episode steps: 65, steps per second: 658, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.080 [-0.945, 1.108], mean_best_reward: --
 81461/100000: episode: 1447, duration: 0.108s, episode steps: 72, steps per second: 670, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.172 [-0.846, 0.541], mean_best_reward: --
 81546/100000: episode: 1448, duration: 0.136s, episode steps: 85, steps per second: 627, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.122 [-1.293, 0.980], mean_best_reward: --
 81591/100000: episode: 1449, duration: 0.072s, episode steps: 45, steps per second: 623, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.032 [-0.574, 1.191], mean_best_reward: --
 81674/100000: episode: 1450, duration: 0.128s, episode steps: 83, steps per second: 649, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.030 [-0.859, 1.021], mean_best_reward: 130.000000
 81712/100000: episode: 1451, duration: 0.058s, episode steps: 38, steps per second: 657, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.166 [-0.823, 0.542], mean_best_reward: --
 81752/100000: episode: 1452, duration: 0.062s, episode steps: 40, steps per second: 646, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.086 [-0.394, 0.963], mean_best_reward: --
 81841/100000: episode: 1453, duration: 0.139s, episode steps: 89, steps per second: 639, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.118 [-0.603, 1.073], mean_best_reward: --
 81880/100000: episode: 1454, duration: 0.060s, episode steps: 39, steps per second: 650, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.194 [-0.550, 1.145], mean_best_reward: --
 81969/100000: episode: 1455, duration: 0.147s, episode steps: 89, steps per second: 606, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.164 [-1.126, 0.809], mean_best_reward: --
 81995/100000: episode: 1456, duration: 0.044s, episode steps: 26, steps per second: 591, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.081 [-0.587, 1.200], mean_best_reward: --
 82044/100000: episode: 1457, duration: 0.077s, episode steps: 49, steps per second: 634, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.166 [-0.617, 1.283], mean_best_reward: --
 82091/100000: episode: 1458, duration: 0.078s, episode steps: 47, steps per second: 602, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.057 [-0.824, 1.353], mean_best_reward: --
 82222/100000: episode: 1459, duration: 0.206s, episode steps: 131, steps per second: 636, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.149 [-1.146, 0.999], mean_best_reward: --
 82288/100000: episode: 1460, duration: 0.112s, episode steps: 66, steps per second: 589, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.104 [-1.190, 0.678], mean_best_reward: --
 82406/100000: episode: 1461, duration: 0.177s, episode steps: 118, steps per second: 665, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.065 [-0.953, 1.307], mean_best_reward: --
 82451/100000: episode: 1462, duration: 0.074s, episode steps: 45, steps per second: 609, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.125 [-0.596, 0.921], mean_best_reward: --
 82497/100000: episode: 1463, duration: 0.081s, episode steps: 46, steps per second: 569, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.842, 1.164], mean_best_reward: --
 82555/100000: episode: 1464, duration: 0.106s, episode steps: 58, steps per second: 547, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.137 [-1.270, 0.750], mean_best_reward: --
 82603/100000: episode: 1465, duration: 0.088s, episode steps: 48, steps per second: 547, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.191 [-1.139, 0.454], mean_best_reward: --
 82723/100000: episode: 1466, duration: 0.198s, episode steps: 120, steps per second: 606, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.067 [-0.806, 1.071], mean_best_reward: --
 82756/100000: episode: 1467, duration: 0.053s, episode steps: 33, steps per second: 628, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.116 [-0.543, 0.959], mean_best_reward: --
 82813/100000: episode: 1468, duration: 0.086s, episode steps: 57, steps per second: 662, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.091 [-0.577, 0.995], mean_best_reward: --
 82930/100000: episode: 1469, duration: 0.208s, episode steps: 117, steps per second: 561, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.038 [-0.631, 0.946], mean_best_reward: --
 82992/100000: episode: 1470, duration: 0.104s, episode steps: 62, steps per second: 593, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.242 [-1.519, 0.496], mean_best_reward: --
 83037/100000: episode: 1471, duration: 0.064s, episode steps: 45, steps per second: 705, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.060 [-0.731, 1.107], mean_best_reward: --
 83165/100000: episode: 1472, duration: 0.209s, episode steps: 128, steps per second: 613, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.108 [-0.716, 0.817], mean_best_reward: --
 83238/100000: episode: 1473, duration: 0.103s, episode steps: 73, steps per second: 707, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.260 [-1.692, 0.500], mean_best_reward: --
 83255/100000: episode: 1474, duration: 0.028s, episode steps: 17, steps per second: 615, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.093 [-0.774, 1.295], mean_best_reward: --
 83326/100000: episode: 1475, duration: 0.107s, episode steps: 71, steps per second: 666, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.210 [-1.686, 1.076], mean_best_reward: --
 83364/100000: episode: 1476, duration: 0.067s, episode steps: 38, steps per second: 570, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.130 [-0.378, 1.162], mean_best_reward: --
 83409/100000: episode: 1477, duration: 0.069s, episode steps: 45, steps per second: 656, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.038 [-0.622, 1.055], mean_best_reward: --
 83487/100000: episode: 1478, duration: 0.120s, episode steps: 78, steps per second: 649, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.070 [-0.752, 1.167], mean_best_reward: --
 83515/100000: episode: 1479, duration: 0.043s, episode steps: 28, steps per second: 655, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.554, 1.151], mean_best_reward: --
 83621/100000: episode: 1480, duration: 0.170s, episode steps: 106, steps per second: 623, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.006 [-0.742, 1.250], mean_best_reward: --
 83717/100000: episode: 1481, duration: 0.158s, episode steps: 96, steps per second: 606, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.118 [-1.013, 1.082], mean_best_reward: --
 83737/100000: episode: 1482, duration: 0.035s, episode steps: 20, steps per second: 567, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.809, 1.264], mean_best_reward: --
 83781/100000: episode: 1483, duration: 0.071s, episode steps: 44, steps per second: 618, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.192 [-0.604, 1.294], mean_best_reward: --
 83869/100000: episode: 1484, duration: 0.147s, episode steps: 88, steps per second: 597, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.073 [-1.499, 0.875], mean_best_reward: --
 83982/100000: episode: 1485, duration: 0.188s, episode steps: 113, steps per second: 602, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.068 [-1.119, 0.822], mean_best_reward: --
 84030/100000: episode: 1486, duration: 0.081s, episode steps: 48, steps per second: 590, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.111 [-0.609, 1.033], mean_best_reward: --
 84098/100000: episode: 1487, duration: 0.118s, episode steps: 68, steps per second: 575, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.171 [-0.582, 0.904], mean_best_reward: --
 84155/100000: episode: 1488, duration: 0.114s, episode steps: 57, steps per second: 502, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.132 [-1.461, 0.659], mean_best_reward: --
 84191/100000: episode: 1489, duration: 0.058s, episode steps: 36, steps per second: 618, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.102 [-0.570, 0.883], mean_best_reward: --
 84395/100000: episode: 1490, duration: 0.345s, episode steps: 204, steps per second: 592, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.077 [-0.990, 0.890], mean_best_reward: --
 84473/100000: episode: 1491, duration: 0.139s, episode steps: 78, steps per second: 560, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.151 [-1.105, 0.589], mean_best_reward: --
 84521/100000: episode: 1492, duration: 0.085s, episode steps: 48, steps per second: 567, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.101 [-0.534, 0.923], mean_best_reward: --
 84560/100000: episode: 1493, duration: 0.067s, episode steps: 39, steps per second: 583, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.100 [-1.060, 0.491], mean_best_reward: --
 84608/100000: episode: 1494, duration: 0.082s, episode steps: 48, steps per second: 587, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.902, 1.209], mean_best_reward: --
 84649/100000: episode: 1495, duration: 0.067s, episode steps: 41, steps per second: 615, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.122 [-0.738, 1.291], mean_best_reward: --
 84757/100000: episode: 1496, duration: 0.167s, episode steps: 108, steps per second: 648, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.209 [-1.454, 0.880], mean_best_reward: --
 84820/100000: episode: 1497, duration: 0.109s, episode steps: 63, steps per second: 578, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.168 [-0.590, 1.268], mean_best_reward: --
 84853/100000: episode: 1498, duration: 0.053s, episode steps: 33, steps per second: 620, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.086 [-0.589, 0.955], mean_best_reward: --
 84972/100000: episode: 1499, duration: 0.198s, episode steps: 119, steps per second: 601, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.154 [-1.601, 0.729], mean_best_reward: --
 85029/100000: episode: 1500, duration: 0.092s, episode steps: 57, steps per second: 618, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.191 [-0.704, 1.321], mean_best_reward: 135.500000
 85061/100000: episode: 1501, duration: 0.058s, episode steps: 32, steps per second: 554, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.109 [-0.603, 1.159], mean_best_reward: --
 85133/100000: episode: 1502, duration: 0.123s, episode steps: 72, steps per second: 583, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.078 [-0.950, 0.870], mean_best_reward: --
 85207/100000: episode: 1503, duration: 0.112s, episode steps: 74, steps per second: 659, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.087 [-0.651, 1.074], mean_best_reward: --
 85293/100000: episode: 1504, duration: 0.133s, episode steps: 86, steps per second: 646, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.153 [-0.864, 1.315], mean_best_reward: --
 85340/100000: episode: 1505, duration: 0.066s, episode steps: 47, steps per second: 713, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.047 [-0.622, 0.999], mean_best_reward: --
 85400/100000: episode: 1506, duration: 0.089s, episode steps: 60, steps per second: 677, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.115 [-0.603, 1.099], mean_best_reward: --
 85474/100000: episode: 1507, duration: 0.122s, episode steps: 74, steps per second: 604, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.035 [-0.502, 1.107], mean_best_reward: --
 85506/100000: episode: 1508, duration: 0.045s, episode steps: 32, steps per second: 711, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.093 [-0.755, 1.131], mean_best_reward: --
 85544/100000: episode: 1509, duration: 0.066s, episode steps: 38, steps per second: 579, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.062 [-0.612, 1.022], mean_best_reward: --
 85631/100000: episode: 1510, duration: 0.124s, episode steps: 87, steps per second: 700, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.122 [-0.907, 1.093], mean_best_reward: --
 85703/100000: episode: 1511, duration: 0.105s, episode steps: 72, steps per second: 689, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.581, 1.117], mean_best_reward: --
 85736/100000: episode: 1512, duration: 0.049s, episode steps: 33, steps per second: 673, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.069 [-1.611, 0.812], mean_best_reward: --
 85792/100000: episode: 1513, duration: 0.085s, episode steps: 56, steps per second: 660, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.185 [-0.874, 1.170], mean_best_reward: --
 85864/100000: episode: 1514, duration: 0.100s, episode steps: 72, steps per second: 719, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.092 [-0.734, 1.022], mean_best_reward: --
 85912/100000: episode: 1515, duration: 0.083s, episode steps: 48, steps per second: 581, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.107 [-1.084, 0.540], mean_best_reward: --
 85956/100000: episode: 1516, duration: 0.083s, episode steps: 44, steps per second: 529, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.114 [-0.448, 0.976], mean_best_reward: --
 86046/100000: episode: 1517, duration: 0.154s, episode steps: 90, steps per second: 583, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.031 [-0.900, 0.984], mean_best_reward: --
 86094/100000: episode: 1518, duration: 0.086s, episode steps: 48, steps per second: 557, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.571, 1.147], mean_best_reward: --
 86139/100000: episode: 1519, duration: 0.081s, episode steps: 45, steps per second: 555, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.126 [-0.563, 0.792], mean_best_reward: --
 86171/100000: episode: 1520, duration: 0.055s, episode steps: 32, steps per second: 586, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.387, 0.996], mean_best_reward: --
 86248/100000: episode: 1521, duration: 0.138s, episode steps: 77, steps per second: 560, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.170 [-0.823, 1.242], mean_best_reward: --
 86320/100000: episode: 1522, duration: 0.125s, episode steps: 72, steps per second: 576, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.062 [-0.633, 0.999], mean_best_reward: --
 86385/100000: episode: 1523, duration: 0.095s, episode steps: 65, steps per second: 682, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.146 [-0.452, 1.243], mean_best_reward: --
 86451/100000: episode: 1524, duration: 0.114s, episode steps: 66, steps per second: 577, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.132 [-0.571, 0.811], mean_best_reward: --
 86466/100000: episode: 1525, duration: 0.026s, episode steps: 15, steps per second: 576, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.108 [-0.772, 1.293], mean_best_reward: --
 86581/100000: episode: 1526, duration: 0.180s, episode steps: 115, steps per second: 637, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.033 [-0.711, 0.978], mean_best_reward: --
 86611/100000: episode: 1527, duration: 0.049s, episode steps: 30, steps per second: 617, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.783, 1.043], mean_best_reward: --
 86645/100000: episode: 1528, duration: 0.049s, episode steps: 34, steps per second: 696, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.101 [-0.380, 0.746], mean_best_reward: --
 86724/100000: episode: 1529, duration: 0.135s, episode steps: 79, steps per second: 584, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.013 [-0.698, 1.085], mean_best_reward: --
 86800/100000: episode: 1530, duration: 0.116s, episode steps: 76, steps per second: 657, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.016 [-0.722, 0.893], mean_best_reward: --
 86818/100000: episode: 1531, duration: 0.028s, episode steps: 18, steps per second: 644, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.047, 0.544], mean_best_reward: --
 86874/100000: episode: 1532, duration: 0.083s, episode steps: 56, steps per second: 673, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.140 [-0.990, 0.610], mean_best_reward: --
 86939/100000: episode: 1533, duration: 0.098s, episode steps: 65, steps per second: 663, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.110 [-0.732, 1.151], mean_best_reward: --
 86967/100000: episode: 1534, duration: 0.044s, episode steps: 28, steps per second: 639, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.635, 1.004], mean_best_reward: --
 87028/100000: episode: 1535, duration: 0.108s, episode steps: 61, steps per second: 564, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.177 [-0.606, 1.293], mean_best_reward: --
 87075/100000: episode: 1536, duration: 0.082s, episode steps: 47, steps per second: 575, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.125 [-0.732, 1.107], mean_best_reward: --
 87234/100000: episode: 1537, duration: 0.267s, episode steps: 159, steps per second: 595, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.110 [-1.096, 1.115], mean_best_reward: --
 87280/100000: episode: 1538, duration: 0.084s, episode steps: 46, steps per second: 548, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.056 [-0.834, 1.055], mean_best_reward: --
 87339/100000: episode: 1539, duration: 0.101s, episode steps: 59, steps per second: 583, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.020 [-0.769, 1.021], mean_best_reward: --
 87412/100000: episode: 1540, duration: 0.131s, episode steps: 73, steps per second: 557, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.245 [-1.300, 0.634], mean_best_reward: --
 87456/100000: episode: 1541, duration: 0.074s, episode steps: 44, steps per second: 592, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.140 [-1.297, 0.573], mean_best_reward: --
 87474/100000: episode: 1542, duration: 0.030s, episode steps: 18, steps per second: 604, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.064 [-0.623, 0.902], mean_best_reward: --
 87539/100000: episode: 1543, duration: 0.115s, episode steps: 65, steps per second: 566, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.078 [-0.559, 0.931], mean_best_reward: --
 87567/100000: episode: 1544, duration: 0.048s, episode steps: 28, steps per second: 580, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.102 [-0.397, 0.790], mean_best_reward: --
 87654/100000: episode: 1545, duration: 0.152s, episode steps: 87, steps per second: 571, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.076 [-1.328, 0.784], mean_best_reward: --
 87724/100000: episode: 1546, duration: 0.113s, episode steps: 70, steps per second: 622, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.093 [-0.793, 1.047], mean_best_reward: --
 87872/100000: episode: 1547, duration: 0.238s, episode steps: 148, steps per second: 623, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.169 [-1.516, 0.809], mean_best_reward: --
 87984/100000: episode: 1548, duration: 0.166s, episode steps: 112, steps per second: 676, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.187 [-1.200, 0.783], mean_best_reward: --
 88034/100000: episode: 1549, duration: 0.077s, episode steps: 50, steps per second: 648, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.205 [-0.819, 1.476], mean_best_reward: --
 88104/100000: episode: 1550, duration: 0.104s, episode steps: 70, steps per second: 670, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.154 [-0.734, 1.466], mean_best_reward: 123.500000
 88136/100000: episode: 1551, duration: 0.047s, episode steps: 32, steps per second: 679, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.105 [-0.992, 0.399], mean_best_reward: --
 88265/100000: episode: 1552, duration: 0.205s, episode steps: 129, steps per second: 629, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.084 [-0.800, 1.477], mean_best_reward: --
 88288/100000: episode: 1553, duration: 0.042s, episode steps: 23, steps per second: 549, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.122 [-1.132, 0.546], mean_best_reward: --
 88392/100000: episode: 1554, duration: 0.172s, episode steps: 104, steps per second: 605, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.074 [-1.318, 0.974], mean_best_reward: --
 88537/100000: episode: 1555, duration: 0.215s, episode steps: 145, steps per second: 675, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.132 [-0.687, 1.132], mean_best_reward: --
 88599/100000: episode: 1556, duration: 0.104s, episode steps: 62, steps per second: 593, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-0.920, 0.762], mean_best_reward: --
 88688/100000: episode: 1557, duration: 0.146s, episode steps: 89, steps per second: 610, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.244 [-0.591, 1.699], mean_best_reward: --
 88785/100000: episode: 1558, duration: 0.158s, episode steps: 97, steps per second: 614, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.212 [-1.499, 0.758], mean_best_reward: --
 88868/100000: episode: 1559, duration: 0.133s, episode steps: 83, steps per second: 624, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.045 [-0.865, 0.944], mean_best_reward: --
 88907/100000: episode: 1560, duration: 0.069s, episode steps: 39, steps per second: 567, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.095 [-0.546, 1.054], mean_best_reward: --
 88982/100000: episode: 1561, duration: 0.120s, episode steps: 75, steps per second: 623, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.026 [-1.401, 0.794], mean_best_reward: --
 89029/100000: episode: 1562, duration: 0.080s, episode steps: 47, steps per second: 587, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.199 [-0.682, 1.097], mean_best_reward: --
 89093/100000: episode: 1563, duration: 0.114s, episode steps: 64, steps per second: 563, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.126 [-0.733, 1.227], mean_best_reward: --
 89129/100000: episode: 1564, duration: 0.069s, episode steps: 36, steps per second: 525, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.762, 1.283], mean_best_reward: --
 89178/100000: episode: 1565, duration: 0.080s, episode steps: 49, steps per second: 615, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.141 [-0.578, 0.934], mean_best_reward: --
 89333/100000: episode: 1566, duration: 0.252s, episode steps: 155, steps per second: 615, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.065 [-0.978, 0.998], mean_best_reward: --
 89393/100000: episode: 1567, duration: 0.101s, episode steps: 60, steps per second: 594, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.171 [-1.107, 0.629], mean_best_reward: --
 89454/100000: episode: 1568, duration: 0.099s, episode steps: 61, steps per second: 618, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.037 [-0.864, 1.048], mean_best_reward: --
 89507/100000: episode: 1569, duration: 0.098s, episode steps: 53, steps per second: 543, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.126 [-0.505, 1.079], mean_best_reward: --
 89534/100000: episode: 1570, duration: 0.050s, episode steps: 27, steps per second: 543, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.068 [-0.809, 1.120], mean_best_reward: --
 89622/100000: episode: 1571, duration: 0.162s, episode steps: 88, steps per second: 544, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.159 [-0.886, 0.645], mean_best_reward: --
 89679/100000: episode: 1572, duration: 0.091s, episode steps: 57, steps per second: 623, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.105 [-0.618, 0.951], mean_best_reward: --
 89712/100000: episode: 1573, duration: 0.057s, episode steps: 33, steps per second: 581, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.139 [-0.534, 0.895], mean_best_reward: --
 89820/100000: episode: 1574, duration: 0.178s, episode steps: 108, steps per second: 608, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.179 [-0.948, 1.112], mean_best_reward: --
 89916/100000: episode: 1575, duration: 0.151s, episode steps: 96, steps per second: 635, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.172 [-1.655, 0.760], mean_best_reward: --
 89947/100000: episode: 1576, duration: 0.047s, episode steps: 31, steps per second: 660, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.079 [-0.982, 0.586], mean_best_reward: --
 90057/100000: episode: 1577, duration: 0.168s, episode steps: 110, steps per second: 654, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.204 [-1.181, 1.015], mean_best_reward: --
 90106/100000: episode: 1578, duration: 0.085s, episode steps: 49, steps per second: 578, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.079 [-0.765, 1.452], mean_best_reward: --
 90144/100000: episode: 1579, duration: 0.063s, episode steps: 38, steps per second: 599, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.098 [-0.444, 0.830], mean_best_reward: --
 90207/100000: episode: 1580, duration: 0.106s, episode steps: 63, steps per second: 595, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.199 [-1.299, 0.628], mean_best_reward: --
 90298/100000: episode: 1581, duration: 0.146s, episode steps: 91, steps per second: 621, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.176 [-0.976, 0.615], mean_best_reward: --
 90367/100000: episode: 1582, duration: 0.106s, episode steps: 69, steps per second: 653, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.183 [-1.275, 0.747], mean_best_reward: --
 90412/100000: episode: 1583, duration: 0.070s, episode steps: 45, steps per second: 643, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.164 [-1.099, 0.742], mean_best_reward: --
 90505/100000: episode: 1584, duration: 0.141s, episode steps: 93, steps per second: 659, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.108 [-1.317, 0.838], mean_best_reward: --
 90573/100000: episode: 1585, duration: 0.103s, episode steps: 68, steps per second: 663, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.650, 1.331], mean_best_reward: --
 90639/100000: episode: 1586, duration: 0.107s, episode steps: 66, steps per second: 619, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.148 [-0.511, 1.253], mean_best_reward: --
 90691/100000: episode: 1587, duration: 0.078s, episode steps: 52, steps per second: 665, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.111 [-0.481, 0.974], mean_best_reward: --
 90718/100000: episode: 1588, duration: 0.042s, episode steps: 27, steps per second: 650, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.108 [-0.371, 1.103], mean_best_reward: --
 90841/100000: episode: 1589, duration: 0.185s, episode steps: 123, steps per second: 664, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.022 [-1.161, 1.277], mean_best_reward: --
 90893/100000: episode: 1590, duration: 0.081s, episode steps: 52, steps per second: 640, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.157 [-1.540, 0.843], mean_best_reward: --
 90986/100000: episode: 1591, duration: 0.169s, episode steps: 93, steps per second: 551, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.115 [-1.199, 0.739], mean_best_reward: --
 91056/100000: episode: 1592, duration: 0.125s, episode steps: 70, steps per second: 561, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.160 [-0.925, 0.774], mean_best_reward: --
 91170/100000: episode: 1593, duration: 0.178s, episode steps: 114, steps per second: 642, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.003 [-0.742, 0.989], mean_best_reward: --
 91225/100000: episode: 1594, duration: 0.083s, episode steps: 55, steps per second: 661, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.105 [-0.777, 1.107], mean_best_reward: --
 91304/100000: episode: 1595, duration: 0.137s, episode steps: 79, steps per second: 577, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.169 [-0.580, 0.938], mean_best_reward: --
 91360/100000: episode: 1596, duration: 0.103s, episode steps: 56, steps per second: 544, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.151 [-0.458, 1.076], mean_best_reward: --
 91494/100000: episode: 1597, duration: 0.202s, episode steps: 134, steps per second: 663, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.025 [-0.845, 0.903], mean_best_reward: --
 91518/100000: episode: 1598, duration: 0.040s, episode steps: 24, steps per second: 606, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.114 [-0.398, 1.024], mean_best_reward: --
 91550/100000: episode: 1599, duration: 0.049s, episode steps: 32, steps per second: 653, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.395, 1.000], mean_best_reward: --
 91573/100000: episode: 1600, duration: 0.039s, episode steps: 23, steps per second: 594, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.078 [-0.789, 1.170], mean_best_reward: 167.500000
 91648/100000: episode: 1601, duration: 0.120s, episode steps: 75, steps per second: 623, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.218 [-0.551, 1.147], mean_best_reward: --
 91707/100000: episode: 1602, duration: 0.098s, episode steps: 59, steps per second: 601, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.076 [-1.041, 0.610], mean_best_reward: --
 91772/100000: episode: 1603, duration: 0.101s, episode steps: 65, steps per second: 647, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.108 [-0.686, 1.067], mean_best_reward: --
 91802/100000: episode: 1604, duration: 0.047s, episode steps: 30, steps per second: 636, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.093 [-0.566, 1.155], mean_best_reward: --
 91929/100000: episode: 1605, duration: 0.214s, episode steps: 127, steps per second: 595, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.111 [-1.203, 1.502], mean_best_reward: --
 92004/100000: episode: 1606, duration: 0.119s, episode steps: 75, steps per second: 630, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.237 [-1.860, 0.677], mean_best_reward: --
 92061/100000: episode: 1607, duration: 0.108s, episode steps: 57, steps per second: 525, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.174 [-0.755, 0.917], mean_best_reward: --
 92090/100000: episode: 1608, duration: 0.052s, episode steps: 29, steps per second: 555, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.142 [-0.611, 0.942], mean_best_reward: --
 92156/100000: episode: 1609, duration: 0.111s, episode steps: 66, steps per second: 593, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.611, 1.312], mean_best_reward: --
 92218/100000: episode: 1610, duration: 0.117s, episode steps: 62, steps per second: 532, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.163 [-1.290, 0.755], mean_best_reward: --
 92319/100000: episode: 1611, duration: 0.177s, episode steps: 101, steps per second: 571, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.094 [-1.467, 0.963], mean_best_reward: --
 92410/100000: episode: 1612, duration: 0.145s, episode steps: 91, steps per second: 626, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.033 [-1.301, 1.450], mean_best_reward: --
 92452/100000: episode: 1613, duration: 0.077s, episode steps: 42, steps per second: 546, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.190 [-0.550, 1.137], mean_best_reward: --
 92475/100000: episode: 1614, duration: 0.044s, episode steps: 23, steps per second: 528, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.091 [-0.624, 1.213], mean_best_reward: --
 92590/100000: episode: 1615, duration: 0.194s, episode steps: 115, steps per second: 591, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.143 [-1.109, 1.690], mean_best_reward: --
 92651/100000: episode: 1616, duration: 0.102s, episode steps: 61, steps per second: 598, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.012 [-0.613, 1.026], mean_best_reward: --
 92689/100000: episode: 1617, duration: 0.068s, episode steps: 38, steps per second: 561, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.469, 1.081], mean_best_reward: --
 92760/100000: episode: 1618, duration: 0.119s, episode steps: 71, steps per second: 595, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.188 [-0.497, 1.357], mean_best_reward: --
 92854/100000: episode: 1619, duration: 0.162s, episode steps: 94, steps per second: 580, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.061 [-0.817, 1.068], mean_best_reward: --
 92905/100000: episode: 1620, duration: 0.081s, episode steps: 51, steps per second: 627, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.209 [-0.975, 0.442], mean_best_reward: --
 92942/100000: episode: 1621, duration: 0.061s, episode steps: 37, steps per second: 611, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.060 [-0.621, 1.231], mean_best_reward: --
 93046/100000: episode: 1622, duration: 0.178s, episode steps: 104, steps per second: 584, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.132 [-0.759, 1.032], mean_best_reward: --
 93129/100000: episode: 1623, duration: 0.150s, episode steps: 83, steps per second: 552, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.311 [-1.355, 0.675], mean_best_reward: --
 93186/100000: episode: 1624, duration: 0.097s, episode steps: 57, steps per second: 588, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.164 [-0.555, 0.956], mean_best_reward: --
 93219/100000: episode: 1625, duration: 0.059s, episode steps: 33, steps per second: 556, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.144 [-0.394, 1.008], mean_best_reward: --
 93286/100000: episode: 1626, duration: 0.117s, episode steps: 67, steps per second: 572, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.223 [-1.456, 0.522], mean_best_reward: --
 93312/100000: episode: 1627, duration: 0.048s, episode steps: 26, steps per second: 538, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.570, 1.204], mean_best_reward: --
 93442/100000: episode: 1628, duration: 0.206s, episode steps: 130, steps per second: 632, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.107 [-1.375, 1.136], mean_best_reward: --
 93485/100000: episode: 1629, duration: 0.068s, episode steps: 43, steps per second: 630, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.086 [-0.543, 1.256], mean_best_reward: --
 93560/100000: episode: 1630, duration: 0.126s, episode steps: 75, steps per second: 595, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.138 [-0.474, 1.105], mean_best_reward: --
 93610/100000: episode: 1631, duration: 0.092s, episode steps: 50, steps per second: 546, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.160 [-0.952, 0.382], mean_best_reward: --
 93684/100000: episode: 1632, duration: 0.114s, episode steps: 74, steps per second: 648, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.210 [-0.849, 0.614], mean_best_reward: --
 93721/100000: episode: 1633, duration: 0.058s, episode steps: 37, steps per second: 634, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.077 [-0.933, 0.735], mean_best_reward: --
 93742/100000: episode: 1634, duration: 0.035s, episode steps: 21, steps per second: 604, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.124 [-0.594, 1.034], mean_best_reward: --
 93757/100000: episode: 1635, duration: 0.030s, episode steps: 15, steps per second: 498, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.096 [-1.257, 0.757], mean_best_reward: --
 93785/100000: episode: 1636, duration: 0.051s, episode steps: 28, steps per second: 545, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.096 [-0.625, 0.928], mean_best_reward: --
 93810/100000: episode: 1637, duration: 0.046s, episode steps: 25, steps per second: 541, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.088 [-0.628, 1.059], mean_best_reward: --
 93844/100000: episode: 1638, duration: 0.053s, episode steps: 34, steps per second: 638, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.134 [-0.563, 1.044], mean_best_reward: --
 93898/100000: episode: 1639, duration: 0.086s, episode steps: 54, steps per second: 625, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.971, 1.077], mean_best_reward: --
 93945/100000: episode: 1640, duration: 0.085s, episode steps: 47, steps per second: 554, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.116 [-0.901, 0.478], mean_best_reward: --
 93989/100000: episode: 1641, duration: 0.074s, episode steps: 44, steps per second: 595, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.869, 1.087], mean_best_reward: --
 94111/100000: episode: 1642, duration: 0.199s, episode steps: 122, steps per second: 612, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.102 [-0.918, 1.152], mean_best_reward: --
 94159/100000: episode: 1643, duration: 0.073s, episode steps: 48, steps per second: 657, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.591, 1.082], mean_best_reward: --
 94180/100000: episode: 1644, duration: 0.036s, episode steps: 21, steps per second: 577, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.070 [-0.762, 1.195], mean_best_reward: --
 94223/100000: episode: 1645, duration: 0.066s, episode steps: 43, steps per second: 649, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.105 [-0.515, 0.978], mean_best_reward: --
 94288/100000: episode: 1646, duration: 0.106s, episode steps: 65, steps per second: 615, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.197 [-1.129, 0.950], mean_best_reward: --
 94316/100000: episode: 1647, duration: 0.045s, episode steps: 28, steps per second: 616, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.583, 1.284], mean_best_reward: --
 94359/100000: episode: 1648, duration: 0.071s, episode steps: 43, steps per second: 610, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.136 [-0.956, 0.565], mean_best_reward: --
 94415/100000: episode: 1649, duration: 0.089s, episode steps: 56, steps per second: 629, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.171 [-1.025, 1.454], mean_best_reward: --
 94472/100000: episode: 1650, duration: 0.084s, episode steps: 57, steps per second: 676, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.169 [-0.965, 0.466], mean_best_reward: 164.500000
 94565/100000: episode: 1651, duration: 0.137s, episode steps: 93, steps per second: 677, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.202 [-0.815, 1.498], mean_best_reward: --
 94588/100000: episode: 1652, duration: 0.035s, episode steps: 23, steps per second: 650, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.127 [-0.364, 0.947], mean_best_reward: --
 94632/100000: episode: 1653, duration: 0.079s, episode steps: 44, steps per second: 557, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.102 [-1.044, 0.463], mean_best_reward: --
 94716/100000: episode: 1654, duration: 0.128s, episode steps: 84, steps per second: 656, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.103 [-0.818, 1.145], mean_best_reward: --
 94808/100000: episode: 1655, duration: 0.138s, episode steps: 92, steps per second: 665, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.086 [-0.993, 1.609], mean_best_reward: --
 94854/100000: episode: 1656, duration: 0.071s, episode steps: 46, steps per second: 646, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.157 [-0.505, 1.357], mean_best_reward: --
 94912/100000: episode: 1657, duration: 0.097s, episode steps: 58, steps per second: 601, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.199 [-0.619, 1.170], mean_best_reward: --
 94954/100000: episode: 1658, duration: 0.072s, episode steps: 42, steps per second: 580, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.125 [-0.451, 0.973], mean_best_reward: --
 94990/100000: episode: 1659, duration: 0.063s, episode steps: 36, steps per second: 567, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.154 [-1.075, 0.568], mean_best_reward: --
 95042/100000: episode: 1660, duration: 0.084s, episode steps: 52, steps per second: 617, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.088 [-0.997, 0.687], mean_best_reward: --
 95084/100000: episode: 1661, duration: 0.067s, episode steps: 42, steps per second: 626, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.012 [-0.811, 1.177], mean_best_reward: --
 95194/100000: episode: 1662, duration: 0.176s, episode steps: 110, steps per second: 624, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.127 [-0.733, 1.560], mean_best_reward: --
 95250/100000: episode: 1663, duration: 0.089s, episode steps: 56, steps per second: 628, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.085 [-0.521, 0.994], mean_best_reward: --
 95321/100000: episode: 1664, duration: 0.110s, episode steps: 71, steps per second: 644, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.102 [-1.083, 0.614], mean_best_reward: --
 95376/100000: episode: 1665, duration: 0.090s, episode steps: 55, steps per second: 612, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.193 [-0.622, 1.081], mean_best_reward: --
 95414/100000: episode: 1666, duration: 0.060s, episode steps: 38, steps per second: 637, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.138 [-0.412, 0.719], mean_best_reward: --
 95543/100000: episode: 1667, duration: 0.223s, episode steps: 129, steps per second: 578, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.133 [-1.469, 1.034], mean_best_reward: --
 95629/100000: episode: 1668, duration: 0.156s, episode steps: 86, steps per second: 552, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.022 [-0.892, 1.128], mean_best_reward: --
 95686/100000: episode: 1669, duration: 0.103s, episode steps: 57, steps per second: 553, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.192 [-0.621, 1.454], mean_best_reward: --
 95760/100000: episode: 1670, duration: 0.133s, episode steps: 74, steps per second: 556, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.206 [-1.034, 1.297], mean_best_reward: --
 95795/100000: episode: 1671, duration: 0.056s, episode steps: 35, steps per second: 622, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.077 [-0.595, 0.823], mean_best_reward: --
 95836/100000: episode: 1672, duration: 0.067s, episode steps: 41, steps per second: 614, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.093 [-1.006, 1.301], mean_best_reward: --
 95873/100000: episode: 1673, duration: 0.061s, episode steps: 37, steps per second: 607, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.102 [-0.449, 1.000], mean_best_reward: --
 95940/100000: episode: 1674, duration: 0.109s, episode steps: 67, steps per second: 612, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.019 [-0.817, 1.411], mean_best_reward: --
 96023/100000: episode: 1675, duration: 0.144s, episode steps: 83, steps per second: 575, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.286 [-1.526, 0.611], mean_best_reward: --
 96077/100000: episode: 1676, duration: 0.095s, episode steps: 54, steps per second: 569, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.105 [-0.487, 1.019], mean_best_reward: --
 96134/100000: episode: 1677, duration: 0.087s, episode steps: 57, steps per second: 657, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.064 [-0.541, 0.932], mean_best_reward: --
 96195/100000: episode: 1678, duration: 0.101s, episode steps: 61, steps per second: 604, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.136 [-0.355, 0.952], mean_best_reward: --
 96245/100000: episode: 1679, duration: 0.087s, episode steps: 50, steps per second: 574, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.167 [-1.112, 0.949], mean_best_reward: --
 96274/100000: episode: 1680, duration: 0.047s, episode steps: 29, steps per second: 623, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.088 [-0.938, 0.591], mean_best_reward: --
 96329/100000: episode: 1681, duration: 0.092s, episode steps: 55, steps per second: 600, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.036 [-0.748, 1.002], mean_best_reward: --
 96371/100000: episode: 1682, duration: 0.074s, episode steps: 42, steps per second: 570, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.076 [-0.498, 0.954], mean_best_reward: --
 96493/100000: episode: 1683, duration: 0.185s, episode steps: 122, steps per second: 660, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.124 [-1.059, 0.758], mean_best_reward: --
 96521/100000: episode: 1684, duration: 0.048s, episode steps: 28, steps per second: 580, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.148 [-0.362, 0.778], mean_best_reward: --
 96538/100000: episode: 1685, duration: 0.026s, episode steps: 17, steps per second: 653, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.088 [-0.592, 0.973], mean_best_reward: --
 96578/100000: episode: 1686, duration: 0.061s, episode steps: 40, steps per second: 654, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.747, 1.098], mean_best_reward: --
 96621/100000: episode: 1687, duration: 0.075s, episode steps: 43, steps per second: 571, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.123 [-0.655, 1.309], mean_best_reward: --
 96666/100000: episode: 1688, duration: 0.074s, episode steps: 45, steps per second: 609, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.031 [-0.623, 1.223], mean_best_reward: --
 96727/100000: episode: 1689, duration: 0.095s, episode steps: 61, steps per second: 640, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.144 [-0.774, 0.821], mean_best_reward: --
 96769/100000: episode: 1690, duration: 0.074s, episode steps: 42, steps per second: 565, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.161 [-0.571, 0.930], mean_best_reward: --
 96801/100000: episode: 1691, duration: 0.054s, episode steps: 32, steps per second: 590, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.135 [-0.756, 1.263], mean_best_reward: --
 96922/100000: episode: 1692, duration: 0.196s, episode steps: 121, steps per second: 618, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.274 [-1.678, 1.169], mean_best_reward: --
 96963/100000: episode: 1693, duration: 0.067s, episode steps: 41, steps per second: 609, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.099 [-0.384, 1.281], mean_best_reward: --
 96990/100000: episode: 1694, duration: 0.047s, episode steps: 27, steps per second: 573, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.073 [-0.636, 0.998], mean_best_reward: --
 97006/100000: episode: 1695, duration: 0.026s, episode steps: 16, steps per second: 617, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.033, 0.542], mean_best_reward: --
 97127/100000: episode: 1696, duration: 0.192s, episode steps: 121, steps per second: 629, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.132 [-0.621, 1.331], mean_best_reward: --
 97192/100000: episode: 1697, duration: 0.109s, episode steps: 65, steps per second: 595, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.225 [-1.317, 0.826], mean_best_reward: --
 97237/100000: episode: 1698, duration: 0.078s, episode steps: 45, steps per second: 580, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.002 [-0.988, 1.323], mean_best_reward: --
 97302/100000: episode: 1699, duration: 0.118s, episode steps: 65, steps per second: 549, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.148 [-0.570, 1.294], mean_best_reward: --
 97360/100000: episode: 1700, duration: 0.100s, episode steps: 58, steps per second: 582, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.170 [-0.765, 1.010], mean_best_reward: 155.500000
 97435/100000: episode: 1701, duration: 0.130s, episode steps: 75, steps per second: 578, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.025 [-0.821, 0.988], mean_best_reward: --
 97488/100000: episode: 1702, duration: 0.094s, episode steps: 53, steps per second: 561, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.182 [-0.788, 1.285], mean_best_reward: --
 97634/100000: episode: 1703, duration: 0.247s, episode steps: 146, steps per second: 592, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.116 [-1.283, 1.214], mean_best_reward: --
 97681/100000: episode: 1704, duration: 0.080s, episode steps: 47, steps per second: 586, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.188 [-1.073, 0.612], mean_best_reward: --
 97772/100000: episode: 1705, duration: 0.155s, episode steps: 91, steps per second: 589, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.196 [-0.561, 1.488], mean_best_reward: --
 97826/100000: episode: 1706, duration: 0.092s, episode steps: 54, steps per second: 590, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.253 [-1.251, 0.771], mean_best_reward: --
 97861/100000: episode: 1707, duration: 0.068s, episode steps: 35, steps per second: 518, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.127 [-0.872, 0.444], mean_best_reward: --
 97893/100000: episode: 1708, duration: 0.058s, episode steps: 32, steps per second: 551, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.128 [-0.565, 0.970], mean_best_reward: --
 97928/100000: episode: 1709, duration: 0.061s, episode steps: 35, steps per second: 578, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.011 [-0.970, 1.247], mean_best_reward: --
 97997/100000: episode: 1710, duration: 0.106s, episode steps: 69, steps per second: 650, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.103 [-1.093, 0.554], mean_best_reward: --
 98057/100000: episode: 1711, duration: 0.098s, episode steps: 60, steps per second: 612, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.173 [-0.530, 1.138], mean_best_reward: --
 98154/100000: episode: 1712, duration: 0.171s, episode steps: 97, steps per second: 567, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.125 [-0.789, 0.910], mean_best_reward: --
 98228/100000: episode: 1713, duration: 0.114s, episode steps: 74, steps per second: 647, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.150 [-1.080, 0.485], mean_best_reward: --
 98257/100000: episode: 1714, duration: 0.045s, episode steps: 29, steps per second: 650, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.060 [-0.624, 1.224], mean_best_reward: --
 98336/100000: episode: 1715, duration: 0.138s, episode steps: 79, steps per second: 572, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.153 [-1.445, 1.147], mean_best_reward: --
 98355/100000: episode: 1716, duration: 0.034s, episode steps: 19, steps per second: 560, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.065 [-0.832, 1.159], mean_best_reward: --
 98380/100000: episode: 1717, duration: 0.039s, episode steps: 25, steps per second: 648, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.053 [-0.592, 1.240], mean_best_reward: --
 98415/100000: episode: 1718, duration: 0.062s, episode steps: 35, steps per second: 565, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.101 [-0.960, 0.554], mean_best_reward: --
 98464/100000: episode: 1719, duration: 0.087s, episode steps: 49, steps per second: 563, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.112 [-0.442, 0.930], mean_best_reward: --
 98506/100000: episode: 1720, duration: 0.074s, episode steps: 42, steps per second: 567, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.131 [-0.865, 0.493], mean_best_reward: --
 98564/100000: episode: 1721, duration: 0.099s, episode steps: 58, steps per second: 587, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.137 [-1.367, 0.620], mean_best_reward: --
 98636/100000: episode: 1722, duration: 0.119s, episode steps: 72, steps per second: 605, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.166 [-0.966, 0.791], mean_best_reward: --
 98655/100000: episode: 1723, duration: 0.035s, episode steps: 19, steps per second: 542, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.075 [-0.742, 1.235], mean_best_reward: --
 98719/100000: episode: 1724, duration: 0.103s, episode steps: 64, steps per second: 619, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.159 [-0.955, 0.724], mean_best_reward: --
 98753/100000: episode: 1725, duration: 0.059s, episode steps: 34, steps per second: 579, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.145 [-0.404, 0.937], mean_best_reward: --
 98789/100000: episode: 1726, duration: 0.065s, episode steps: 36, steps per second: 557, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.012 [-0.761, 1.118], mean_best_reward: --
 98819/100000: episode: 1727, duration: 0.051s, episode steps: 30, steps per second: 588, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.087 [-0.433, 1.098], mean_best_reward: --
 98853/100000: episode: 1728, duration: 0.053s, episode steps: 34, steps per second: 647, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.763, 1.122], mean_best_reward: --
 98883/100000: episode: 1729, duration: 0.052s, episode steps: 30, steps per second: 577, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: 0.124 [-0.422, 0.799], mean_best_reward: --
 98978/100000: episode: 1730, duration: 0.168s, episode steps: 95, steps per second: 567, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.154 [-0.772, 1.106], mean_best_reward: --
 99186/100000: episode: 1731, duration: 0.358s, episode steps: 208, steps per second: 581, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.011 [-1.137, 1.487], mean_best_reward: --
 99214/100000: episode: 1732, duration: 0.055s, episode steps: 28, steps per second: 506, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.434, 1.131], mean_best_reward: --
 99253/100000: episode: 1733, duration: 0.065s, episode steps: 39, steps per second: 597, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.126 [-0.524, 0.842], mean_best_reward: --
 99296/100000: episode: 1734, duration: 0.073s, episode steps: 43, steps per second: 589, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.175 [-0.682, 1.008], mean_best_reward: --
 99332/100000: episode: 1735, duration: 0.059s, episode steps: 36, steps per second: 606, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.125 [-0.466, 0.943], mean_best_reward: --
 99349/100000: episode: 1736, duration: 0.029s, episode steps: 17, steps per second: 592, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.092 [-0.638, 1.087], mean_best_reward: --
 99487/100000: episode: 1737, duration: 0.209s, episode steps: 138, steps per second: 660, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.015 [-1.133, 0.951], mean_best_reward: --
 99547/100000: episode: 1738, duration: 0.091s, episode steps: 60, steps per second: 660, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.186 [-0.934, 1.155], mean_best_reward: --
 99587/100000: episode: 1739, duration: 0.062s, episode steps: 40, steps per second: 648, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.120 [-0.637, 1.155], mean_best_reward: --
 99636/100000: episode: 1740, duration: 0.075s, episode steps: 49, steps per second: 651, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.124 [-0.384, 0.800], mean_best_reward: --
 99685/100000: episode: 1741, duration: 0.081s, episode steps: 49, steps per second: 604, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.182 [-0.536, 1.129], mean_best_reward: --
 99782/100000: episode: 1742, duration: 0.167s, episode steps: 97, steps per second: 580, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.097 [-0.681, 1.262], mean_best_reward: --
 99877/100000: episode: 1743, duration: 0.156s, episode steps: 95, steps per second: 608, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.064 [-1.010, 0.744], mean_best_reward: --
 99913/100000: episode: 1744, duration: 0.057s, episode steps: 36, steps per second: 629, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.164 [-0.591, 0.965], mean_best_reward: --
 99977/100000: episode: 1745, duration: 0.102s, episode steps: 64, steps per second: 629, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.172 [-0.721, 1.094], mean_best_reward: --
done, took 165.536 seconds
highest reward total seen : 200.0
Traceback (most recent call last):
  File "cem_cartpole.py", line 59, in <module>
    cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)
  File "build/bdist.linux-x86_64/egg/rl/agents/cem.py", line 64, in save_weights
  File "/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py", line 2437, in save_weights
    import h5py
ImportError: No module named h5py


