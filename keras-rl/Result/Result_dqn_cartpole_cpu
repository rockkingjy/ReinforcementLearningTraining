$ python dqn_cartpole.py 
Using TensorFlow backend.
[2016-10-09 10:45:10,457] Making new env: CartPole-v0
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
flatten_1 (Flatten)              (None, 4)             0           flatten_input_1[0][0]            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 16)            80          flatten_1[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 16)            0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 16)            272         activation_1[0][0]               
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 16)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 16)            272         activation_2[0][0]               
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 16)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 2)             34          activation_3[0][0]               
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 2)             0           dense_4[0][0]                    
====================================================================================================
Total params: 658
____________________________________________________________________________________________________
None
Training for 50000 steps ...
    15/50000: episode: 1, duration: 1.115s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.100 [-1.213, 0.585], loss: 0.626156, mean_absolute_error: 0.558272, mean_q: 0.174376
    50/50000: episode: 2, duration: 0.567s, episode steps: 35, steps per second: 62, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.038 [-1.527, 1.210], loss: 0.411177, mean_absolute_error: 0.446559, mean_q: 0.349045
    66/50000: episode: 3, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.079 [-1.618, 0.850], loss: 0.152964, mean_absolute_error: 0.253315, mean_q: 0.859077
    86/50000: episode: 4, duration: 0.333s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.082 [-0.613, 1.155], loss: 0.127074, mean_absolute_error: 0.194718, mean_q: 1.211297
   130/50000: episode: 5, duration: 0.733s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.117 [-1.003, 0.441], loss: 0.105943, mean_absolute_error: 0.175071, mean_q: 1.470645
   145/50000: episode: 6, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.080 [-1.755, 0.995], loss: 0.077661, mean_absolute_error: 0.146645, mean_q: 1.713725
   176/50000: episode: 7, duration: 0.516s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.083 [-0.843, 0.439], loss: 0.069146, mean_absolute_error: 0.132967, mean_q: 1.839940
   197/50000: episode: 8, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.074 [-1.479, 0.789], loss: 0.062218, mean_absolute_error: 0.111328, mean_q: 1.984861
   221/50000: episode: 9, duration: 0.400s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.101 [-0.757, 1.716], loss: 0.059020, mean_absolute_error: 0.091971, mean_q: 2.220005
   246/50000: episode: 10, duration: 0.415s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.067 [-2.328, 1.362], loss: 0.071560, mean_absolute_error: 0.096297, mean_q: 2.377227
   274/50000: episode: 11, duration: 0.467s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.061 [-0.826, 1.604], loss: 0.108011, mean_absolute_error: 0.108832, mean_q: 2.588683
   288/50000: episode: 12, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.088 [-1.314, 0.827], loss: 0.080051, mean_absolute_error: 0.117277, mean_q: 2.618591
   298/50000: episode: 13, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.109 [-2.031, 1.221], loss: 0.138943, mean_absolute_error: 0.109631, mean_q: 2.860867
   307/50000: episode: 14, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.115 [-1.803, 1.189], loss: 0.127650, mean_absolute_error: 0.117018, mean_q: 2.877737
   323/50000: episode: 15, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.598, 1.038], loss: 0.124060, mean_absolute_error: 0.117722, mean_q: 2.932747
   345/50000: episode: 16, duration: 0.367s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.036 [-1.446, 0.983], loss: 0.131440, mean_absolute_error: 0.119441, mean_q: 3.120672
   373/50000: episode: 17, duration: 0.465s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.382, 0.806], loss: 0.157059, mean_absolute_error: 0.132417, mean_q: 3.243088
   397/50000: episode: 18, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.075 [-1.219, 0.590], loss: 0.125531, mean_absolute_error: 0.117499, mean_q: 3.373356
   419/50000: episode: 19, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.054 [-1.612, 0.823], loss: 0.172655, mean_absolute_error: 0.138550, mean_q: 3.528702
   431/50000: episode: 20, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.658, 0.988], loss: 0.147112, mean_absolute_error: 0.126081, mean_q: 3.651539
   448/50000: episode: 21, duration: 0.285s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.066 [-1.415, 0.996], loss: 0.164150, mean_absolute_error: 0.136445, mean_q: 3.777619
   470/50000: episode: 22, duration: 0.363s, episode steps: 22, steps per second: 61, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.045 [-1.573, 2.415], loss: 0.209255, mean_absolute_error: 0.156416, mean_q: 3.922796
   485/50000: episode: 23, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.075 [-1.463, 1.013], loss: 0.185634, mean_absolute_error: 0.138404, mean_q: 4.151829
   533/50000: episode: 24, duration: 0.800s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.006 [-1.133, 0.793], loss: 0.244979, mean_absolute_error: 0.178546, mean_q: 4.299961
   619/50000: episode: 25, duration: 1.431s, episode steps: 86, steps per second: 60, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.133 [-1.185, 0.941], loss: 0.230483, mean_absolute_error: 0.160925, mean_q: 4.849193
   660/50000: episode: 26, duration: 0.683s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.035 [-1.428, 0.991], loss: 0.263947, mean_absolute_error: 0.172580, mean_q: 5.356207
   681/50000: episode: 27, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.032 [-0.990, 1.534], loss: 0.322341, mean_absolute_error: 0.180038, mean_q: 5.573714
   764/50000: episode: 28, duration: 1.383s, episode steps: 83, steps per second: 60, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.035 [-0.895, 0.944], loss: 0.324848, mean_absolute_error: 0.182127, mean_q: 6.013792
   795/50000: episode: 29, duration: 0.516s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.046 [-0.906, 0.450], loss: 0.359915, mean_absolute_error: 0.189387, mean_q: 6.485615
   809/50000: episode: 30, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.084 [-1.562, 1.024], loss: 0.200678, mean_absolute_error: 0.161136, mean_q: 6.689571
   830/50000: episode: 31, duration: 0.352s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.127 [-1.175, 0.558], loss: 0.283141, mean_absolute_error: 0.170295, mean_q: 6.910850
   842/50000: episode: 32, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-1.031, 1.748], loss: 0.314470, mean_absolute_error: 0.185111, mean_q: 6.992903
   861/50000: episode: 33, duration: 0.316s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.086 [-0.758, 1.171], loss: 0.353825, mean_absolute_error: 0.185931, mean_q: 7.121571
   873/50000: episode: 34, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.092 [-2.065, 1.417], loss: 0.323252, mean_absolute_error: 0.185537, mean_q: 7.135190
   970/50000: episode: 35, duration: 1.616s, episode steps: 97, steps per second: 60, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.192 [-0.930, 1.112], loss: 0.436463, mean_absolute_error: 0.202369, mean_q: 7.665860
  1021/50000: episode: 36, duration: 0.849s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.077 [-1.199, 1.305], loss: 0.445693, mean_absolute_error: 0.215753, mean_q: 8.235912
  1062/50000: episode: 37, duration: 0.684s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.051 [-0.389, 0.912], loss: 0.489077, mean_absolute_error: 0.206319, mean_q: 8.578235
  1081/50000: episode: 38, duration: 0.315s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.085 [-0.574, 1.093], loss: 0.807167, mean_absolute_error: 0.304266, mean_q: 8.640684
  1176/50000: episode: 39, duration: 1.583s, episode steps: 95, steps per second: 60, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.412 [-2.607, 1.278], loss: 0.537493, mean_absolute_error: 0.230804, mean_q: 9.252512
  1238/50000: episode: 40, duration: 1.033s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.222 [-1.048, 1.698], loss: 0.702986, mean_absolute_error: 0.259778, mean_q: 9.826502
  1256/50000: episode: 41, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-1.633, 1.170], loss: 0.442577, mean_absolute_error: 0.232334, mean_q: 10.271936
  1326/50000: episode: 42, duration: 1.171s, episode steps: 70, steps per second: 60, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.880, 0.928], loss: 0.594665, mean_absolute_error: 0.231755, mean_q: 10.626060
  1395/50000: episode: 43, duration: 1.145s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.027 [-1.478, 1.402], loss: 0.634002, mean_absolute_error: 0.240290, mean_q: 11.104869
  1435/50000: episode: 44, duration: 0.665s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.050 [-0.757, 1.547], loss: 0.619427, mean_absolute_error: 0.252527, mean_q: 11.515650
  1518/50000: episode: 45, duration: 1.383s, episode steps: 83, steps per second: 60, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.087 [-1.219, 1.264], loss: 0.751364, mean_absolute_error: 0.276852, mean_q: 11.988094
  1644/50000: episode: 46, duration: 2.099s, episode steps: 126, steps per second: 60, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.407 [-2.008, 1.206], loss: 0.646252, mean_absolute_error: 0.268304, mean_q: 12.885488
  1813/50000: episode: 47, duration: 2.816s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.145 [-1.332, 1.096], loss: 0.836366, mean_absolute_error: 0.273999, mean_q: 14.258034
  1971/50000: episode: 48, duration: 2.632s, episode steps: 158, steps per second: 60, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-1.186, 1.420], loss: 1.014493, mean_absolute_error: 0.311436, mean_q: 15.716685
  2080/50000: episode: 49, duration: 1.818s, episode steps: 109, steps per second: 60, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.089 [-1.323, 1.350], loss: 1.101994, mean_absolute_error: 0.313377, mean_q: 16.950432
  2236/50000: episode: 50, duration: 2.598s, episode steps: 156, steps per second: 60, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.299 [-1.709, 0.774], loss: 1.164332, mean_absolute_error: 0.304769, mean_q: 18.138979
  2439/50000: episode: 51, duration: 3.382s, episode steps: 203, steps per second: 60, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.192 [-1.067, 1.718], loss: 1.354541, mean_absolute_error: 0.341590, mean_q: 19.831104
  2667/50000: episode: 52, duration: 3.798s, episode steps: 228, steps per second: 60, episode reward: 228.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.306 [-2.420, 0.865], loss: 1.322758, mean_absolute_error: 0.348809, mean_q: 21.873331
  2907/50000: episode: 53, duration: 3.999s, episode steps: 240, steps per second: 60, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.299 [-2.420, 1.018], loss: 1.531620, mean_absolute_error: 0.372695, mean_q: 24.253456
  3190/50000: episode: 54, duration: 4.715s, episode steps: 283, steps per second: 60, episode reward: 283.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.249 [-2.418, 1.009], loss: 1.478140, mean_absolute_error: 0.384834, mean_q: 26.845034
  3433/50000: episode: 55, duration: 4.049s, episode steps: 243, steps per second: 60, episode reward: 243.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.288 [-2.402, 0.795], loss: 2.075904, mean_absolute_error: 0.442983, mean_q: 29.352753
  3644/50000: episode: 56, duration: 3.516s, episode steps: 211, steps per second: 60, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.333 [-2.422, 0.828], loss: 2.246144, mean_absolute_error: 0.459918, mean_q: 31.431154
  3925/50000: episode: 57, duration: 4.685s, episode steps: 281, steps per second: 60, episode reward: 281.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.241 [-2.410, 0.935], loss: 2.485108, mean_absolute_error: 0.486118, mean_q: 33.657169
  4136/50000: episode: 58, duration: 3.512s, episode steps: 211, steps per second: 60, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.327 [-2.425, 0.739], loss: 2.101593, mean_absolute_error: 0.465935, mean_q: 35.988289
  4392/50000: episode: 59, duration: 4.266s, episode steps: 256, steps per second: 60, episode reward: 256.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.274 [-2.411, 1.015], loss: 2.540286, mean_absolute_error: 0.506217, mean_q: 38.078403
  4675/50000: episode: 60, duration: 4.716s, episode steps: 283, steps per second: 60, episode reward: 283.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.241 [-2.414, 1.052], loss: 2.659241, mean_absolute_error: 0.525980, mean_q: 40.544132
  4915/50000: episode: 61, duration: 3.999s, episode steps: 240, steps per second: 60, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.294 [-2.430, 0.754], loss: 2.849005, mean_absolute_error: 0.532991, mean_q: 42.817894
  5086/50000: episode: 62, duration: 2.849s, episode steps: 171, steps per second: 60, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.376 [-2.275, 0.813], loss: 2.812810, mean_absolute_error: 0.575256, mean_q: 44.309990
  5294/50000: episode: 63, duration: 3.466s, episode steps: 208, steps per second: 60, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.329 [-2.411, 0.733], loss: 3.137253, mean_absolute_error: 0.573687, mean_q: 45.906792
  5485/50000: episode: 64, duration: 3.182s, episode steps: 191, steps per second: 60, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.357 [-2.419, 0.920], loss: 3.639252, mean_absolute_error: 0.608194, mean_q: 47.219318
  5658/50000: episode: 65, duration: 2.883s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.384 [-2.417, 0.778], loss: 2.504900, mean_absolute_error: 0.557109, mean_q: 48.698982
  5866/50000: episode: 66, duration: 3.465s, episode steps: 208, steps per second: 60, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.327 [-2.344, 0.782], loss: 3.015739, mean_absolute_error: 0.591966, mean_q: 50.043869
  6052/50000: episode: 67, duration: 3.099s, episode steps: 186, steps per second: 60, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.368 [-2.414, 0.956], loss: 1.888079, mean_absolute_error: 0.536434, mean_q: 51.565849
  6275/50000: episode: 68, duration: 3.716s, episode steps: 223, steps per second: 60, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.309 [-2.404, 1.031], loss: 2.878875, mean_absolute_error: 0.580548, mean_q: 52.925587
  6477/50000: episode: 69, duration: 3.365s, episode steps: 202, steps per second: 60, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.350 [-2.431, 1.005], loss: 2.859515, mean_absolute_error: 0.602571, mean_q: 54.341854
  6703/50000: episode: 70, duration: 3.766s, episode steps: 226, steps per second: 60, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.307 [-2.421, 0.908], loss: 2.854481, mean_absolute_error: 0.585177, mean_q: 56.133305
  6992/50000: episode: 71, duration: 4.815s, episode steps: 289, steps per second: 60, episode reward: 289.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.240 [-2.403, 0.653], loss: 2.983217, mean_absolute_error: 0.623193, mean_q: 57.665428
  7210/50000: episode: 72, duration: 3.632s, episode steps: 218, steps per second: 60, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.314 [-2.408, 0.986], loss: 3.551770, mean_absolute_error: 0.628087, mean_q: 59.644680
  7397/50000: episode: 73, duration: 3.116s, episode steps: 187, steps per second: 60, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.366 [-2.412, 0.884], loss: 2.291494, mean_absolute_error: 0.574276, mean_q: 60.857834
  7645/50000: episode: 74, duration: 4.137s, episode steps: 248, steps per second: 60, episode reward: 248.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.275 [-2.406, 0.870], loss: 3.324120, mean_absolute_error: 0.607804, mean_q: 62.013748
  7840/50000: episode: 75, duration: 3.245s, episode steps: 195, steps per second: 60, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.352 [-2.423, 0.891], loss: 3.098803, mean_absolute_error: 0.613053, mean_q: 63.414371
  8055/50000: episode: 76, duration: 3.582s, episode steps: 215, steps per second: 60, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.304 [-2.306, 0.939], loss: 3.825815, mean_absolute_error: 0.644497, mean_q: 64.432289
  8238/50000: episode: 77, duration: 3.050s, episode steps: 183, steps per second: 60, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.367 [-2.408, 0.979], loss: 3.754275, mean_absolute_error: 0.618318, mean_q: 65.686295
  8462/50000: episode: 78, duration: 3.731s, episode steps: 224, steps per second: 60, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.309 [-2.413, 0.759], loss: 3.696730, mean_absolute_error: 0.623499, mean_q: 66.780685
  8654/50000: episode: 79, duration: 3.199s, episode steps: 192, steps per second: 60, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.360 [-2.423, 0.786], loss: 2.460511, mean_absolute_error: 0.558629, mean_q: 67.597870
  8822/50000: episode: 80, duration: 2.799s, episode steps: 168, steps per second: 60, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.398 [-2.430, 1.174], loss: 3.686587, mean_absolute_error: 0.594843, mean_q: 68.634758
  9031/50000: episode: 81, duration: 3.482s, episode steps: 209, steps per second: 60, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.328 [-2.420, 0.982], loss: 4.138186, mean_absolute_error: 0.623230, mean_q: 69.313492
  9210/50000: episode: 82, duration: 2.983s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.372 [-2.419, 0.829], loss: 3.371535, mean_absolute_error: 0.573130, mean_q: 70.279549
  9400/50000: episode: 83, duration: 3.169s, episode steps: 190, steps per second: 60, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.354 [-2.405, 0.870], loss: 2.973854, mean_absolute_error: 0.576099, mean_q: 71.098770
  9582/50000: episode: 84, duration: 3.029s, episode steps: 182, steps per second: 60, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.361 [-2.404, 0.889], loss: 2.759915, mean_absolute_error: 0.554571, mean_q: 72.237167
  9785/50000: episode: 85, duration: 3.382s, episode steps: 203, steps per second: 60, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.328 [-2.434, 0.721], loss: 2.990036, mean_absolute_error: 0.574217, mean_q: 72.336281
  9969/50000: episode: 86, duration: 3.066s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.357 [-2.429, 1.148], loss: 3.358196, mean_absolute_error: 0.570803, mean_q: 73.385635
 10155/50000: episode: 87, duration: 3.100s, episode steps: 186, steps per second: 60, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.358 [-2.447, 1.070], loss: 3.295640, mean_absolute_error: 0.589745, mean_q: 74.455475
 10335/50000: episode: 88, duration: 2.997s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.372 [-2.407, 0.875], loss: 4.236497, mean_absolute_error: 0.580955, mean_q: 75.133041
 10512/50000: episode: 89, duration: 2.950s, episode steps: 177, steps per second: 60, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.370 [-2.440, 0.936], loss: 2.340858, mean_absolute_error: 0.534668, mean_q: 76.369438
 10676/50000: episode: 90, duration: 2.732s, episode steps: 164, steps per second: 60, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.391 [-2.542, 0.909], loss: 3.986854, mean_absolute_error: 0.598554, mean_q: 75.920372
 10868/50000: episode: 91, duration: 3.200s, episode steps: 192, steps per second: 60, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.340 [-2.531, 1.023], loss: 3.806686, mean_absolute_error: 0.554078, mean_q: 76.815102
 11059/50000: episode: 92, duration: 3.182s, episode steps: 191, steps per second: 60, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.336 [-2.609, 0.966], loss: 3.654695, mean_absolute_error: 0.595059, mean_q: 76.304367
 11234/50000: episode: 93, duration: 2.916s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.378 [-2.581, 1.054], loss: 3.880325, mean_absolute_error: 0.574244, mean_q: 77.543869
 11431/50000: episode: 94, duration: 3.282s, episode steps: 197, steps per second: 60, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.326 [-2.607, 0.935], loss: 4.200617, mean_absolute_error: 0.594603, mean_q: 77.710167
 11643/50000: episode: 95, duration: 3.533s, episode steps: 212, steps per second: 60, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.301 [-2.404, 1.143], loss: 1.972104, mean_absolute_error: 0.499184, mean_q: 78.646446
 11832/50000: episode: 96, duration: 3.149s, episode steps: 189, steps per second: 60, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.343 [-2.408, 0.907], loss: 3.379800, mean_absolute_error: 0.525304, mean_q: 79.034538
 12007/50000: episode: 97, duration: 2.916s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.368 [-2.791, 1.308], loss: 2.045670, mean_absolute_error: 0.472960, mean_q: 78.805603
 12192/50000: episode: 98, duration: 3.082s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.346 [-2.557, 1.133], loss: 2.790387, mean_absolute_error: 0.487580, mean_q: 79.723946
 12406/50000: episode: 99, duration: 3.566s, episode steps: 214, steps per second: 60, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.203 [-1.691, 1.141], loss: 3.045795, mean_absolute_error: 0.500543, mean_q: 80.050720
 12577/50000: episode: 100, duration: 2.850s, episode steps: 171, steps per second: 60, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.377 [-2.408, 1.077], loss: 3.731349, mean_absolute_error: 0.517934, mean_q: 80.213310
 12761/50000: episode: 101, duration: 3.065s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.357 [-2.585, 1.209], loss: 2.233345, mean_absolute_error: 0.435200, mean_q: 80.481239
 12987/50000: episode: 102, duration: 3.766s, episode steps: 226, steps per second: 60, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.314 [-2.423, 1.258], loss: 2.510365, mean_absolute_error: 0.450063, mean_q: 80.344009
 13183/50000: episode: 103, duration: 3.266s, episode steps: 196, steps per second: 60, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.335 [-2.416, 1.346], loss: 4.265839, mean_absolute_error: 0.500122, mean_q: 80.387146
 13380/50000: episode: 104, duration: 3.282s, episode steps: 197, steps per second: 60, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.331 [-2.970, 1.380], loss: 2.944056, mean_absolute_error: 0.470475, mean_q: 80.656578
 13610/50000: episode: 105, duration: 3.832s, episode steps: 230, steps per second: 60, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.306 [-2.352, 1.356], loss: 2.191914, mean_absolute_error: 0.427613, mean_q: 80.674294
 13820/50000: episode: 106, duration: 3.500s, episode steps: 210, steps per second: 60, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.329 [-2.421, 1.290], loss: 3.226284, mean_absolute_error: 0.444792, mean_q: 81.282394
 14070/50000: episode: 107, duration: 4.165s, episode steps: 250, steps per second: 60, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.275 [-2.421, 1.391], loss: 3.541351, mean_absolute_error: 0.441904, mean_q: 80.798874
 14369/50000: episode: 108, duration: 4.983s, episode steps: 299, steps per second: 60, episode reward: 299.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.229 [-2.406, 1.370], loss: 2.700284, mean_absolute_error: 0.416340, mean_q: 81.498978
 14729/50000: episode: 109, duration: 5.998s, episode steps: 360, steps per second: 60, episode reward: 360.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.232 [-1.218, 2.311], loss: 3.204154, mean_absolute_error: 0.444954, mean_q: 82.007187
 14942/50000: episode: 110, duration: 3.549s, episode steps: 213, steps per second: 60, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.273 [-1.875, 1.296], loss: 2.946180, mean_absolute_error: 0.437329, mean_q: 83.142891
 15482/50000: episode: 111, duration: 8.999s, episode steps: 540, steps per second: 60, episode reward: 540.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.154 [-1.306, 2.434], loss: 3.482054, mean_absolute_error: 0.430646, mean_q: 83.569443
 15735/50000: episode: 112, duration: 4.216s, episode steps: 253, steps per second: 60, episode reward: 253.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.265 [-2.402, 1.451], loss: 1.536365, mean_absolute_error: 0.371804, mean_q: 83.892822
 15980/50000: episode: 113, duration: 4.082s, episode steps: 245, steps per second: 60, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.298 [-1.065, 2.418], loss: 3.490906, mean_absolute_error: 0.439656, mean_q: 83.298599
 16195/50000: episode: 114, duration: 3.582s, episode steps: 215, steps per second: 60, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.281 [-2.117, 1.317], loss: 3.456075, mean_absolute_error: 0.432408, mean_q: 84.361374
 16394/50000: episode: 115, duration: 3.316s, episode steps: 199, steps per second: 60, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.249 [-1.909, 1.406], loss: 3.098184, mean_absolute_error: 0.435108, mean_q: 83.826485
 16639/50000: episode: 116, duration: 4.084s, episode steps: 245, steps per second: 60, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.279 [-1.217, 2.436], loss: 3.645752, mean_absolute_error: 0.448481, mean_q: 84.180016
 16858/50000: episode: 117, duration: 3.646s, episode steps: 219, steps per second: 60, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.319 [-2.586, 1.692], loss: 2.925074, mean_absolute_error: 0.387965, mean_q: 84.016342
 17081/50000: episode: 118, duration: 3.716s, episode steps: 223, steps per second: 60, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.285 [-2.371, 3.537], loss: 2.691516, mean_absolute_error: 0.383440, mean_q: 83.720230
 17368/50000: episode: 119, duration: 4.783s, episode steps: 287, steps per second: 60, episode reward: 287.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.244 [-2.242, 1.571], loss: 3.031754, mean_absolute_error: 0.431364, mean_q: 84.274910
 17588/50000: episode: 120, duration: 3.666s, episode steps: 220, steps per second: 60, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.195 [-1.662, 1.507], loss: 2.263549, mean_absolute_error: 0.399091, mean_q: 84.251297
 17755/50000: episode: 121, duration: 2.799s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.366 [-2.106, 1.624], loss: 2.747458, mean_absolute_error: 0.409944, mean_q: 83.751740
 17925/50000: episode: 122, duration: 2.832s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.327 [-2.902, 3.755], loss: 2.957816, mean_absolute_error: 0.419427, mean_q: 84.216812
 18114/50000: episode: 123, duration: 3.149s, episode steps: 189, steps per second: 60, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.304 [-2.254, 3.519], loss: 1.730098, mean_absolute_error: 0.378200, mean_q: 83.730042
 18248/50000: episode: 124, duration: 2.232s, episode steps: 134, steps per second: 60, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.410 [-2.785, 3.765], loss: 4.327031, mean_absolute_error: 0.471944, mean_q: 84.586464
 18417/50000: episode: 125, duration: 2.818s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.334 [-2.373, 3.558], loss: 4.794404, mean_absolute_error: 0.512698, mean_q: 84.374466
 18583/50000: episode: 126, duration: 2.764s, episode steps: 166, steps per second: 60, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.390 [-2.400, 1.434], loss: 5.064529, mean_absolute_error: 0.532755, mean_q: 83.538109
 18734/50000: episode: 127, duration: 2.516s, episode steps: 151, steps per second: 60, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.563 [0.000, 1.000], mean observation: 0.369 [-2.445, 3.599], loss: 3.372534, mean_absolute_error: 0.435967, mean_q: 84.303177
 18868/50000: episode: 128, duration: 2.233s, episode steps: 134, steps per second: 60, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.434 [-1.355, 3.018], loss: 4.974319, mean_absolute_error: 0.498027, mean_q: 83.762230
 19039/50000: episode: 129, duration: 2.849s, episode steps: 171, steps per second: 60, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.350 [-1.532, 3.143], loss: 2.575122, mean_absolute_error: 0.422694, mean_q: 85.080292
 19232/50000: episode: 130, duration: 3.216s, episode steps: 193, steps per second: 60, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.327 [-2.247, 1.076], loss: 4.521762, mean_absolute_error: 0.451209, mean_q: 84.085129
 19414/50000: episode: 131, duration: 3.033s, episode steps: 182, steps per second: 60, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.376 [-2.311, 1.247], loss: 2.952168, mean_absolute_error: 0.454776, mean_q: 83.981499
 19633/50000: episode: 132, duration: 3.634s, episode steps: 219, steps per second: 60, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.278 [-2.471, 3.582], loss: 5.121319, mean_absolute_error: 0.524838, mean_q: 83.004066
 19913/50000: episode: 133, duration: 4.664s, episode steps: 280, steps per second: 60, episode reward: 280.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.252 [-2.211, 1.592], loss: 3.583120, mean_absolute_error: 0.480089, mean_q: 84.265869
 20137/50000: episode: 134, duration: 3.733s, episode steps: 224, steps per second: 60, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.264 [-2.119, 3.392], loss: 4.283517, mean_absolute_error: 0.495435, mean_q: 82.945419
 20297/50000: episode: 135, duration: 2.665s, episode steps: 160, steps per second: 60, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.340 [-2.881, 3.750], loss: 3.736193, mean_absolute_error: 0.486954, mean_q: 83.975807
 20476/50000: episode: 136, duration: 2.982s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.325 [-1.803, 3.132], loss: 3.600909, mean_absolute_error: 0.478521, mean_q: 83.324997
 20639/50000: episode: 137, duration: 2.716s, episode steps: 163, steps per second: 60, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.365 [-1.917, 3.181], loss: 2.960691, mean_absolute_error: 0.469329, mean_q: 82.882324
 20838/50000: episode: 138, duration: 3.316s, episode steps: 199, steps per second: 60, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.293 [-1.967, 3.203], loss: 3.128338, mean_absolute_error: 0.456415, mean_q: 83.847458
 20983/50000: episode: 139, duration: 2.418s, episode steps: 145, steps per second: 60, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.566 [0.000, 1.000], mean observation: 0.386 [-2.656, 3.531], loss: 1.963185, mean_absolute_error: 0.429927, mean_q: 84.355347
 21125/50000: episode: 140, duration: 2.363s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.563 [0.000, 1.000], mean observation: 0.402 [-2.450, 3.388], loss: 2.221846, mean_absolute_error: 0.403022, mean_q: 85.080490
 21253/50000: episode: 141, duration: 2.133s, episode steps: 128, steps per second: 60, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.578 [0.000, 1.000], mean observation: 0.425 [-3.137, 3.756], loss: 3.309422, mean_absolute_error: 0.520078, mean_q: 84.034409
 21498/50000: episode: 142, duration: 4.081s, episode steps: 245, steps per second: 60, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.218 [-2.600, 3.559], loss: 3.609261, mean_absolute_error: 0.496996, mean_q: 84.181221
 21620/50000: episode: 143, duration: 2.032s, episode steps: 122, steps per second: 60, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.574 [0.000, 1.000], mean observation: 0.451 [-2.509, 3.434], loss: 3.434156, mean_absolute_error: 0.496841, mean_q: 83.950752
 21762/50000: episode: 144, duration: 2.366s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.570 [0.000, 1.000], mean observation: 0.391 [-3.015, 3.751], loss: 4.080322, mean_absolute_error: 0.554958, mean_q: 84.100449
 21891/50000: episode: 145, duration: 2.149s, episode steps: 129, steps per second: 60, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.574 [0.000, 1.000], mean observation: 0.433 [-2.740, 3.600], loss: 2.824369, mean_absolute_error: 0.463427, mean_q: 84.118988
 22072/50000: episode: 146, duration: 3.015s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.302 [-2.752, 3.558], loss: 3.456381, mean_absolute_error: 0.526699, mean_q: 85.049118
 22229/50000: episode: 147, duration: 2.617s, episode steps: 157, steps per second: 60, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.359 [-2.668, 3.540], loss: 2.222149, mean_absolute_error: 0.500800, mean_q: 83.864059
 22363/50000: episode: 148, duration: 2.232s, episode steps: 134, steps per second: 60, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.411 [-3.087, 3.778], loss: 1.584366, mean_absolute_error: 0.438645, mean_q: 84.058159
 22538/50000: episode: 149, duration: 2.916s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.344 [-1.157, 2.783], loss: 2.541937, mean_absolute_error: 0.462918, mean_q: 82.889801
 22682/50000: episode: 150, duration: 2.400s, episode steps: 144, steps per second: 60, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.569 [0.000, 1.000], mean observation: 0.372 [-3.182, 3.768], loss: 2.460268, mean_absolute_error: 0.476453, mean_q: 84.678261
 22818/50000: episode: 151, duration: 2.270s, episode steps: 136, steps per second: 60, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.441 [-1.539, 2.980], loss: 2.223684, mean_absolute_error: 0.466510, mean_q: 84.657951
 22959/50000: episode: 152, duration: 2.345s, episode steps: 141, steps per second: 60, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: 0.405 [-2.613, 3.527], loss: 3.876209, mean_absolute_error: 0.543184, mean_q: 84.017097
 23108/50000: episode: 153, duration: 2.482s, episode steps: 149, steps per second: 60, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.389 [-2.727, 3.586], loss: 1.856564, mean_absolute_error: 0.482068, mean_q: 85.207870
 23248/50000: episode: 154, duration: 2.336s, episode steps: 140, steps per second: 60, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.408 [-2.244, 3.350], loss: 2.618417, mean_absolute_error: 0.491009, mean_q: 84.812943
 23382/50000: episode: 155, duration: 2.230s, episode steps: 134, steps per second: 60, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.447 [-1.648, 2.995], loss: 1.887960, mean_absolute_error: 0.454974, mean_q: 83.584877
 23526/50000: episode: 156, duration: 2.404s, episode steps: 144, steps per second: 60, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.399 [-2.274, 3.357], loss: 3.621367, mean_absolute_error: 0.539188, mean_q: 83.480515
 23656/50000: episode: 157, duration: 2.162s, episode steps: 130, steps per second: 60, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.569 [0.000, 1.000], mean observation: 0.435 [-2.268, 3.356], loss: 2.288372, mean_absolute_error: 0.498907, mean_q: 83.151535
 23797/50000: episode: 158, duration: 2.352s, episode steps: 141, steps per second: 60, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.431 [-1.239, 2.791], loss: 3.305437, mean_absolute_error: 0.491515, mean_q: 83.865791
 23938/50000: episode: 159, duration: 2.345s, episode steps: 141, steps per second: 60, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.443 [-0.687, 2.419], loss: 2.068904, mean_absolute_error: 0.482142, mean_q: 83.431160
 24094/50000: episode: 160, duration: 2.600s, episode steps: 156, steps per second: 60, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.403 [-1.094, 2.419], loss: 1.452575, mean_absolute_error: 0.428736, mean_q: 84.419960
 24223/50000: episode: 161, duration: 2.149s, episode steps: 129, steps per second: 60, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.458 [-1.159, 2.772], loss: 2.416013, mean_absolute_error: 0.444283, mean_q: 81.604332
 24349/50000: episode: 162, duration: 2.102s, episode steps: 126, steps per second: 60, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.563 [0.000, 1.000], mean observation: 0.478 [-1.415, 2.996], loss: 1.665612, mean_absolute_error: 0.419940, mean_q: 84.268951
 24485/50000: episode: 163, duration: 2.263s, episode steps: 136, steps per second: 60, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.456 [-1.097, 2.920], loss: 3.561978, mean_absolute_error: 0.522764, mean_q: 82.973961
 24633/50000: episode: 164, duration: 2.466s, episode steps: 148, steps per second: 60, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.420 [-0.872, 2.609], loss: 2.803262, mean_absolute_error: 0.493599, mean_q: 82.994125
 24787/50000: episode: 165, duration: 2.566s, episode steps: 154, steps per second: 60, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.406 [-1.368, 2.946], loss: 2.081686, mean_absolute_error: 0.460305, mean_q: 82.923546
 24929/50000: episode: 166, duration: 2.367s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.440 [-1.296, 2.920], loss: 1.397036, mean_absolute_error: 0.424265, mean_q: 82.002899
 25069/50000: episode: 167, duration: 2.332s, episode steps: 140, steps per second: 60, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.449 [-0.820, 2.594], loss: 4.509666, mean_absolute_error: 0.549615, mean_q: 81.899330
 25226/50000: episode: 168, duration: 2.616s, episode steps: 157, steps per second: 60, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.410 [-0.707, 2.428], loss: 2.398823, mean_absolute_error: 0.515209, mean_q: 82.298103
 25382/50000: episode: 169, duration: 2.601s, episode steps: 156, steps per second: 60, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.412 [-0.722, 2.570], loss: 1.843369, mean_absolute_error: 0.473377, mean_q: 82.097794
 25535/50000: episode: 170, duration: 2.547s, episode steps: 153, steps per second: 60, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.425 [-0.826, 2.403], loss: 1.551151, mean_absolute_error: 0.430683, mean_q: 82.636993
 25690/50000: episode: 171, duration: 2.582s, episode steps: 155, steps per second: 60, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.413 [-0.978, 2.408], loss: 1.885025, mean_absolute_error: 0.421771, mean_q: 81.903603
 25846/50000: episode: 172, duration: 2.599s, episode steps: 156, steps per second: 60, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.432 [-0.742, 2.430], loss: 1.212759, mean_absolute_error: 0.406082, mean_q: 82.583107
 26007/50000: episode: 173, duration: 2.683s, episode steps: 161, steps per second: 60, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.398 [-1.117, 2.408], loss: 1.726344, mean_absolute_error: 0.457185, mean_q: 81.894135
 26157/50000: episode: 174, duration: 2.499s, episode steps: 150, steps per second: 60, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.439 [-0.908, 2.430], loss: 1.304633, mean_absolute_error: 0.381163, mean_q: 83.233582
 26310/50000: episode: 175, duration: 2.549s, episode steps: 153, steps per second: 60, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.429 [-0.946, 2.401], loss: 1.728352, mean_absolute_error: 0.420460, mean_q: 83.081032
 26494/50000: episode: 176, duration: 3.066s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.377 [-0.856, 2.426], loss: 2.758235, mean_absolute_error: 0.471594, mean_q: 83.137436
 26698/50000: episode: 177, duration: 3.399s, episode steps: 204, steps per second: 60, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.360 [-1.062, 2.412], loss: 2.244467, mean_absolute_error: 0.460373, mean_q: 83.913376
 26879/50000: episode: 178, duration: 3.083s, episode steps: 181, steps per second: 59, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.388 [-1.097, 2.427], loss: 2.036361, mean_absolute_error: 0.396292, mean_q: 84.696373
 27059/50000: episode: 179, duration: 2.999s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.394 [-0.885, 2.403], loss: 2.329606, mean_absolute_error: 0.424206, mean_q: 84.605568
 27262/50000: episode: 180, duration: 3.383s, episode steps: 203, steps per second: 60, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.358 [-1.296, 2.419], loss: 1.836350, mean_absolute_error: 0.431365, mean_q: 85.782639
 27446/50000: episode: 181, duration: 3.066s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.382 [-0.949, 2.435], loss: 1.257035, mean_absolute_error: 0.392212, mean_q: 86.643616
 27629/50000: episode: 182, duration: 3.050s, episode steps: 183, steps per second: 60, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.380 [-0.890, 2.401], loss: 1.632623, mean_absolute_error: 0.415459, mean_q: 87.441376
 27835/50000: episode: 183, duration: 3.432s, episode steps: 206, steps per second: 60, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.350 [-1.015, 2.405], loss: 2.410302, mean_absolute_error: 0.449631, mean_q: 86.337982
 28108/50000: episode: 184, duration: 4.548s, episode steps: 273, steps per second: 60, episode reward: 273.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.274 [-1.365, 2.713], loss: 1.428241, mean_absolute_error: 0.414301, mean_q: 88.130638
 28368/50000: episode: 185, duration: 4.330s, episode steps: 260, steps per second: 60, episode reward: 260.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.301 [-1.161, 2.402], loss: 1.540520, mean_absolute_error: 0.397192, mean_q: 88.905884
 28713/50000: episode: 186, duration: 5.746s, episode steps: 345, steps per second: 60, episode reward: 345.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.252 [-1.415, 2.566], loss: 1.623763, mean_absolute_error: 0.390143, mean_q: 89.117096
 30049/50000: episode: 187, duration: 22.255s, episode steps: 1336, steps per second: 60, episode reward: 1336.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.131 [-1.347, 2.430], loss: 1.599070, mean_absolute_error: 0.409833, mean_q: 93.536942
 34685/50000: episode: 188, duration: 77.298s, episode steps: 4636, steps per second: 60, episode reward: 4636.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.035 [-1.391, 2.233], loss: 3.085656, mean_absolute_error: 0.550090, mean_q: 109.756943
 35004/50000: episode: 189, duration: 5.307s, episode steps: 319, steps per second: 60, episode reward: 319.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.162 [-1.358, 2.207], loss: 3.816754, mean_absolute_error: 0.516909, mean_q: 116.495224
 35519/50000: episode: 190, duration: 8.582s, episode steps: 515, steps per second: 60, episode reward: 515.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.028 [-1.860, 2.268], loss: 5.494314, mean_absolute_error: 0.593156, mean_q: 115.922127
 35927/50000: episode: 191, duration: 6.799s, episode steps: 408, steps per second: 60, episode reward: 408.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.252 [-2.408, 1.194], loss: 4.591250, mean_absolute_error: 0.549552, mean_q: 115.344765
 37265/50000: episode: 192, duration: 22.299s, episode steps: 1338, steps per second: 60, episode reward: 1338.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.499 [0.000, 1.000], mean observation: -0.159 [-2.401, 1.587], loss: 4.909949, mean_absolute_error: 0.519992, mean_q: 114.653374
 37664/50000: episode: 193, duration: 6.648s, episode steps: 399, steps per second: 60, episode reward: 399.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.101 [-1.555, 2.234], loss: 5.816664, mean_absolute_error: 0.539554, mean_q: 113.584373
 37883/50000: episode: 194, duration: 3.697s, episode steps: 219, steps per second: 59, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.265 [-2.450, 1.679], loss: 5.099012, mean_absolute_error: 0.541804, mean_q: 113.237549
 38523/50000: episode: 195, duration: 10.650s, episode steps: 640, steps per second: 60, episode reward: 640.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.110 [-2.406, 1.675], loss: 4.540508, mean_absolute_error: 0.499091, mean_q: 112.655273
 39038/50000: episode: 196, duration: 8.546s, episode steps: 515, steps per second: 60, episode reward: 515.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.199 [-1.679, 2.371], loss: 4.471668, mean_absolute_error: 0.494106, mean_q: 111.852737
 39447/50000: episode: 197, duration: 6.783s, episode steps: 409, steps per second: 60, episode reward: 409.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.225 [-1.535, 2.412], loss: 3.484081, mean_absolute_error: 0.466306, mean_q: 112.004471
 40214/50000: episode: 198, duration: 12.783s, episode steps: 767, steps per second: 60, episode reward: 767.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.128 [-1.860, 2.409], loss: 3.991201, mean_absolute_error: 0.502073, mean_q: 110.719536
 40862/50000: episode: 199, duration: 10.795s, episode steps: 648, steps per second: 60, episode reward: 648.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.173 [-1.784, 2.437], loss: 3.090439, mean_absolute_error: 0.470968, mean_q: 110.248047
 41466/50000: episode: 200, duration: 10.065s, episode steps: 604, steps per second: 60, episode reward: 604.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.176 [-1.642, 2.429], loss: 3.577959, mean_absolute_error: 0.480984, mean_q: 109.359917
 41869/50000: episode: 201, duration: 6.715s, episode steps: 403, steps per second: 60, episode reward: 403.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.248 [-1.480, 2.423], loss: 2.196145, mean_absolute_error: 0.444715, mean_q: 109.096001
 42114/50000: episode: 202, duration: 4.083s, episode steps: 245, steps per second: 60, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.337 [-1.259, 2.421], loss: 2.866770, mean_absolute_error: 0.445199, mean_q: 108.617676
 49140/50000: episode: 203, duration: 117.048s, episode steps: 7026, steps per second: 60, episode reward: 7026.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-1.952, 1.977], loss: 1.647286, mean_absolute_error: 0.396950, mean_q: 107.939415
done, took 834.189 seconds
Episode 1: reward: 319.000, steps: 319
Episode 2: reward: 694.000, steps: 694
Episode 3: reward: 793.000, steps: 793
Episode 4: reward: 602.000, steps: 602
Episode 5: reward: 976.000, steps: 976

